[["index.html", "MLOps Architecture from scratch Chapter 1 About Topics", " MLOps Architecture from scratch Sebastian Blum 2022-11-03 Chapter 1 About This document is written during my journey in the realm of MLOps. It is therefore in a state of continuous development. Topics The overall aim is to build and create a MLOps architecture based on Airflow running on AWS EKS. Ideally, this architecture is create using terraform. Model tracking might be done using MLFlow, Data tracking using DVC. Further mentioned might be best practices in software development, CI/CD, Docker, and pipelines. I might also include a small Data Science use case utilizing the Airflow Cluster we built. "],["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction This project started out of an interest in multiple domains. At first, I was working on a Kubeflow platform at the time and haven’t had much experience in the realm of MLOps. I was starting with K8s and Terraform and was interested to dig deeper. I did so by teaching myself and I needed a project. What better also to do than to build “my own” MLOps plattform. I used Airflow since it is widely uses for workflow management and I also wanted to use it from the perspective of a data scientist, meaning to actually build some pipelines with it. This idea of extending the project to actually have a running use case expanded this work about including MLFlow for model tracking and DVC for data version control. A work in progress This project / book / tutorial / whatever this is or will be startet by explaining the concept of Kubernetes. The plan is to continuously update it by further sections. Since there is no deadline, there is no timeline, and I am also not sure whether there will exist something final to be honest. "],["terraform.html", "Chapter 3 Terraform 3.1 Prerequisites 3.2 Basic usage 3.3 Core Concepts 3.4 State 3.5 Modules 3.6 Additional tips &amp; tricks 3.7 Putting it together 3.8 TODO", " Chapter 3 Terraform Terraform is an open-source, declarative programming language developed by HashiCorp and allows to create both on-prem and cloud resources in the form of writing code. This is also know as Infrastructure as Code (IAC). There are several IaC tools in the market today and Terraform is only one of them. Yet it is a well known and established tool and during the course of this project we only focus on this. Hashicorp describes that: Terraform is an infrastructure as code (IaC) tool that allows you to build, change, and version infrastructure safely and efficiently. This includes both low-level components like compute instances, storage, and networking, as well as high-level components like DNS entries and SaaS features. This means that users are able to manage and provision an entire IT infrastructure using machine-readable definition files and thus allowing faster execution when configuring infrastructure, as well as enableing full traceability of changes. Terraform comes with several hundred different providers that can be used to provision infrastructure, such as Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, DataDog, etc. The given chapter introduces the concepts &amp; usage of Terraform which will be needed to create the introduced MLOps Airflow deployment in an automated and tracable way. We will learn how to use Terraform to provision ressources as well as to structure a Terraform Project. 3.1 Prerequisites To be able to follow this tutorial, one needs to have the AWS CLI installed as well as the AWS credentials set up. Needles to say an AWS accounts needs to be present. It is also recommended to have basic knowledge of the AWS Cloud as this tutorials used the AWS infrastructure to provision cloud resources. The attached resource definitions are specified to the AWS region eu-central-1. It might be necessary to change accordingly if you are set in another region. Further, Terraform itself needs to be installed. Please refer to the corresponding sites. The scripts are run under Terraform version v1.2.4. Later releases might have breaking changes. One can check its installation via terraform version. 3.2 Basic usage A Terraform project is basically just a set of files in a directory containing resource definitions of cloud ressources to be created. Those Terraform files, denoted by the ending .tf, use Terraform’s configuration language to define the specified resources. In the following example there are two definitions made: a provider and a resource. Later in this chapter we will dive deeper in the structure of the language. For now, we only need to know this script is creating a file called hello.txt that includes the text \"Hello, Terraform\". It’s our Terraform version of Hello World! provider &quot;local&quot; { version = &quot;~&gt; 1.4&quot; } resource &quot;local_file&quot; &quot;hello&quot; { content = &quot;Hello, Terraform&quot; filename = &quot;hello.txt&quot; } 3.2.1 terraform init When a project is run for the first time the terraform project needs to be initialized. This is done via the terraform init command. Terraform scans the project files in this step and downloads any required providers needed (more details to providers in a following section). In the given example this is the local procider. # Initializes the working directory which consists of all the configuration files terraform init Terraform init 3.2.2 terraform validate The terraform validate command checks the code for syntax errors. This is optional yet a way to handle initial errors or minor careless mistakes # Validates the configuration files in a directory terraform validate Terraform validate 3.2.3 terraform plan The terraform plan command verifies what action Terraform will perform and what resources will be created. This step is basically a dry run of the code to be executed. It also returns the provided values and some permission attributes which have been set. # Creates an execution plan to reach a desired state of the infrastructure terraform plan Terraform plan 3.2.4 terraform apply The command Terraform apply creates the resource specified in the .tf files. Initially, the same output as in the terraform plan step is shown (hence its dry run). The output further states which resources are added, which will be changed, and which resources will be destroyed. After confirming the actions the resource creation will be executed. Modifications to previously deployed ressources can be implemented by using terraform apply again. The output will denote that there are resources to change. # Provision the changes in the infrastructure as defined in the plan terraform apply Terraform apply 3.2.5 terraform destroy To destoy all created ressouces and to delete everything we did before, there is a terraform destroy command. # Deletes all the old infrastructure resources terraform destroy Terraform destroy 3.3 Core Concepts The following section will explain the core concepts and building blocks of Terraform. This will enable you to build your very first Terraform definition files. 3.3.1 Providers Terraform relies on plugins called providers to interact with Cloud providers, SaaS providers, and other APIs. Each provider adds specific resource types and/or data sources that can be managed by Terraform. For example, the aws provider shown below allows to specify resources related to the AWS Cloud such as S3 Buckets or EC3 Instances. Depending on the provider it is necessary to supply it with specific parameters. The aws provier for example needs the region as well as username and password. If nothing is specified it will automatically pull these information from the AWS CLI and the credentials specified under the directory .aws/config. It is also a best practice to specify the version of the provider, as the providers are usually maintained and updated on a regular basis. provider &quot;aws&quot; { region = &quot;us-east-1&quot; } 3.3.2 Resources A resource is the core building block when working with Terraform. It can be a \"local_file\" such as shown in the example above, or a cloud resource such as an \"aws_instance\" on aws. The resource type is followed by the custom name of the resource in Terraform. Resource definitions are usually specified in the main.tffile. Each customization and setting to a ressource is done within its resource specification. The style convention when writing Terraform code states that the resource name is named in lowercase as well as it should not repeat the resource type. An example can be seen below # Ressource type: aws_instance # Ressource name: my-instance resource &quot;aws_instance&quot; &quot;my-instance&quot; { # resource specification ami = &quot;ami-0ddbdea833a8d2f0d&quot; instance_type = &quot;t2.micro&quot; tags = { Name = &quot;my-instance&quot; ManagedBy = &quot;Terraform&quot; } } 3.3.3 Data Sources Data sources in Terraform are “read-only” resources, meaning that it is possible to get information about existing data sources but not to create or change them. They are usually used to fetch parameters needed to create resources or generally for using parameters elsewhere in Terraform configuration. A typical example is shown below as the \"aws_ami\" data source available in the AWS provider. This data source is used to recover attributes from an existing AMI (Amazon Machine Image). The example creates a data source called \"ubuntu” that queries the AMI registry and returns several attributes related to the located image. data &quot;aws_ami&quot; &quot;ubuntu&quot; { most_recent = true filter { name = &quot;name&quot; values = [&quot;ubuntu/images/hvm-ssd/ubuntu-trusty-14.04-amd64-server-*&quot;] } filter { name = &quot;virtualization-type&quot; values = [&quot;hvm&quot;] } owners = [&quot;099720109477&quot;] # Canonical } Data sources and their attributes can be used in resource definitions by prepending the data prefix to the attribute name. The following example used the \"aws_ami\" data source within an \"aws_instace\" resource. resource &quot;aws_instance&quot; &quot;web&quot; { ami = data.aws_ami.ubuntu.id instance_type = &quot;t2.micro&quot; } 3.4 State A Terraform state stores all details about the resources and data created within a given context. Whenever a resource is create terrafrom stores its identifier in the statefile terraform.tfstate. Providing information about already existing resources is the primary purpose of the statefile. Whenever a Terraform script is applied or whenever the resource definitions are modified, Terraform knows what to create, change, or delete based on the existing entries within the statefile. Everything specified and provisioned within Terraform will be stored in the statefile. This should be kept in mind and detain to store sensitive information such as initial passwords. Terraform uses the concept of a backend to store and retrieve its statefile. The default backend is the local backend which means to store the statefile in the project’s root folder. However, we can also configure an alternative (remote) backend to store it elsewhere. The backend can be declared within a terraform block in the project files. The given example stores the statefile in an AWS S3 Bucket callen some-bucket. Keep in mind this needs access to an AWS account and also needs the AWS provider of terraform. terraform { backend &quot;s3&quot; { bucket = &quot;some-bucket&quot; key = &quot;some-storage-key&quot; region = &quot;us-east-1&quot; } } 3.5 Modules A Terraform module allows to reuse resources in multiple places throughout the project. They act as a container to package resource configurations. Much like in standard programming languages, Terraform code can be organized across multiple files and packages instead of having one single file containing all the code. Wrapping code into a module not only allows to reuse it throughout the project, but also in different environments, for example when deploying a dev and a prod infrastructure. Both environments can reuse code from the same module, just with different settings. A Terraform module is build as a directory containing one or more resource definition files. Basically, when putting all our code in a single directory, we already have a module. This is exactly what we did in our previous examples. However, terraform does not include subdirectories on its own. Subdirectories must be called explicitly using a terraform moduleparameter. The example below references a module located in a ./network subdirectory and passes two parameters to it. # main.tf module &quot;network&quot; { source = &quot;./networking&quot; create_public_ip = true environment = &quot;prod&quot; } Each module consists of a similar file structure as the root directory. This includes a main.tf where all resources are specified, as well as files for different data sources such as variables.tf and outputs.tf. However, providers are usually configured only in the root module and are not reused in modules. Note that there are different approaches on where to specify the providers. They are either specified in the main.tf or a separate providers.tf. It does not make a difference for Terraform as it does not distinguish between the resource definition files. It is merely a strategy to keep code and project in a clean and consistent structure. root │ main.tf │ variables.tf │ outputs.tf │ └── networking │ main.tf │ variables.tf │ outputs.tf 3.5.1 Input Variables Each module can have multiple Input Variables. Input Variables serve as parameters for a Terraform module so users can customize behavior without editing the source. In the previous example of importing a network module, there have been two input variables specified, create_public_ip and environment. Input variables are usually specified in the variables.tf file. # variables.tf variable &quot;instance_name&quot; { type = string default = &quot;awesome-instance&quot; description = &quot;Name of the aws instance to be created&quot; } Each variable has a type (e.g. string, map, set, boolen) and may have a default value and description. Any variable that has no default must be supplied with a value when calling the module reference. This means that variables defined at the root module need values assigned to as a requirement so Terraform will not fail. This can be done by different resources, for example a variable’s default value via the command line using the terraform apply -var=\"variable=value\"option via environment variables starting with TF_VAR_; Terraform will check them automatically a .tfvars file where the variable values are specified; Terraform can load variable definitions from these files automatically (please check online resources for further insights) Variables can be used in expressions using the var.prefix such as shown in below example. We use the resource configuration of the previous example to create an aws_instance but this time its name is provided by an input variable. # main.tf resource &quot;aws_instance&quot; &quot;awesome-instance&quot; { ami = &quot;ami-0ddbdea833a8d2f0d&quot; instance_type = &quot;t2.micro&quot; tags = { Name = var.instance_name } } 3.5.2 Output Variables Similar to Input variables, a terraform module has output variables. As their name states, output variables return values of a Terraform module and are denoted in the outputs.tf file as expected. A module’s consumer has no access to any resources or data created within the module itself. However, sometimes a modules attrivutes are needed for another module or resource. Output variables address this issue by exposing a defined subset of the created resources. The example below defines an output value instance_address containing the IP address of an EC2 instance the we create with a module. Any module that reference this module can use the instance_address value by referencing it via module.module_name.instance_address # outputs.tf output &quot;instance_address&quot; { value = aws_instance.awesome-instance.private_ip description = &quot;Web server&#39;s private IP address&quot; } outputs 3.5.3 Local Variables Additionally to Input variables and output variables a module provides the use of local variables. Local values are basically just a convenience feature to assign a shorter name to an expression and work like standard variables. This means theor scope is also limited to the module they are declared in. Using local variables reduces code repetitions which can be especially valuable when dealing with output variables from a module. # main.tf locals { vpc_id = module.network.vpc_id } module &quot;network&quot; { source = &quot;./network&quot; } module &quot;service1&quot; { source = &quot;./service1&quot; vpc_id = local.vpc_id } module &quot;service2&quot; { source = &quot;./service2&quot; vpc_id = local.vpc_id } 3.6 Additional tips &amp; tricks Of course there is much more to Terraform than these small examples can provide. Yet there are also some contrains when working with Terraform or declarative languages in general. Typically they do not have for-loops or other traditional procedural logic built into the language to repeat a piece of logic or conditional if-statements to configure reasources on demand. However, there are ways there are some ways to deal with this issue and to create multiple respurces without copy and paste. Terraform comes with different looping constructs, each used slightly different. The count and for_each meta arguments enable us to create multiple instances of a resource. 3.6.1 count Count can be used to loop over any resource and module. Every Terraform resource has a meta-parameter count one can use. Count is the simplest, and most limited iteration construct and all it does is to define how many copies to create of a resource. When creating multiple instance with one specification, the problem is that each instance must have a unique name, otherwise Terraform would cause an error. Therefore we need to index the meta-parameter just like doing it in a for-loop to give each resource a unique name. The example below shows how to do this on an AWS IAM user. resource &quot;aws_iam_user&quot; &quot;example&quot; { count = 2 name = &quot;neo.${count.index}&quot; } count variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;snake&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.user_names) # returns the number of items in the given array name = var.user_names[count.index] } count list After using count on a resource it becomes an array of resources rather than one single resource. The same hold when using count on modules. When adding count to a module it turns it into an array of modules. This can round into problems because the way Terraform identifies each resource within the array is by its index. Now, when removing an item from the middle of the array, all items after it shift one index back. This will result in Terraform deleting every resource after that item and then re-creating these resources again from scratch So after running terraform plan with just three names, Terraform’s internal representation will look like this: variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.user_names) # returns the number of items in the given array name = var.user_names[count.index] } count index deletion Count as conditional Count can also be used as a form of a conditional if statement. This is possible as Terraform supports conditional expressions. If count is set to one 1, one copy of that resource is created; if set to 0, the resource is not created at all. Writing this as a conditional expression could look something like the follow, where var.enable_autoscaling is a boolean variable either set to True or False. resource &quot;example-1&quot; &quot;example&quot; { count = var.enable_autoscaling ? 1 : 0 name = var.user_names[count.index] } 3.6.2 for-each The for_each expression allows to loop over lists, sets, and maps to create multiple copies of a resource just like the count meta. The main difference between them is that count expects a non-negative number, whereas for_each only accepts a list or map of values. Using the same example as above it would look like this: variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;snake&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { for_each = toset(var.user_names) name = each.value } output &quot;all_users&quot; { value = aws_iam_user.example } &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD ======= ab16f8b8832cb3f4c8851aa0fafa06fff554647a Using a map of resource with the for_each meta rather than an array of resources as with count has the benefit to remove items from the middle of the collection safely and without re-creating the resources following the deleted item. Of course, the same can also be done for modules. module &quot;users&quot; { source = &quot;./iam-user&quot; for_each = toset(var.user_names) user_name = each.value } 3.6.3 for Terraform also offers a similar functionality as python list comprehension in the form of a for expression. This should not be confused with the for-each expression seen above. The basic syntax is shown below to convert the list of names of previous examples in var.names to uppercase: output &quot;upper_names&quot; { value = [for name in var.names : upper(name)] } output &quot;short_upper_names&quot; { value = [for name in var.names : upper(name) if length(name) &lt; 5] } for on list Using for to loop over lists and maps within a string can be used similarly. This allows us to use control statements directly withing strings using a syntax similar to string interpolation. output &quot;for_directive&quot; { value = &quot;%{ for name in var.names }${name}, %{ endfor }&quot; } Bild nicht gefunden: images/02-Terraform/additionals_for_on_string.png 3.6.4 Workspaces Terraform workspaces allow us to keep multiple state files for the same project. When we run Terraform for the first time in a project, the generated state file will go into the default workspace. Later, we can create a new workspace with the terraform workspace new command, optionally supplying an existing state file as a parameter. workspaces 3.7 Putting it together Building a VPC with EC2 &amp; S3 Finally, we want to put everything together and provision our own cloud infrastructure using Terraform. We will create a AWS Virtual private Cloud with EC2 Instances running on it, and S3 Buckets attached to the them. We will use for_each and count to create multiple instaces. The Terraform infrastructure is separated into three modules VPS, EC2, and S3. root │ main.tf │ variables.tf │ outputs.tf │ └── networking │ │ main.tf │ │ variables.tf │ │ outputs.tf │ └── ec2 │ │ main.tf │ │ variables.tf │ │ outputs.tf │ └── s3 │ main.tf │ variables.tf │ outputs.tf 3.7.1 Root 3.7.1.1 main.tf 3.7.1.2 variables.tf 3.7.1.3 outputs.tf 3.7.1.4 providers.tf 3.7.2 Networking 3.7.2.1 main.tf 3.7.2.2 variables.tf 3.7.2.3 outputs.tf 3.7.3 EC2 3.7.3.1 main.tf 3.7.3.2 variables.tf 3.7.3.3 outputs.tf 3.7.4 S3 3.7.4.1 main.tf 3.7.4.2 variables.tf 3.7.4.3 outputs.tf 3.8 TODO when is the statefile created? make images for usage steps make images for loops what is terraform {} disclaimer for aws cloud schreiben dass das wissen vorher benötigt wird disclaimer of cloud und netwerke schreiben check yamls insert images for workspace Korrektur lesen ab output variables. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
