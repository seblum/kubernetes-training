[["index.html", "MLOps Engineering Preamble Topics", " MLOps Engineering Sebastian Blum 2022-12-24 Preamble This document is written during my journey in the realm of MLOps. It is therefore in a state of continuous development. Topics The overall aim is to build and create a MLOps architecture based on Airflow running on AWS EKS. Ideally, this architecture is create using terraform. Model tracking might be done using MLFlow, Data tracking using DVC. Further mentioned might be best practices in software development, CI/CD, Docker, and pipelines. I might also include a small Data Science use case utilizing the Airflow Cluster we built. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction This project started out of an interest in multiple domains. At first, I was working on a Kubeflow platform at the time and haven’t had much experience in the realm of MLOps. I was starting with K8s and Terraform and was interested to dig deeper. I did so by teaching myself and I needed a project. What better also to do than to build “my own” MLOps plattform. I used Airflow since it is widely uses for workflow management and I also wanted to use it from the perspective of a data scientist, meaning to actually build some pipelines with it. This idea of extending the project to actually have a running use case expanded this work about including MLFlow for model tracking and DVC for data version control. A work in progress This project / blog / tutorial / whatever this is or will be startet by explaining the concept of Kubernetes. The plan is to continuously update it by further sections. Since there is no deadline, there is no timeline, and I am also not sure whether there will exist something final to be honest. "],["terraform.html", "Chapter 2 Terraform 2.1 Prerequisites 2.2 Basic usage 2.3 Core Concepts 2.4 State 2.5 Modules 2.6 Additional tips &amp; tricks 2.7 Putting it together", " Chapter 2 Terraform Terraform is an open-source, declarative programming language developed by HashiCorp and allows to create both on-prem and cloud resources in the form of writing code. This is also know as Infrastructure as Code (IAC). There are several IaC tools in the market today and Terraform is only one of them. Yet it is a well known and established tool and during the course of this project we only focus on this. Hashicorp describes that: Terraform is an infrastructure as code (IaC) tool that allows you to build, change, and version infrastructure safely and efficiently. This includes both low-level components like compute instances, storage, and networking, as well as high-level components like DNS entries and SaaS features. (hashicorpwebsite?) (https://www.terraform.io/docs) This means that users are able to manage and provision an entire IT infrastructure using machine-readable definition files and thus allowing faster execution when configuring infrastructure, as well as enableing full traceability of changes. Terraform comes with several hundred different providers that can be used to provision infrastructure, such as Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, DataDog, etc. The given chapter introduces the concepts &amp; usage of Terraform which will be needed to create the introduced MLOps Airflow deployment in an automated and tracable way. We will learn how to use Terraform to provision ressources as well as to structure a Terraform Project. 2.1 Prerequisites To be able to follow this tutorial, one needs to have the AWS CLI installed as well as the AWS credentials set up. Needles to say an AWS accounts needs to be present. It is also recommended to have basic knowledge of the AWS Cloud as this tutorials used the AWS infrastructure to provision cloud resources. The attached resource definitions are specified to the AWS region eu-central-1. It might be necessary to change accordingly if you are set in another region. Further, Terraform itself needs to be installed. Please refer to the corresponding sites. The scripts are run under Terraform version v1.2.4. Later releases might have breaking changes. One can check its installation via terraform version. 2.2 Basic usage A Terraform project is basically just a set of files in a directory containing resource definitions of cloud ressources to be created. Those Terraform files, denoted by the ending .tf, use Terraform’s configuration language to define the specified resources. In the following example there are two definitions made: a provider and a resource. Later in this chapter we will dive deeper in the structure of the language. For now, we only need to know this script is creating a file called hello.txt that includes the text \"Hello, Terraform\". It’s our Terraform version of Hello World! provider &quot;local&quot; { version = &quot;~&gt; 1.4&quot; } resource &quot;local_file&quot; &quot;hello&quot; { content = &quot;Hello, Terraform&quot; filename = &quot;hello.txt&quot; } 2.2.1 terraform init When a project is run for the first time the terraform project needs to be initialized. This is done via the terraform init command. Terraform scans the project files in this step and downloads any required providers needed (more details to providers in a following section). In the given example this is the local procider. # Initializes the working directory which consists of all the configuration files terraform init Terraform init 2.2.2 terraform validate The terraform validate command checks the code for syntax errors. This is optional yet a way to handle initial errors or minor careless mistakes # Validates the configuration files in a directory terraform validate Terraform validate 2.2.3 terraform plan The terraform plan command verifies what action Terraform will perform and what resources will be created. This step is basically a dry run of the code to be executed. It also returns the provided values and some permission attributes which have been set. # Creates an execution plan to reach a desired state of the infrastructure terraform plan Terraform plan 2.2.4 terraform apply The command Terraform apply creates the resource specified in the .tf files. Initially, the same output as in the terraform plan step is shown (hence its dry run). The output further states which resources are added, which will be changed, and which resources will be destroyed. After confirming the actions the resource creation will be executed. Modifications to previously deployed ressources can be implemented by using terraform apply again. The output will denote that there are resources to change. # Provision the changes in the infrastructure as defined in the plan terraform apply Terraform apply 2.2.5 terraform destroy To destoy all created ressouces and to delete everything we did before, there is a terraform destroy command. # Deletes all the old infrastructure resources terraform destroy Terraform destroy 2.3 Core Concepts The following section will explain the core concepts and building blocks of Terraform. This will enable you to build your very first Terraform definition files. 2.3.1 Providers Terraform relies on plugins called providers to interact with Cloud providers, SaaS providers, and other APIs. Each provider adds specific resource types and/or data sources that can be managed by Terraform. For example, the aws provider shown below allows to specify resources related to the AWS Cloud such as S3 Buckets or EC3 Instances. Depending on the provider it is necessary to supply it with specific parameters. The aws provier for example needs the region as well as username and password. If nothing is specified it will automatically pull these information from the AWS CLI and the credentials specified under the directory .aws/config. It is also a best practice to specify the version of the provider, as the providers are usually maintained and updated on a regular basis. provider &quot;aws&quot; { region = &quot;us-east-1&quot; } 2.3.2 Resources A resource is the core building block when working with Terraform. It can be a \"local_file\" such as shown in the example above, or a cloud resource such as an \"aws_instance\" on aws. The resource type is followed by the custom name of the resource in Terraform. Resource definitions are usually specified in the main.tffile. Each customization and setting to a ressource is done within its resource specification. The style convention when writing Terraform code states that the resource name is named in lowercase as well as it should not repeat the resource type. An example can be seen below # Ressource type: aws_instance # Ressource name: my-instance resource &quot;aws_instance&quot; &quot;my-instance&quot; { # resource specification ami = &quot;ami-0ddbdea833a8d2f0d&quot; instance_type = &quot;t2.micro&quot; tags = { Name = &quot;my-instance&quot; ManagedBy = &quot;Terraform&quot; } } 2.3.3 Data Sources Data sources in Terraform are “read-only” resources, meaning that it is possible to get information about existing data sources but not to create or change them. They are usually used to fetch parameters needed to create resources or generally for using parameters elsewhere in Terraform configuration. A typical example is shown below as the \"aws_ami\" data source available in the AWS provider. This data source is used to recover attributes from an existing AMI (Amazon Machine Image). The example creates a data source called \"ubuntu” that queries the AMI registry and returns several attributes related to the located image. data &quot;aws_ami&quot; &quot;ubuntu&quot; { most_recent = true filter { name = &quot;name&quot; values = [&quot;ubuntu/images/hvm-ssd/ubuntu-trusty-14.04-amd64-server-*&quot;] } filter { name = &quot;virtualization-type&quot; values = [&quot;hvm&quot;] } owners = [&quot;099720109477&quot;] # Canonical } Data sources and their attributes can be used in resource definitions by prepending the data prefix to the attribute name. The following example used the \"aws_ami\" data source within an \"aws_instace\" resource. resource &quot;aws_instance&quot; &quot;web&quot; { ami = data.aws_ami.ubuntu.id instance_type = &quot;t2.micro&quot; } 2.4 State A Terraform state stores all details about the resources and data created within a given context. Whenever a resource is create terrafrom stores its identifier in the statefile terraform.tfstate. Providing information about already existing resources is the primary purpose of the statefile. Whenever a Terraform script is applied or whenever the resource definitions are modified, Terraform knows what to create, change, or delete based on the existing entries within the statefile. Everything specified and provisioned within Terraform will be stored in the statefile. This should be kept in mind and detain to store sensitive information such as initial passwords. Terraform uses the concept of a backend to store and retrieve its statefile. The default backend is the local backend which means to store the statefile in the project’s root folder. However, we can also configure an alternative (remote) backend to store it elsewhere. The backend can be declared within a terraform block in the project files. The given example stores the statefile in an AWS S3 Bucket callen some-bucket. Keep in mind this needs access to an AWS account and also needs the AWS provider of terraform. terraform { backend &quot;s3&quot; { bucket = &quot;some-bucket&quot; key = &quot;some-storage-key&quot; region = &quot;us-east-1&quot; } } 2.5 Modules A Terraform module allows to reuse resources in multiple places throughout the project. They act as a container to package resource configurations. Much like in standard programming languages, Terraform code can be organized across multiple files and packages instead of having one single file containing all the code. Wrapping code into a module not only allows to reuse it throughout the project, but also in different environments, for example when deploying a dev and a prod infrastructure. Both environments can reuse code from the same module, just with different settings. A Terraform module is build as a directory containing one or more resource definition files. Basically, when putting all our code in a single directory, we already have a module. This is exactly what we did in our previous examples. However, terraform does not include subdirectories on its own. Subdirectories must be called explicitly using a terraform moduleparameter. The example below references a module located in a ./network subdirectory and passes two parameters to it. # main.tf module &quot;network&quot; { source = &quot;./networking&quot; create_public_ip = true environment = &quot;prod&quot; } Each module consists of a similar file structure as the root directory. This includes a main.tf where all resources are specified, as well as files for different data sources such as variables.tf and outputs.tf. However, providers are usually configured only in the root module and are not reused in modules. Note that there are different approaches on where to specify the providers. They are either specified in the main.tf or a separate providers.tf. It does not make a difference for Terraform as it does not distinguish between the resource definition files. It is merely a strategy to keep code and project in a clean and consistent structure. root │ main.tf │ variables.tf │ outputs.tf │ └── networking │ main.tf │ variables.tf │ outputs.tf 2.5.1 Input Variables Each module can have multiple Input Variables. Input Variables serve as parameters for a Terraform module so users can customize behavior without editing the source. In the previous example of importing a network module, there have been two input variables specified, create_public_ip and environment. Input variables are usually specified in the variables.tf file. # variables.tf variable &quot;instance_name&quot; { type = string default = &quot;awesome-instance&quot; description = &quot;Name of the aws instance to be created&quot; } Each variable has a type (e.g. string, map, set, boolen) and may have a default value and description. Any variable that has no default must be supplied with a value when calling the module reference. This means that variables defined at the root module need values assigned to as a requirement so Terraform will not fail. This can be done by different resources, for example a variable’s default value via the command line using the terraform apply -var=\"variable=value\"option via environment variables starting with TF_VAR_; Terraform will check them automatically a .tfvars file where the variable values are specified; Terraform can load variable definitions from these files automatically (please check online resources for further insights) Variables can be used in expressions using the var.prefix such as shown in below example. We use the resource configuration of the previous example to create an aws_instance but this time its name is provided by an input variable. # main.tf resource &quot;aws_instance&quot; &quot;awesome-instance&quot; { ami = &quot;ami-0ddbdea833a8d2f0d&quot; instance_type = &quot;t2.micro&quot; tags = { Name = var.instance_name } } 2.5.2 Output Variables Similar to Input variables, a terraform module has output variables. As their name states, output variables return values of a Terraform module and are denoted in the outputs.tf file as expected. A module’s consumer has no access to any resources or data created within the module itself. However, sometimes a modules attrivutes are needed for another module or resource. Output variables address this issue by exposing a defined subset of the created resources. The example below defines an output value instance_address containing the IP address of an EC2 instance the we create with a module. Any module that reference this module can use the instance_address value by referencing it via module.module_name.instance_address # outputs.tf output &quot;instance_address&quot; { value = aws_instance.awesome-instance.private_ip description = &quot;Web server&#39;s private IP address&quot; } outputs 2.5.3 Local Variables Additionally to Input variables and output variables a module provides the use of local variables. Local values are basically just a convenience feature to assign a shorter name to an expression and work like standard variables. This means theor scope is also limited to the module they are declared in. Using local variables reduces code repetitions which can be especially valuable when dealing with output variables from a module. # main.tf locals { vpc_id = module.network.vpc_id } module &quot;network&quot; { source = &quot;./network&quot; } module &quot;service1&quot; { source = &quot;./service1&quot; vpc_id = local.vpc_id } module &quot;service2&quot; { source = &quot;./service2&quot; vpc_id = local.vpc_id } 2.6 Additional tips &amp; tricks Of course there is much more to Terraform than these small examples can provide. Yet there are also some contrains when working with Terraform or declarative languages in general. Typically they do not have for-loops or other traditional procedural logic built into the language to repeat a piece of logic or conditional if-statements to configure reasources on demand. However, there are ways there are some ways to deal with this issue and to create multiple respurces without copy and paste. Terraform comes with different looping constructs, each used slightly different. The count and for_each meta arguments enable us to create multiple instances of a resource. 2.6.1 count Count can be used to loop over any resource and module. Every Terraform resource has a meta-parameter count one can use. Count is the simplest, and most limited iteration construct and all it does is to define how many copies to create of a resource. When creating multiple instance with one specification, the problem is that each instance must have a unique name, otherwise Terraform would cause an error. Therefore we need to index the meta-parameter just like doing it in a for-loop to give each resource a unique name. The example below shows how to do this on an AWS IAM user. resource &quot;aws_iam_user&quot; &quot;example&quot; { count = 2 name = &quot;neo.${count.index}&quot; } count variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;snake&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.user_names) # returns the number of items in the given array name = var.user_names[count.index] } count list After using count on a resource it becomes an array of resources rather than one single resource. The same hold when using count on modules. When adding count to a module it turns it into an array of modules. This can round into problems because the way Terraform identifies each resource within the array is by its index. Now, when removing an item from the middle of the array, all items after it shift one index back. This will result in Terraform deleting every resource after that item and then re-creating these resources again from scratch So after running terraform plan with just three names, Terraform’s internal representation will look like this: variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.user_names) # returns the number of items in the given array name = var.user_names[count.index] } count index deletion Count as conditional Count can also be used as a form of a conditional if statement. This is possible as Terraform supports conditional expressions. If count is set to one 1, one copy of that resource is created; if set to 0, the resource is not created at all. Writing this as a conditional expression could look something like the follow, where var.enable_autoscaling is a boolean variable either set to True or False. resource &quot;example-1&quot; &quot;example&quot; { count = var.enable_autoscaling ? 1 : 0 name = var.user_names[count.index] } 2.6.2 for-each The for_each expression allows to loop over lists, sets, and maps to create multiple copies of a resource just like the count meta. The main difference between them is that count expects a non-negative number, whereas for_each only accepts a list or map of values. Using the same example as above it would look like this: variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;snake&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { for_each = toset(var.user_names) name = each.value } output &quot;all_users&quot; { value = aws_iam_user.example } for each on list Using a map of resource with the for_each meta rather than an array of resources as with count has the benefit to remove items from the middle of the collection safely and without re-creating the resources following the deleted item. Of course, the same can also be done for modules. module &quot;users&quot; { source = &quot;./iam-user&quot; for_each = toset(var.user_names) user_name = each.value } 2.6.3 for Terraform also offers a similar functionality as python list comprehension in the form of a for expression. This should not be confused with the for-each expression seen above. The basic syntax is shown below to convert the list of names of previous examples in var.names to uppercase: output &quot;upper_names&quot; { value = [for name in var.names : upper(name)] } output &quot;short_upper_names&quot; { value = [for name in var.names : upper(name) if length(name) &lt; 5] } for on list Using for to loop over lists and maps within a string can be used similarly. This allows us to use control statements directly withing strings using a syntax similar to string interpolation. output &quot;for_directive&quot; { value = &quot;%{ for name in var.names }${name}, %{ endfor }&quot; } Bild nicht gefunden: images/02-Terraform/additionals_for_on_string.png 2.6.4 Workspaces Terraform workspaces allow us to keep multiple state files for the same project. When we run Terraform for the first time in a project, the generated state file will go into the default workspace. Later, we can create a new workspace with the terraform workspace new command, optionally supplying an existing state file as a parameter. workspaces 2.7 Putting it together Building a VPC with EC2 &amp; S3 Finally, we want to put everything together and provision our own cloud infrastructure using Terraform. We will create a AWS Virtual private Cloud with EC2 Instances running on it, and S3 Buckets attached to the them. We will use for_each and count to create multiple instaces. The Terraform infrastructure is separated into three modules VPS, EC2, and S3. root │ main.tf │ variables.tf │ outputs.tf │ └── networking │ │ main.tf │ │ variables.tf │ │ outputs.tf │ └── ec2 │ │ main.tf │ │ variables.tf │ │ outputs.tf │ └── s3 │ main.tf │ variables.tf │ outputs.tf "],["kubernetes.html", "Chapter 3 Kubernetes 3.1 Prerequisites 3.2 Nodes 3.3 Pods 3.4 Deployments 3.5 Services 3.6 Labels, Selectors and Annotations 3.7 Namespaces 3.8 Service Discovery 3.9 Volume &amp; Storage 3.10 ConfigMaps 3.11 Secrets 3.12 Health Checks 3.13 Resource Management 3.14 Daemon Sets 3.15 StatefulSets 3.16 Jobs &amp; Cron Jobs", " Chapter 3 Kubernetes What is Kubernetes? Kubernetes (short: K8s) is greek and means pilot. K8s is an applications orchestrator that originated from Google and is open source. Beeing an application orchestrator, K8s deploys and manages application containers. It scales up and down so called Pod (A Pod manages a container) as needed and allows for zero downtime as well as the possibility of rollbacks. Thus, the meaning of pilot relates to its functionining in piloting containers. 3.1 Prerequisites As a prerequisites to go through this tutorial and implement the scripts one has to have Docker installed, Kubectl to interact via the command line with K8s, as well as Minikube to run a K8s cluster on a local machine. Please refer to the corresponding sites. 3.2 Nodes A K8s Cluster usually consistes of a set of nodes. A Node can hereby be a virtual machine (VM) in the cloud, e.g. AWS, Azure, or GCP, or a node can also be of course a physical on-premise instance. K8s distinguishes the nodes between a master node and worker nodes. The master node is basically the brain of the cluster. This is where everything is organized, handled, and managed. In comparison, a worker nodes is where the heavy lifting is happening, such as running application. Both, master and worker nodes communicate with each other via the so called kubelet. One cluster has only one master node and usually multiple worker nodes. K8s Cluster 3.2.1 Master &amp; Control Plane To be able to work as the brain of the cluster, the master node contains a controll plane made of several components, each of which serves a different function. Scheduler Cluster Store API Server Controller Manager Cloud Controller Manager Master Node 3.2.1.0.1 API Server The api servers serves as the connection between the frontend and the K8s controll plane. All communications, external and interal, go through it. Frontend to Kubernetes Controll Plane. It exposes a restful api on port 443 to allow communication, as well as performes authentication and authorization checks. Whenever we perform something on the K8s cluster, e.g. using a command like kubectl apply -f &lt;file.yaml&gt;, we communicate with the api server (what we do here is shown in the section about pods). 3.2.1.0.2 Cluster store The cluster store stores the configuration and state of the entire cluster. It is a distributed key-value data store and the single source of truth database of the cluster. As in the example before, whenever we apply a command like kubectl apply -f &lt;file.yaml&gt;, the file is stored on the cluster store to store the configuration. 3.2.1.0.3 Scheduler The scheduler of the control plane watches for new workloads/pods and assigns them to a node based on several scheduling factors. These factors include whether a node is healthy, whether there are enough resources available, whether the port is available, or according to affinity or anti-affinity rules. 3.2.1.0.4 Controller manager The controller manager is a daemon that manages the control loop. This means, the controller manager is basically a controller of other controllers. Each controller watches the api server for changes to their state. Whenever a current state of a controller does not match the desired state, the control manager administers the changes. These controllers are for example replicasets, endpoints, namespace, or service accounts. There is also the cloud controller manager, which is responsible to interact with the underlying cloud infrastructure. 3.2.2 Worker Nodes There worker nodes are the part of the cluster where the heavy lifting happens. Their VMs (or physical machines) often run linux and thus provide a suitable and running environment for each application. Worker Node A worker node consists of three main components. Kubelet Container runtime Kube proxy 3.2.2.0.1 Kubelet The kubelet is the main agent of a worker node that runs on every single node. It receives pod definitions from the API server and interacts with the container runtime to run containers associated with the corresponding pods. The kubelet also reports node and pod state to the master node. 3.2.2.0.2 Container runtime The container runtime is responsible to pull images from container registries, e.g. from DockerHub, or AWS ECR, as well as starting, and stoping containers. The container runtime thus abstracts container management for K8s and runs a Container Runtime Interface (CRI) within. 3.2.2.0.3 Kube-proxy The kube-proxy runs on every node via a DaemondSet. It is responsible for network communications by maintaining network rules to allow communication to pods from inside and outside the cluster. If two pods want to talk to each other, the kube-proxy handles their communication. Each node of the cluster gets its own unique IP adress. The kube-proxy handels the local cluster networking as well as routing the network traffic to a load balanced service. 3.3 Pods A pod is the smallest deployable unit in K8s (In contrast to K8s, the smallest deployable unit for docker are containers.). Therefore, a pod is a running process that runs on a clusters’ node. Within a pod, there is always one main container representing the application (in whatever language written, e.g. JS, Python, Go). There also may or may not be init containers, and/or side containers. Init containers are containers that are executed before the main container. Side containers are containers that support the main containers, e.g. a container that acts as a proxy to your main container. There may be volumes specified within a pod, which enables containers to share and store data. Pod The containers running within a pod communicate with each other using localhost and whatever port they expose. The port itself has a unique ip adress, which enables outward communication between pods. The problem is that a pods does not have a long lifetime (also denoted as ephemeral) and is disposable. This suggests to never create a pod on its own within a K8s cluster and to rather use controllers instead to deploy and maintain a pods lifecycle, e.g. controllers like Deployments. In general, managing ressources in K8s is done via an imperative or declarative management. Imperative &amp; Declarative Management Imperative management means managing the pods via a CLI and specifying all necessary parameters using it. It is good for learning, troubleshooting, and experimenting on the cluster. In contrast, the declarative approach uses a yaml file to state all necessary parameters needed for a ressource, and then using the CLI to administer the changes. The declarative approach is reproducible, which means the same configuration can be applied in different environments (prod/dev). This is best practice to use when building a cluster. As stated, this differentiation does not only hold for pods, but for all ressources within a cluster. Imperative Management # start a pod by specifying the pods name, # the container image to run, and the port exposed kubectl run &lt;pod-name&gt; --image=&quot;&lt;image-name&gt;&quot; --port=80 # run following command to test the pod specified # It will forward to localhost:8080 kubectl port-forward pod/&lt;pod-name&gt; 8080:80 Declarative Management / Configuration Declarative configuration is done using a yaml format, which works on key-value pairs. # pod.yaml apiVersion: v1 # specify which kind of configuration this is # lets configure a simple pod kind: Pod # metadata will be explained later on in more detail metadata: name: hello-world labels: name: hello-world spec: # remember: a pod is a selection of one or more containers # we could therefore specify multiple containers containers: # specify the container name - name: hello # specify which container image should be pulled image: seblum/mlops-public:cat-v1 # ressource configurations will be handled later as well ressources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # specify the port on which the container should be exposed # similar to the imperative approach ports: ContainerPorts: 80 Appyl this declarative configuration using the following kubectl command via the CLI. kubectl apply -f &quot;file-name.yaml&quot; # similar to before, run following to test your pod on localhost:8080 kubectl port-forward pod/&lt;pod-name&gt; 8080:80 3.3.0.1 Kubectl One word to interacting with the cluster using the CLI. In general, kubectl is used to interact with the K8s cluster. This allows to run and apply pod configurations such as seen before, as well as the already shown port forwarding. We can also inspect the cluster, see what ressources are running on which nodes, see their configurations, and watch their logs. A small selection of commands are shown below. # forward the pods to localhost:8080 kubectl port-forward &lt;ressource&gt;/&lt;pod-name&gt; 8080:80 # show all pods currently running in the cluster kubectl get pods # delete a specific pod kubectl delete pods &lt;pod-name&gt; # delete a previously applied configuration kubectl delete -f &lt;file-name&gt; # show all instances of a specific resource running on the cluster # (nodes, pods, deployments, statefulsets, etc) kubectl get &lt;resource&gt; # describe and show specific settings of a pods kubectl describe pod &lt;pod-name&gt; 3.4 Deployments We should never deploy a pod using kind:Pod. Pods are ephemeral, so never treat them like pets. They do not heal on their own and if a pod is terminated, it does not restart by themselves. This is dangerous as there should always be one replica running of and application. This demands for a mechanism for the application to self heal and this is exactly where Deployments and ReplicaSets come in to solve the problem. In general, Pods should be managed through Deployments. The purpose of a Deployment is to facilitate software deployment. They manage releases of a new application, they provide zero downtime of an application and create a ReplicaSet behind the scenes. K8s will take care of the full deployment process when applying a Deployment, even if we want to make a rolling update to change the version. Replicasets A ReplicaSet makes sure that a desired number of pods is running. When looking at Pods’ name of a Deployment, it usually has a random string attached. This is because a deployment can have multiple replicas and the random suffix ensures a different name after all. The way ReplicaSets work is that they implement a background control loop that checks the desired number of pods are always present on the cluster. We can specify the number of replicas by creating a yaml-file of a Deployment, similar to previous specifications done to a Pod. As a reminder, the Deployment can be applied using the kubectl apply -f as well. # deployment.yaml apiVersion: apps/v1 # specify that we want a deployment kind: Deployment metadata: name: hello-world spec: # specify number of replicas replicas: 3 selector: matchLabels: app: hello-world template: metadata: labels: app: hello-world spec: containers: - name: hello-world image: seblum/mlops-public:cat-v1 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 3.4.0.1 Rolling updates A rolling update means that a new version of the application is rolled out. In general, a basic deployments strategy will delete every single pod before it creates a new version. This is very dangerous since there is downtime. The preferred strategy is to perform a rolling update. This ensures keeping traffic to the previous version until the new one is up and running and alternates traffic until the new version is fully healthy. K8s perfoms the update of an application while the application is up and running. For example, when there are two replicasets running, one with version v1 and one with v2, K8s performs the update such that it only scales v1 down when v2 is already up and running and the traffic has been redirected to v2 as well. How do the deployments need to be configured for that? # deployment_rolling-update.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: replicas: 3 # a few new things have been added here revisionHistoryLimit: 20 # specify the deployments strategy strategy: type: RollingUpdate rollingUpdate: # only one pod at a time to become unavailable # in our case scaling down of v1 maxUnavailable: 1 # never have more than one pod above the mentioned replicas # with three replicas, there will never be 5 pods running during a rollout maxSurge: 1 selector: matchLabels: app: hello-world template: metadata: labels: app: hello-world annotations: # just an annotation the get the version change kubernetes.io/change-cause: &quot;seblum/mlops-public:cat-v2&quot; spec: containers: - name: hello-world # only change specification of the image to v2, k8s performs the update itself image: seblum/mlops-public:cat-v2 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 The changes can be applied as well using kubectl apply -f \"file-name.yaml\". Good to know, K8s is not deleting the replicasets of previous versions. They are still stored on the Cluster Store. The spec.revisionHistory: &lt;?&gt; state in the yaml denoted this. The last ten previous versions are stored on default. However, it doesn’t really make sense to keep more such for example in the previous yaml where there are the last 20 versions specified. This enables to perform Rollbacks to previous versions. To not have discrepancies in a cluster, one should always update using the declarative approach. Below stated are a number of commands that trigger and help with a rollback or with rollouts in general. # check the status of the current deployment process kubectl rollout status deployments &lt;name&gt; # pause the rollout of the deployment. kubectl rollout pause # check the rollout history of a specific deployment kubectl rollout history deployment &lt;name&gt; # undo the rollout of a deployment and switch to previous version kubectl rollout undo deployment &lt;name&gt; # goes back to a specific revision # there is a limit of history and k8s only keeps 10 previous versions kubectl rollout undo deployment &lt;name&gt; --to-revision= 3.5 Services To overall question when deploying an application is how we can access it. Each individual Pod has its own IP address. Thereby, the client can access this Pod via its IP address. Previously, we have made the app available via kubectl port-forward by forwarding the Pods’ IP to localhost. However, should only be done for testing purposes and is not a reliable and stable way to enable access. Since Pods are ephemeral, the client cannot rely on the ip address alone. For example, if an application is scaled up or down, there will be new IPs associated with each new Pod. Services Instead, Services should be used. A service is an abstract way to expose an application as a network service. The service can connect access to a pod via an interal reference, so a change of the Pods IP will not interfere with its accessibility. The service itself has a stable IP adress, a stable DNS name, and a stable port. This allows for a reliable and stable connection from the client to the service, which can then direct the traffic to the pod. There are different types of Services. ClusterIP (Default) NodePort ExternalName LoadBalancer 3.5.0.1 ClusterIP The ClusterIP is the default K8s service. This service type will be chosen if no specific type is selected. The ClusterIP is used for cluster internal access and does not allow for external communication. If one Pod wants to talk to another Pod inside the cluster, it will use ClusterIP to do so. The service will allow and send traffic to any pod that is healthy. 3.5.0.2 NodePort The NodePort service allows to open a static port simultaneously on all nodes. Its range lies between between 30.000/32.767. If a client wants to communicate with a node of the cluster, the client directly communicates with the node via its IP address. When the request reaches the port of the node, the NodePort service handles the request and forwards it to the specifically marked pod. This way, an application running on a pod can be exposed directly on a nodes’ IP under a specific port. You’ll be able to contact the NodePort Service from outside the cluster by requesting &lt;NodeIP&gt;:&lt;NodePort&gt;. Using a NodePort is beneficial for example when a request is sent to a node without a pod. The NodePort service will forward the request to a node which has a healthy associated pod running. However, only having one service specified per port is also a disadvantage. Having one ingress and multiple services is more desireable. The point of running K8s in the cloud is to scale up and down and if the NodeIP address changes, then we have a problem. So we should not aim to access a Node IP directly in the first place If applying below example using the frontend-deployment and the backend-deployment, we can access the frontend using the nodeport. Since using minikube, we can access the service by using minikube service frontend-node --url. Using the given IP adress it is possible to access the frontend using the NodePort service. We can also test the NodePort service when inside of a node. When accessing a node e.g. via minikube ssh, we can run curl localhost:PORT/ inside the node to derive the to derive the website data from the frontend. 3.5.0.3 LoadBalancer Loadbalancers are a standard way of exposing applications to the extern, for example the internet. Loadbalancers automatically distribute incoming traffic across multiple targets to balance the load in an equal level. If K8s is running on the cloud, e.g. AWS or GCP, a Network Load Balancer (NLB) is created. The Cloud Controller Manager (remember the Controller Manager of a Node) is resposible to talk to the underlying cloud provier. In Minikube, the external IP to access the application via the LoadBalancer can be exposed using the command minikube tunnel. 3.5.0.4 default kubernetes services There are also default K8s services created automatically to access K8s with the K8s API. Check the endpoints of the kubernetes service and the endpoints of the api-service pod within kube-system namespace. They should be the same. It is also possible to show all endpoints of the cluster using kubectl get endpoints 3.5.1 Exemplary setup of database and frontend microservices The following example show the deployment and linking of two different deployments. A frontend-deployment.yaml that pulls a container running a Streamlit App, and a database-deployment.yaml that runs a flask application exposing a dictionary as an exemplary and very basic database. The frontend accesses the flask database using a ClusterIP Service linked to the database-deployment. It also exposes an external IP via a Loadbalancer Service, so the streamlit app can be accesses via the browser and without the use of kubectl port-forward. Since minikube is a closed network, use minikube tunnel to allow access to it using the LoadBalancer. When looking at the ClusterIP service with kubectl describe service backendflask the IP address of the service to exposes, as well as the listed endpoints that connect to the database-deployments are shown. One can compare them to the IPs of the actual deployments - they are the same. # services_frontend-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 2 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: containers: - name: frontend image: seblum/mlops-public:frontend-streamlit imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # enviroment variable defined in the application and dockerfile # value is ip adress of the order env: # using the ip adress would be a bad idea. # use the service ip adress. # value: &quot;&lt;order-service-ip-adress&gt;:8081&quot; # how to do it should be this. # we reference to the order service - name: DB_SERVICE value: &quot;backendflask:5001&quot; ports: # we can actually use the actual ip of the service or # use the dns, as done in the example above. - containerPort: 8501 --- apiVersion: v1 kind: Service metadata: name: frontend-lb spec: type: LoadBalancer selector: app: frontend ports: - port: 80 targetPort: 8501 --- apiVersion: v1 kind: Service metadata: name: frontend-node spec: type: NodePort selector: app: frontend ports: - port: 80 targetPort: 8501 nodePort: 30000 # services_backend-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: backendflask spec: replicas: 2 selector: matchLabels: app: backendflask template: metadata: labels: app: backendflask spec: containers: - name: backendflask image: seblum/mlops-public:backend-flask imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 --- apiVersion: v1 kind: Service metadata: name: backendflask spec: # send traffic to any pod that matches the label type: ClusterIP # does not need to be specified selector: app: backendflask ports: # port the service is associated with - port: 5001 # port to access targeted by the access # in our case has to be the same as in backendflask. targetPort: 5000 3.6 Labels, Selectors and Annotations In the previous sections we already made use of labels, selectors, and annotations, e.g. when matching the ClusterIP service to the back-deployments. Labels are a key-value pair that can be attached to objects such as Pods, Deployments, Replicaset, Services, etc. Overall, they are used to organize and select objects. Annotations are an unstructured key-value mapping stored with a resource that may be set by external tools to store and retrieve any metadata. In contrast to labels and selectors, annotations are not used for querying purposes but rather to attach arbitrary non-identifying metadata. These data are used to assist tools and libraries to work with the K8s ressource, for example to pass configuration around between systems, or to send values so external tools can perform more informed decisions based on the annotations provided. Selectors are used to filter K8s objects based on a set of labels. A selector basically simply uses a boolean language to select pods. The selector matches the labels under a an all or nothing principle, meaning everything specified in the selector must be fulfilled by the labels. However, this works not the other way around. If there are multiple labels specified and the selector matches only one of them, the selector will match the ressource itself. How a selector matches the labels can be tested using the kubectl commands as seen below. # Show all pods including their labels kubectl get pods --show-labels # Show only pods that match the specified selector key-value pairs kubectl get pods --selector=&quot;key=value&quot; kubectl get pods --selector=&quot;key=value,key2=value2&quot; # in short one can also write kubectl get pods -l key=value # or also look for multiple kubectl get pods -l &#39;key in (value1, value2)&#39; When using ReplicaSets in a Deployment, their selector matches the labels to a specific pod (check e.g. the section describing Deployments). Any Pods matching the label of the selector will be created according to the specified replicas. Of course, there can also be multiple labels specified. The same principle accounts when working with Services. Below example shows two different Pods and two NodePort services. Each service matches to a Pod based on their selector-label relationship. Have a look at their specific settings using kubectl. The Nodeport Service labels-and-selectors-2 has no endpoints, as it is a all-or-none-principle and none of the created Pods matches the label environment=dev. In contrast, even though the Pod cat-v1 has multiple labels specified app: cat-v1; version: one, the NodePort Service labels-and-selectors is linked to it. It is also linked to the second Pod cat-v2. # labels.yaml apiVersion: v1 kind: Pod metadata: name: cat-v1 labels: app: cat-v1 version: one spec: containers: - name: cat-v1 image: &quot;seblum/mlops-public:cat-v1&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; --- apiVersion: v1 kind: Pod metadata: name: cat-v2 labels: app: cat-v1 spec: containers: - name: cat-v2 image: &quot;seblum/mlops-public:cat-v2&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; --- apiVersion: v1 kind: Service metadata: name: labels-and-selectors spec: type: NodePort selector: app: cat-v1 ports: - port: 80 targetPort: 5000 --- apiVersion: v1 kind: Service metadata: name: labels-and-selectors-2 spec: type: NodePort selector: app: cat-v1 environment: dev ports: - port: 80 targetPort: 5000 3.7 Namespaces Namespaces allow to organize resources in the cluster, which makes it more overseeable when there are multiple resources for different needs. Maybe we want to organize by team, department, or according to a development environment (dev/prod), etc. By default, K8s will use the default-namespace for resources that have not been specified otherwise. Similarly, kubectl interacts with the default namespace as well. Yet, there are already different namespace in a basic K8s cluster default - The default namespace for objects with no other namespace kube-system - The namespace for objects created by the Kubernetes system kube-public - This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. kube-node-lease - This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales. Of course, there is also the possibility of creating ones own namespace and using it by attaching a e.g. Deployment to it, such as seen in the following example. # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring --- apiVersion: apps/v1 kind: Deployment metadata: name: monitoring-deployment namespace: monitoring spec: replicas: 1 selector: matchLabels: app: monitoring-deployment template: metadata: labels: app: monitoring-deployment spec: containers: - name: monitoring-deployment image: &quot;grafana/grafana:latest&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 When creating a Service, a corresponding DNS entry is created as well, such as seen in the Services section when calling backendflask directly. This entry is created according to the namespace which is denoted to the service. This can be useful when using the same configuration across multiple namespaces such as development, staging, and production. It is also possible to reach across namespaces. One needs to use the fully qualified domain name (FQDN) tough, such as &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local. 3.8 Service Discovery Service Discovery is a mechanism that lets applications and microservices locate each other on a network. In fact, we have aready used Service Discovery in the previous sections, they just haven’t been mentioned yet. If a client wants to communicate with the application, it should not use the IP of an individual Pod should not use the individual pod ip. Instead, we should rely on services as they have a stable IP address. We have already seen this in the section about Services. Yet, each pod has also an individual DNS (Domain Name System). A DNS translates a domain names to an IP address, just one lookes up a number in a telephone book, so it’s much easier to reference to a resource online. This is where service Discovery enters the game. Service Discovery Whenever a service is created, it is registered in the service registry with the service name and the service IP. Most clusters use CoreDNS as a service registry (this would be the telephone book itself). When having a look at the minikube cluster one should see are core-dns service running. Now you know what it is for. Having a closer look using kubectl describe svc &lt;name&gt;, the core-dns service has only one endpoint. If you want to have an even closer look, you can dive into a pod itself and check the file /etc/resolv.conf. There you find a nameserver where the IP is the one of the core-dns. # when querying services, it necessary # to specify the corresponding namespace kubectl get service -n kube-system # command for queriying the dns nslookup &lt;podname&gt; 3.8.0.1 kube-proxy As mentioned earlier, each node has three main components: Kubelet, Container Runtime, and the Kube-Proxy. Kube-Proxy is a network proxy running on each node and is responsible for internal network communications as well as external. It also implements a controller that watches the API server for new services and endpoints. Whenever there is a new service or endpoint, the kube-proxy creates a local IPVS rule (IP Virtual Server) that tells the node to intercept traffic destined to the ClusterIP Service. IPVS is built on top of the network filter and implements a transport-layer load balancing. This gives the ability to load balance to real service as well as redirecting traffic to pods that match service label selectors. This means, kube-proxy is intercepting all the requests and makes sure that when a request to the ClusterIP service is sent using endpoints, the request is forwarded to the healthy pods behind the endpoint. 3.9 Volume &amp; Storage Since Pods are ephemeral, any data associated is deleted when a Pod or container restarts. Applications are run stateless the majority of the times, meaning the data does not needs to be kept on the node and the data is stored on an external database. However, there are times when the data wants to be kept, shared between Pods, or when it should persist on the host file system (disk). As described in the section about Pods, a Pod can contain volumes. Volumes are exactly what is needed for such tasks. They are used to store and access data which can be persistent or long lived on K8s. There are different types of volumes, e.g.: EmptyDir HostPath Volume awsElasticBlockStore: AWS EBS volumes are persistent and originally unmounted. They are read-write-once-only tough. There are multiple other types of volumes, a full list can be found here: https://kubernetes.io/docs/concepts/storage/volumes/#volume-types 3.9.1 EmptyDir Volume An EmptyDir Volume is initially empty (as the name suggests). The volume is a temporary directory that shares the pods lifetime. If the pod dies, the contents of the emptyDir are lost as well. The EmptyDir is also used to share data between containers inside a Pod during runtime. # volume_empty-dir.yaml apiVersion: apps/v1 kind: Deployment metadata: name: emptydir-volume spec: selector: matchLabels: app: emptydir-volume template: metadata: labels: app: emptydir-volume spec: # add a volume to the deployment volumes: # mimics a caching memory type - name: cache # specify the volume type and the temp directory emptyDir: {} # of course there could also be a second volume added containers: - name: container-one image: busybox # image used for testing purposes # since the testing image immediately dies, we want to # execute an own sh command to interact with the volume volumeMounts: # The name must match the name of the volume - name: cache # interal reference of the pod mountPath: /foo command: - &quot;/bin/sh&quot; args: - &quot;-c&quot; - &quot;touch /foo/bar.txt &amp;&amp; sleep 3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # create a second container with a different internal mountPath - name: container-two image: busybox volumeMounts: - name: cache mountPath: /footwo command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; As stated in the yaml, the busybox image immediately dies. If the Containers where created without the shell commands, the pod would be in a crashloopbackoff-state. To prevent the Pod to do so it is caught with the sleepcommands until it scales down. Accessing a container using kubectl exec, it can be checked whether the foo/bar.txt has been created in container-one. When checking the second container container-two, the same file should be visible as well. This is because both containers refer to the same volume. Keep in mind though that the mountPath of the container-two is different. # get in container kubectl exec -it &lt;emptydir-volume-name&gt; -c container-one -- sh # check whether bar.txt is present ls # accessing the second container, there is also a file foo/bar.txt # remember, both containers share the same volume kubectl exec -it &lt;emptydir-volume-name&gt; -c container-two -- sh ls 3.9.2 HostPath Volume THe HostPath Volume type is used when an application needs to access the underlying host file system, meaning the file system of the node. HostPath represents a pre-existing file or directory on the host machine. However, this can be quite dangerous and should be used with caution. If having the right access, the application can interfere and basically mess up the host. It is therefore recommended to set the rights to read only to prevent this from happening. # volume_hostpath.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hostpath-volume spec: selector: matchLabels: app: hostpath-volume template: metadata: labels: app: hostpath-volume spec: volumes: - name: var-log # specify the HostPath volume type hostPath: path: /var/log containers: - name: container-one image: busybox volumeMounts: - mountPath: /var/log name: var-log readOnly: true command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; Similar to the EmptyDir Volume example, you can check the implementation of the HostPath Volume by accessing the volume. When comparing the file structures of the hostpath-volume deployment and the directory path: /var/log on the node the deployment is running, they should be the same. All the changes made to either on of them will make the changes available on the other. By making changes via the pod we can directly influence the Node. Again, this is why it is important to keep it read-only. # access the kubectl exec -it &lt;hostpath-volume-name&gt; -- sh # ssh into node minikube ssh &lt;node&gt; 3.9.3 Persistent Volumes Persistent Volumes allow to store data beyond a Pods lifecycle. If a Pod fails, dies or moves to a different node, the data is still intact and can be shared between pods. Persistent Volume types are implemented as plugins that K8s can support(a full list can be found online). Different types of Persistent Volumes are: NFS Local Cloud Network storage (AWS EBS, Azure File Storage, Google Persistent Disk) The following example show how the usage of Persistent Volumes works on the AWS cloud. K8s is running on an AWS EKS Cluster and AWS EBS Volumes attached to it. The Container storage interface (CSI) of K8s to use Persistent Volumes is implemented by the EBS provider, e.g. the aws-ebs-plugin. This enables a the use of Persistent Volumes in the EKS cluster. Therefore, a Persistent Volume (PV) is rather the mapping between the storage provider (EBS) and the K8s cluster, than a volume itself. The storage class of a Persistent Volume can be configured to the specific needs. Should the storage be fast or slow, or do we want to have each as a storage? Or might there be other parameters to configure the storage? If a Pods or Deployments want to consume storage of the PV, they need to get access to the PV. This is done via a so called persistent volume claim (PVC). All of this is part of a Persistent Volume Subsystem. The Persistent Volume Subsystem provides an API for users and administrators. The API abstracts details of how storage is provided from how it is consumed. Again, the provisioning of storage is done via a PV and the consumption via a PCV. Persistent Volume Subsystem Listed below are again the three main components when dealing with Persistent Volumes in K8s Persistent Volume: is a storage resource provisioned by an administrator PVC: is a user’s request for and claim to a persistent volume. Storage Class: describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned. So how are Persistent Volumes specified in our deployments yamls? As there are kind:Pod ressources, there can similarly kind:PersistentVolume and kind:PersistentVolumeClaim be specified. At first, a PV is created. As we run on minikube and not on the cloud, a local storage in the node is allocated. Second, a PVC is created requesting a certain amount of that storage. This PVC is then linked in the specifications of a Deployment to allow its containers to utilized the storage. Before applying the yaml-files we need to allocate the local storage by claiming storage on the node and set the paths specified in the yamls. To do this, we ssh into the node using minikube ssh. We can then create a specific path on the node such as /mnt/data/. We might also create a file in it to test accessing it when creating a PVC to a Pod. Since we do not know yet on what node the Pod is scheduled, we should create the directory on both nodes. Below are all steps listed again. # ssh into node minikube ssh # create path sudo mkdir /mnt/data # create a file with text sudo sh -c &quot;echo &#39;this is a pvc test&#39; &gt; /mnt/data/index.html&quot; # do this on both nodes as pod can land on either one of them Afterward we can apply the yaml files and create a PV, PVC, and the corresponding Deployment utilizing the PVC. The yaml code below shows this process. # volume_persistent-volume.yaml apiVersion: v1 kind: PersistentVolume metadata: name: mypv spec: # specifiy the capacity of the PersistentVolume capacity: storage: &quot;100Mi&quot; volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: manual hostPath: path: &quot;/mnt/data&quot; # specify the hostPath on the node # that&#39;s the path we specified on our node --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mypvc spec: resources: requests: # we request the same as the PV is specified # so we basically request everything storage: &quot;100Mi&quot; volumeMode: Filesystem storageClassName: &quot;manual&quot; accessModes: - ReadWriteOnce --- apiVersion: apps/v1 kind: Deployment metadata: name: pv-pvc-deployment spec: selector: matchLabels: app: pv-pvc template: metadata: labels: app: pv-pvc spec: volumes: - name: data # define the use of the PVC by specifying the name # specify the pod/deployment can use the PVC persistentVolumeClaim: claimName: mypvc containers: - name: pv-pvc image: nginx:latest volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; # since the PVC is stated, the container needs to # mount inside it # name is equal to the pvc name specified name: data resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: pv-pvc spec: type: LoadBalancer selector: app: pv-pvc ports: - port: 80 targetPort: 80 By accessing a pod using kubectl exec -it &lt;persistent-volume-name&gt; -- sh we can check whether the path is linked using the PVC. Now, the end result may seem the same as what we did with the HostPath Volume. But it actually is not, it just looks like it since both, the PersistentVolume and the HostPath connect to the Host. Yet, the locally mounted path would be somewhere else when running in the cloud. The PV configuration would point to another storage source instead of a local file system, for example an attached EBS of EFS storage. Since we also created a LoadBalancer service, we can run minikube tunnel to expose the application deplyment under localhost:80. It should show the input of the index.html file we created on the storage. 3.10 ConfigMaps When building software, the same container image should be used for development, testing, staging, and production stage. Thus, container images should be reusable. What usually changes are only the configuration settings of the application. ConfigMaps allow to store such configurations as a simple mapping of key-value pairs. Most of the time, the configuration within a config map is injected using environment variables and volumes. However, ConfigMaps should only be used to store configuration files, not sensitive data, as they do not secure them. Besides allow for an easy change of variables, another benefit of using ConfigMaps is that changes in the configuration are not disruptive, meaning the application can still run while the configuration changes without affecting the application. However, one needs to keep in mind that change made to ConfigMaps and environment variables will not be reflected on already and currently running containers. The following example creates two different ConfigMaps. The first one includes three environment variables as data. The second one include a more complex configuration of an nginx server. # configmaps.yaml apiVersion: v1 kind: ConfigMap metadata: name: app-properties data: app-name: kitty app-version: 1.0.0 team: engineering --- apiVersion: v1 kind: ConfigMap metadata: name: nginx-conf data: # configuration in .conf nginx.conf: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location /health { access_log off; return 200 &quot;healthy\\n&quot;; } } Additionally, a Deployment is created which uses both ConfigMaps. A ConfigMap is declared under spec.volumes as well. It is also possible to state a reference to both ConfigMaps simultaneously. The Deployment creates two containers. The first container mounts each ConfigMap as a Volume. Container two uses environment variables to access and configure the key-value pairs of the ConfigMaps and store them on the container. # configmaps_deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: config-map spec: selector: matchLabels: app: config-map template: metadata: labels: app: config-map spec: volumes: # specify ConfigMap nginx-conf - name: nginx-conf configMap: name: nginx-conf # specify ConfigMap app-properties - name: app-properties configMap: name: app-properties # if both configmaps shall be mounted under one directory, # we need to use projected - name: config projected: sources: - configMap: name: nginx-conf - configMap: name: app-properties containers: - name: config-map-volume image: busybox volumeMounts: - mountPath: /etc/cfmp/ngnix # is defined here in the nginx-volume to mount name: nginx-conf # everything from that configMap is mounted as a file # the file content is the value themselves - mountPath: /etc/cfmp/properties name: app-properties - mountPath: etc/cfmp/config name: config command: - &quot;/bin/sh&quot; - &quot;-c&quot; args: - &quot;sleep 3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; - name: config-map-env image: busybox resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # as previously, keep the busybox container alive command: - &quot;/bin/sh&quot; - &quot;-c&quot; args: - &quot;env &amp;&amp; sleep 3600&quot; env: # environment variables to read in from config map # for every data key-value pair in config Map, an own # environment variable is created, which gets # the value from the corresponding key - name: APP_VERSION valueFrom: configMapKeyRef: name: app-properties key: app-version - name: APP_NAME valueFrom: configMapKeyRef: name: app-properties key: app-name - name: TEAM valueFrom: configMapKeyRef: name: app-properties key: team # reads from second config map - name: NGINX_CONF valueFrom: configMapKeyRef: name: nginx-conf key: nginx.conf We can check for the attached configs by accessing the containers via the shell, similar to what we did in the section about Volumes. In the container config-map-volume, the configs are saved under the respective mountPath of the volume. In the config-map-env, the configs are stored as environment variables. # get in container -volume or -env kubectl exec -it &lt;config-map-name&gt; -c &gt;container-name&lt; -- sh # check subdirectories ls # print environment variables printenv 3.11 Secrets Secrets, as the name suggests, store and manage sensitive information. However, secrets are actually not secrets in K8s. They can quite easily decoded using kubectl describe on a secret and decode it using the shell command echo &lt;password&gt; | base64 -d. Thus, sensitive information like database password should never be stored in secrets. There are much better ressources to store such data, for example a Vault on the cloud provider itself. However, secret can be used so that you don’t need to include confidential data in your application code. Since they are stored and created independently of the Pods that use them, there is less risk of being exposed during the workflow of creating, viewing, and editing Pods. It is possible to create secrets using imperative approach as shown below. # create the two secrets db-password and api-token kubectl create secret generic mysecret-from-cli --from-literal=db-password=123 --from-literal=api-token=token # output the new secret as yaml kubectl get secret mysecret -o yaml # create a file called secret with a file-password in it echo &quot;super-save-password&quot; &gt; secret # create a secret from file kubectl create secret generic mysecret-from-file --from-file=secret Similar to ConfigMaps, secrets are accessed via an environment variable or a volume. # secrets.yaml apiVersion: apps/v1 kind: Deployment metadata: name: secrets spec: selector: matchLabels: app: secrets template: metadata: labels: app: secrets spec: volumes: # get the secret from a volume - name: secret-vol secret: # the name of the secret we created earlier secretName: mysecret-from-cli containers: - name: secrets image: busybox volumeMounts: - mountPath: /etc/secrets name: secret-vol env: # nane of the secret in the container - name: CUSTOM_SECRET # get the secret from an environment variable valueFrom: secretKeyRef: # name and key of the secret we created earlier name: mysecret-from-file key: secret command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; 3.11.1 Exemplary use case of secrets When pulling from a private dockerhub repository, applying the deployment will throw an error since there are no username and password specified. As they should not be coded into the deployment yaml itself, they can be accessed via a secret. In fact, a specific secret can be specified for docker registry. The secret can be specified using the imperative approach. kubectl create secret docker-registry docker-hub-private \\ --docker-username=YOUR_USERNAME \\ --docker-password=YOUR_PASSWORD \\ --docker-email=YOUR_EMAIL Finally, the secret is specified in the deployment configuration where it can be accessed during application. # secret_dockerhub.yaml apiVersion: apps/v1 kind: Deployment metadata: name: secret-app spec: selector: matchLabels: app: secret-app template: metadata: labels: app: secret-app spec: # specifiy the docker-registry secret to be accessed imagePullSecrets: - name: docker-hub-private containers: - name: secret-app # of course you need an own private repository # to pull and change the name accordingly image: seblum/private:cat-v1 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 3.12 Health Checks Applications running in service need to be healthy at all times so they are ready to receive traffic. K8s uses a process called health checks to test whether an application is alive. If there are any issues and the application is unhealthy, K8s will restart the process. Yet, checking only whether a process is up and running might be not sufficient. What if, e.g., a client wants to connect to a database and the connection cannot be established, even though the app is up and running? To solve more specific issues like this, health checks like a liveness probe or readiness probe can be used. If there have not been specified, K8s will use the default checks to test whether a process is running. 3.12.1 Liveness Probe The Kubelet of a node uses Liveness Probes to check whether an application runs fine or whether it is unable to make progress and its stuck on a broken state. For example, it could catch a deadlock, a database connection failure, etc. The Liveness Probe can restart a container accordingly. To use a Liveness Probe, an endpoint needs to be specified. The benefit of this is, that it is simple to define what it means for an application to be healthy just by defining a path. 3.12.2 Readiness Probe Similar to a Liveness Probe, the Readniness Probe is used by the kubelet to check when the container is ready to start accepting traffic. A Pod is considered ready when all of its containers are ready. Its configuration is also done by specifying a path to what it means the application is healthy. A lot of frameworks, like e.g. springboot, actually provide a path to use. Belows configuration shows a Deployment which includes a Liveness and a Readiness Probe. The image of the deployment is set up so its process is killed after a given number of seconds. This has been passed using environment variables such as seen in the script. Both, the Liveness and the Readiness Probe have the same parameters in the given example. initialDelaySeconds: The probe will not be called until x seconds after all the containers in the Pod are created. timeoutSeconds: The probe must respond within a x-second timeout and the HTTP status code must be equal to or greater than 200 and less than 400. periodSeconds: The probe is called every x seconds by K8s failureThreshold: The container will fail and restart if more than x consecutive probes fail # healthchecks.yaml apiVersion: apps/v1 kind: Deployment metadata: name: backendflask-healthcheck spec: replicas: 1 selector: matchLabels: app: backendflask-healthcheck template: metadata: labels: app: backendflask-healthcheck environment: test tier: backend department: engineering spec: containers: - name: backendflask-healthcheck # check whether I have to change the backend app to do this. image: &quot;seblum/mlops-public:backend-flask&quot; imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # Specification of the the Liveness Probe livenessProbe: httpGet: # path of the url path: /liveness port: 5000 # time the liveness probe starts after pod is started initialDelaySeconds: 5 timeoutSeconds: 1 failureThreshold: 3 # period of time when the checks should be performed periodSeconds: 5 # Specification of the Readiness Probe readinessProbe: httpGet: path: /readiness port: 5000 initialDelaySeconds: 10 # change to 1 seconds and see the pod not going to go ready timeoutSeconds: 3 failureThreshold: 1 periodSeconds: 5 env: # variable for the container to be killed after 60 seconds - name: &quot;KILL_IN_SECONDS&quot; value: &quot;60&quot; ports: - containerPort: 5000 --- apiVersion: v1 kind: Service metadata: name: backendflask-healthcheck spec: type: NodePort selector: app: backendflask-healthcheck ports: - port: 80 targetPort: 8080 nodePort: 30000 --- apiVersion: v1 kind: Service metadata: name: backendflask-healthcheck spec: type: ClusterIP selector: app: backendflask-healthcheck ports: - port: 80 targetPort: 8080 3.13 Resource Management Besides the importance of a healthy application itself, there should be also enough resources allocated so the application can perform well, e.g. memory &amp; CPU. Yet, it should also only consume the resources needed and not block unneeded ones. It might be dangerous, as one application using a lot of ressources, leaving nothing left for other applications and eventually starving them. To prevent this from happening in K8s. there can be a minimum amount of resources defined a container needs (request) as well as the maximum amount of resources a container can have (limit). Configuring limits and requests for a container can be done within the spec for a Pod or Deployment. Actually, we have been using them all the time previously. # resource-management.yaml apiVersion: apps/v1 # specify that we want a deployment kind: Deployment metadata: name: rm-deployment spec: # specify number of replicas replicas: 3 selector: matchLabels: app: rm-deployment template: metadata: labels: app: rm-deployment spec: containers: - name: rm-deployment image: seblum/mlops-public:cat-v1 requests: memory: &quot;512Mi&quot; cpu: &quot;1000m&quot; # Limits limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 3.14 Daemon Sets The master node of K8s decides on what worker nodes a pod is scheduled or not. However, there are times where we want to have a copy of a pod across the cluster. A DaemonSet ensures a copy of the specified Pod is exactly doing this. This can be useful for example to deploy system daemons such as log collectors and monitoring agents. DaemonSets are automatically deployed on every single node, unless specified on which node to run. They therefore do not need a specification of nodes and can scale up and down with the cluster as needed. They will automatically scheduled a pod on each new node. The given example deploys a DaemonSet to cover logging using K8s FluendID. # daemonsets.yaml apiVersion: v1 kind: Namespace metadata: name: logging --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: logging labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can&#39;t run pods - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule # specify the containers as done in Pods or Deployments volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers containers: - name: fluentd-elasticsearch # allows to collect logs from nodes image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 3.15 StatefulSets StatefulSets are used to deploy and manage stateful applications. Stateful applications are applications which are long lived, for example databases. Most applications of K8s are stateless as they only run for a specific task. However, a database is a state of truth and should be present at all time. StatefulSets manage the pods based on the same container specifications such as Deployments. Lets assume we have a StatefulSet with 3 replicas. Each Pod has a PV attached. 3.16 Jobs &amp; Cron Jobs Using the busybox image in the section about volumes we experienced that the image is very short lived. K8s is not aware of this and runs into a CrashLoopBackOff-Error. K8s will try and restart the container itself though until it BackOffs completley. Because the image is so short live, a job within the image has to be executed such as done with a shell command previously. However, what if we have a simple task that only should run like every 5 minutes, or every single day? A good idea is to use CronJobs for such tasks that start the image if needed. When comparingJobs jobs and CronJobs, jobs execute only once, whereas CronJobs execute depending on an specified expression. The following job simulates a backup to a database that runs 30 seconds in total. The part in the args specifies that the container will sleep for 20 seconds (the hypothetical backup). Afterward, the container will wait 10 seconds to shut down, as specified in ttlSecondsAfterFinished. # job.yaml apiVersion: batch/v1 kind: Job metadata: name: db-backup-job spec: # time it takes to terminate the job for one completion ttlSecondsAfterFinished: 10 template: spec: containers: - name: backup image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: - &quot;echo &#39;performing db backup...&#39; &amp;&amp; sleep 20&quot; restartPolicy: Never The CronJob below runs run every minute. Given the structure of ( * * * * * * ) - ( Minutes Hours Day-of-month Month Day-of-week Year), the cronjob expression defines as follows: # cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: db-backup-cron-job spec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: backup image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: - &quot;echo &#39;performing db backup...&#39; &amp;&amp; sleep 20&quot; restartPolicy: Never "],["airflow.html", "Chapter 4 Airflow 4.1 Prerequisites 4.2 Introduction to usage 4.3 Templates 4.4 Deploying Airflow", " Chapter 4 Airflow Apache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows. Airflow comes with a web interface to help you manage the state of your workflows. Airflow can be deployed deployable in many ways, starting from a single process on your own laptop to a distributed setup with multiple compute resources for the biggest workflows. The aim of this tutorial is to show you how to use Airflow from a developer / user perspective and is based on the local installation. Please refer to the prerequisits on what is needed to follow through. A detailed description of what an Airflow deployment involves is shown in the chapter Airflow Infrastructure. 4.1 Prerequisites installation with pip? 4.2 Introduction to usage What is Airflow? Airflow is a platform to program workflows, including their creation, schedulung, and monitoring. A workflow describes here a set of steps to accomplish a given data engineering tasks, e.g. downloading files, copying data, filtering information, writing to a database, etc. Workflows can be of varying levels of complexity and in general it is a term with various meaning depending on context. Airflow can also be referred to as “Workflows as code”. It serves several purposes: + dynamic + extensible + flexible + Airflow can implement programs from any language, but workflows are defined in Python. They can be assessed via code, command-line, or via web interface. Airflow’s user interface provides both in-depth views of pipelines and individual tasks, and an overview of pipelines over time. From the interface, you can inspect logs and manage tasks, for example retrying a task in case of failure. The web interface aims to make managing workflows as easy as possible. However, the philosophy of Airflow is to define workflows as code so coding will always be required. Airflow Web Interface Why Airflow? Airflow serves as a batch workflow orchestration plattform. contains operators to connect with many technologies and is easily extensible to connect with new technologies. If you prefer coding over clicking, Airflow is the tool for you. Workflows are defined as Python code which means: Workflows can be stored in version control so that you can roll back to previous versions Workflows can be developed by multiple people simultaneously Tests can be written to validate functionality Components are extensible and you can build on a wide collection of existing components Apache Airflow can be used to schedule: ETL pipelines that extract data from multiple sources and run Spark jobs or any other data transformations Training machine learning models Report generation Backups and similar DevOps operations And much more! You can even write a pipeline to brew coffee every few hours, it will need some custom integrations but that’s the biggest power of Airflow — it’s pure Python and everything can be programmed. 4.2.1 DAGs Workflows are implemented as DAGs: Directed Acyclic Graphs. It is Directed, because there is an inherent flow representing dependencies between components. It is Acyclic, because it does not loop / cycle / repeat. Graph describes the actual set of components. DAGs are also seen in Airflow, Apache Spark, Luigi An Airflow Python script is really just a confguration file specifying a DAG’s structure as code. We’ll need a DAG object to nest our tasks into. Here we pass a string that defines the dag_id, which serves as a unique identifier for your DAG. We also pass the default argument dictionary from airflow.models import DAG # of course there are many other arguments as well default_args = { &#39;start_data&#39;:&#39;2023-01-01&#39;, &#39;schedule_interval&#39;:&#39;None&#39; } example_dag = DAG( dag_id=&#39;etl_pipeline&#39;, default_args=default_args ) Creating a time zone aware DAG is quite simple. a workflow can be run via the cli using airflow run &lt;dag_id&gt; &lt;task_id&gt; &lt;start_date&gt; # initialize the database tables airflow db init # print the list of active DAGs airflow dags list # prints the list of tasks in the &quot;tutorial&quot; DAG airflow tasks list tutorial # prints the hierarchy of tasks in the &quot;tutorial&quot; DAG airflow tasks list tutorial --tree airflow tasks test runs task instances locally, outputs their log to stdout (on screen), does not bother with dependencies, and does not communicate state (running, success, failed, …) to the database. It simply allows testing a single task instance. Same goes with airflow dags test # testing # testing print_date airflow tasks test tutorial print_date 2015-06-01 # testing sleep airflow tasks test tutorial sleep 2015-06-01 airflow dags test People sometimes think of the DAG definition file as a place where they can do some actual data processing - that is not the case at all! The script’s purpose is to define a DAG object. It needs to evaluate quickly (seconds, not minutes) since the scheduler will execute it periodically to reflect the changes if any. An Airflow pipeline is just a Python script that happens to define an Airflow DAG object. 4.2.2 Workloads A DAG runs through a series of Tasks, and there are three common types of task you will see: Operators, predefined tasks that you can string together quickly to build most parts of your DAGs. Sensors, a special subclass of Operators which are entirely about waiting for an external event to happen. A TaskFlow-decorated @task, which is a custom Python function packaged up as a Task. Internally, these are all actually subclasses of Airflow’s BaseOperator, and the concepts of Task and Operator are somewhat interchangeable, but it’s useful to think of them as separate concepts - essentially, Operators and Sensors are templates, and when you call one in a DAG file, you’re making a Task. 4.2.3 Operators Operators represent a single task in a workflow / unit of work for Airflow to complete. They run independently (usually) and generally do not share any information. There are various operators to perform different tasks. Gotchas: They are not guaranteed to run in the same location/environment. May require extensive use of Environment variables. Can be difficult to run tasks with elevated privileges. Some of the most popular operators are the PythonOperator, the BashOperator, and the KubernetesPodOperator. Airflow completes work based on the arguments you pass to your operators. BashOperator - expects a bash_command PythonOperator - expects a python_callable BranchPython - requires a python_callable and provide_context=True. The callable must accept **kwargs EmailOperator The EmailOperator does require the Airflow system to be configured with email server details. from airflow.operators.email_operator import EmailOperator email_task = EmailOperator( task_id=&#39;email_sales_report&#39;, to=&#39;sales_manager@example.com&#39;, subject=&#39;automated Sales Report&#39;, html_content=&#39;Attached is the latest sales report&#39;, files=&#39;latest_sales.xlsx&#39;, dag=example_dag ) Using operators is the classic approach to defining work in Airflow. For some use cases, it’s better to use the TaskFlow API to define work in a Pythonic context as described in Working with TaskFlow. Operators can be split into three categories: Action operators — for example, BashOperator (executes any bash command), PythonOperator (executes a python function) or TriggerDagRunOperator (triggers another DAG). Transfer operators — designed to transfer data from one place to another, for example GCSToGCSOperator which copies data from one Google Cloud Storage bucket to another one. Those operators are a separate group because they are often stateful (the data is first downloaded from source storage and stored locally on a machine running Airflow and then uploaded to destination storage). Sensors — are operators classes inheriting from BaseSensorOperator and are designed to wait for an operation to complete. When implementing a sensor, users have to implement the poke method which is then invoked by a special executemethod of BaseSensorOperator. The poke method should return True or False. The sensor will be executed by Airflow until it returns True. 4.2.4 Tasks To use an operator in a DAG, you have to instantiate it as a task. Tasks determine how to execute your operator’s work within the context of a DAG. Tasks are Instances of operators. They are usually assigned to a variable on Python. They are referred to by the task_id within the airflow tools. example_task = BashOperator(task_id=&#39;bash_example&#39;, bash_command=&#39;echo &quot;Example!&quot;&#39;, dag=example_dag) t1 = BashOperator( task_id=&quot;print_date&quot;, bash_command=&quot;date&quot;, ) t2 = BashOperator( task_id=&quot;sleep&quot;, depends_on_past=False, bash_command=&quot;sleep 5&quot;, retries=3, ) 4.2.4.1 Setting up Dependencies It is possible to define a give order of task completion, meaning task dependencies. They are either referred to as upstream or downstream tasks. In Airflow 1.8 and later, they are defined using the bitshift operators: * &gt;&gt;, or the upstream operator (before) * &lt;&lt;, or the downstream operator (after) t1.set_downstream(t2) # chained dependencies task_1 &gt;&gt; task_2 &gt;&gt; task_3 # mixed dependencies task_1 &gt;&gt; task_2 &lt;&lt; task_3 task_1 &gt;&gt; task_2 task_3 &gt;&gt; task_2 the actual tasks defined in a DAG run on different workers, which means that in the below script the cannot be used to cross communicate between tasks arguments supports arguments to tasks * Positional * Keyword Use the op_kwargs dictionary def sleep(length_of_time): time.sleep(lenght_of_time) sleep_task = PythonOperator( task_id=&#39;sleep&#39;, python_callable=sleep, op_kwargs={&#39;length_of_time&#39;:5}, dag=example_dag ) 4.2.5 Scheduling DAG Runs A DAG run is a specific instance of a workflow at a point in time. It can be run manually or via schedule_interval. Maintain state for each workflow and the tasks within: * running * failed * success When scheduling a DAG, there are severyl attributes to note: * start_date - the date/time to initially schedule the DAG run * end_date - optional attribute for when to stop running new DAG instances * max_tries - optional attribute for how many attempts to make * schedule_interval - how often to run 4.2.6 Sensors A Sensor is an operator that waits for a certain condition to be true, e.g.: * creation of a file (existence of a file FileSensor) * upload of a database record * certain response from a web request It can define how often to check for the condition to be true Sensors are assigned to tasks. Sensors have different arguments: mode: how to check for a condition mode='poke' the default, run repeatedly mode='reschedule' give up task slot and try again later poke_interval: how often to wait between checks timeout: how long to wait before failing task from airflow.sensors.base_sensor_operator from airflow.contrib.sensors.file_sensor import FileSensor file_sensor_task = FileSensor(task=&#39;file_sense&#39;, filepath=&#39;salesdata.csv&#39;, poke_intervall=300, dag=sales_report_dag ) init_sales_cleanup &gt;&gt; file_sensor_task &gt;&gt; generate_report Other sensors: * ExternalTaskSensor - wait for a task in another DAG to complete * HttpSensor - Request a web URL and check for content * SqlSensor - Runs a SQL query to check for content * Many other in aorflow.sensors and airflow.contrib.sensors * ### Executor What is an executor? Executors run tasks. Different executors handle running the tasks differently, e.g. + SequentialExecutor + LocalExecutor + CeleryExecutor + * LocalExecutor—executes the tasks in separate processes on a single machine. It’s the only non-distributed executor which is production ready. It works well in relatively small deployments. * CeleryExecutor—the most popular production executor, which uses under the hood the Celery queue system. When using this executor users can deploy multiple workers that read tasks from the broker queue (Redis or RabbitMQ) where tasks are sent by scheduler. This executor can be distributed between many machines and users can take advantage of queues that allow them to specify what task should be executed where. This is for example useful for routing compute-heavy tasks to more resourceful workers. * KubernetesExecutor— is another widely used production-ready executor. As the name suggests it requires a Kubernetes cluster. When using this executor Airflow will spawn a new pod with an Airflow instance to run each task. This creates an additional overhead which can be problematic for short running tasks. * CeleryKubernetsExecutor— the name says it all, this executor uses both CeleryExecutor and KubernetesExecutor. When users select this executor they can use a special kubernetes queue to specify that particular tasks have to be executed using KubernetesExecutor. Otherwise tasks are routed to celery workers. In this way users can take full advantage of horizontal auto scaling of worker pods and possibility of delegating longrunning / compute heavy tasks to KubernetesExecutor. * DebugExecutor—this is a debug executor. Its main purpose is to debug DAG locally. It’s the only executor that uses a single process to execute all tasks. By doing so it’s simple to use it from IDE level as described in docs. Executor is one of the crucial components of Airflow and it can be configured by the users. It defines where and how the Airflow tasks should be executed. The executor should be chosen to fit your needs and as it defines many aspects of how Airflow should be deployed. 4.2.7 XCom While hooks’ purpose is to implement communication layer with external services, the XCom purpose is to implement communication mechanism that allows information passing between tasks in DAG. The fundamental part of XCom is the underlying metadatabase table (with same name) which works as a key-value storage. The key consists is a tuple (dag_id, task_id, execution_date, key) where the key attribute is a configurable string (by default it’s return_value). The stored value has to be json serializable and relatively small (max 48KB is allowed). This means that the XCom purpose is to store metadata not the data. For example, if we have a dag with task_a &gt;&gt; task_b and a big data frame has to be passed from task_a to task_b then we have to store it somewhere in a persistent place (storage bucket, database etc) between those tasks. Then task_a should upload the data to storage and write to the XCom table an information where this data can be found, for example a uri to storage bucket or name of a temporary table. Once this information is in the XCom table, the task_b can access this value and retrieve the data from external storage. In many cases this may sound like a lot of additional logic of uploading and downloading the data in operators. That’s true. But first of all, that’s where hooks came to the rescue — you can reuse logic for storing data in many different places. Second, there’s a possibility to specify a custom XCom backend. In this way users can simply write a class that will define how to serialize data before it’s stored in the XCom table and how to deserialize it when it’s retrieved from metadatabase. This, for example, allows users to automate the logic of persisting data frames as we described in this article. 4.2.8 taskflow writing data pipelines using the TaskFlow API paradigm airflow 2.0 In this data pipeline, tasks are created based on Python functions using the @task decorator as shown below. The function name acts as a unique identifier for the task. Main flow of the DAG Now that we have the Extract, Transform, and Load tasks defined based on the Python functions, we can move to the main part of the DAG. 4.3 Templates allow substituting DAG information 4.4 Deploying Airflow Before Data Scientists and Machine Learning Engineers can utilize the power of Airflow Workflows, Airflow obviously needs to be set up and deployed. There are multiple ways an Airflow deployment can take place. It can be run either on a single machine or in a distributed setup on a cluster of machines. As stated in the prerequisites for this tutorial we set up Airflow locally on a single machine to introduce you on how to work with airflow. Although Airflow can be run on a single machine, it is beneficial to deploy it as a distributed system to utilize its full power. 4.4.1 Airflow as a distributed system Airflow consists of several separate parts. While this separation is somewhat simulated on a local deployment, each several part of Airflow can be deployed separately when deploying Airflow in a distributed manner. This comes with benefits of safety, security, and reliability (TODO check terms). An Airflow deployment generally consists of five different compontens: Scheduler: The schedules handles triggering scheduled workflows and submitting tasks to the executor to run. Executor: The executor handles running the tasks themselves. In a local installation of Airflow, the tasks are run by the executor itself, whereas in a production ready deployment if Airflow the executor pushes the task execution to separate worker instances. Webserver: The webserver provides the user interface of Airflow we have seen before that allows to inspect, trigger, and debug DAGs and tasks. DAG Directory: The DAG directory is a directory that contains the DAG files which are read by the scheduler and executor. Metadata Database: The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, and user settings. The following graphs shows how the components build up the Airflow architecture. airflow_architecture.png 4.4.2 Scheduler The scheduler is basically the brain and heart of Airflow. It handles triggering and scheduling of workflows as well as submitting tasks to the executor to run. To be able to do this, the scheduler is responsible to parse the DAG files from the DAG directory, manage the database states in the metadata database, and to communicate with the executor to schedule tasks. Since the release of Airflow 2.0 it is possible to run multiple schedulers at a time to ensure a high availability and reliability (TODO check term) of this centerpiece of a distributed Airflow. 4.4.3 Webserver The webserver runs the web interface of Airflow and thus the user interface every Airflow user sees. This allows to inspect, trigger, and debug DAGs and tasks in Airflow (and much more!), such as seen in the previous chapters. Each user interaction and change is written to the DAG directory or the metadata database, from where the scheduler will read and act upon. 4.4.4 Executor The executor handles running the tasks themselves. In a local installation of Airflow, the tasks are run by the executor itself, whereas in a production ready deployment if Airflow the executor pushes the task execution to separate worker instances. The benefit of a distributed deployment is either reliability, but also the possibility to run tasks on different instances based on their needs, for example to run the training step of a machine learning model on a GPU node. TODO a Celery worker application which consumes and executes tasks scheduled by scheduler when using a Celery-like executor (more details in next section). It is possible to have many workers in different places (for example using separate VMs or multiple kubernetes pods). 4.4.5 DAG Directory The DAG directory contains the DAG files written in python. Each file is read by the the other Airflow components for a different purpose. The web interface lists all written DAGs from the directory as well as their content. The scheduler and executor run a DAG or a task based on the input read from the DAG directory. The DAG directory can be of different nature either a local folder in case of a local installation, or a separate (git) repository where the DAG files are stored. (TODO how does it handle subdirectories?) 4.4.6 Metadata Database The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, and user settings. It is beneficial to run the metadata database as a separate component to keep all data safe and secure in case there are bugs in other parts of the infrastructure. Such thoughts also account for the deployment of the metadata database itself. It is possible to run Airflow on an instance within a Kubernetes cluster along all other components of a distributed Airflow installation. However, this is not necessarily recommended and it is also possible to use a clouds’ distinguished database resources to store the metadata, for example the Relational Database Service (RDS) on the amazon cloud. 4.4.6.0.0.0.1 LINKS https://towardsdatascience.com/a-complete-introduction-to-apache-airflow-b7e238a33df "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
