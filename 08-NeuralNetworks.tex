%\setcounter{chapter}{-1}
\chapter{Artificial Neural Networks}
%
%% -------------------------------------------------------------------------------
%
\section{Introduction}
%
%% -------------------------------------------------------------------------------
%
Artificial Intelligence aims to mimic human intelligence using various mathematical and logical tools. Initial AI systems were rule based systems and thus based on learning of formal mathematical rules. However, what about problems which do not have any formal rules, for example identifying objects, understanding spoken words etc.? This is where Artificial Neural Networks (ANN) come into play. \\

While neural networks were inspired by the human mind, their goal are not to copy the human mind, but to use mathematical tools to solve problems without formal rules like image recognition, speech/dialogue, language translation, art generation etc. This is done by learning a model to depict the given problem space. The most basic ANN is called a \textit{Perceptron} and proposed by Frank Rosenblatt. A Perceptron is based on the simplification of a neuron architecture as proposed by McCulloch–Pitts. Not going into much details, it has two inputs and one output and the neuron itself (it is sometimes also referred to as unit as it's not biological anymore) has a predefined threshold. Now, if we feed the inputs to the neuron and the sum of inputs exceed the threshold of the unit, the output is active else it is inactive.

This means, we have a linear activation like 

$$ z = x_{1}w_{11}^{(1)} + x_{2}w_{21}^{(1)} + b_{1}$$


\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[>=latex,scale=0.2]
		% \draw [help lines, black!30, step=0.5] (0,0) grid (20,50);

		% Styles for states, and state edges
		\tikzstyle{unit} = [draw, thick, fill=white, circle, minimum height=1em, minimum width=1em, node distance=1em, font={\sffamily}]
		\tikzstyle{stateEdgePortion} = [black,thick];
		\tikzstyle{stateEdge} = [stateEdgePortion,->];
		\tikzstyle{edgeLabel} = [pos=0.5, text centered, font={\sffamily\small}];

		% ACT-R
		\node[unit, name = input1, scale = .75] {$x_{1}$};
		\node[unit, name = input2, below =1cm of input1, scale = .75] {$x_{2}$};
		\node[unit, name = bias1, below =0.5cm of input2, scale = .75] {$b_{1}$};

		\node[unit, name = unit, below right =0.2cm and 2cm of input1, scale = 1.5] {$z$};

		\node[unit, name = output, right =1cm of unit, scale = .75] {$y$};

		\draw 	(input1) edge [stateEdge] node[edgeLabel]{} (unit);
		\draw 	(input2) edge [stateEdge] node[edgeLabel]{} (unit);
		\draw 	(bias1)  edge [stateEdge] node[edgeLabel]{} (unit);
		\draw 	(unit)   edge [stateEdge] node[edgeLabel]{} (output);

		\end{tikzpicture}
	\end{center}
	\caption[Perceptron]{Perceptron} 
	\label{fig:perceptron}
\end{figure}

However, Minsky and Papert concluded that perceptrons only separate linearly separable classes and are thus incapable of learning very simple functions that exceed a 2-D space. They chose the Exclusive-OR (XOR)-problem to prove that uni layered perceptrons cannot learn beyond linearly separable data.
%
%% -------------------------------------------------------------------------------
%
\section{The XOR Problem}	
%
%% -------------------------------------------------------------------------------
%
In the XOR problem, we try to train a perceptron to mimic a 2D XOR function. The logical “exclusive OR” function states that for two given logical statements, the XOR function would return TRUE if one of the statements is true and FALSE if both statements are true. If neither of the statements is true, it also returns FALSE. If we plot it, we get the following chart

%
\vspace{0.5cm}
\begin{table}[h!]
	\begin{center}
		\small\sffamily\renewcommand{\arraystretch}{0.9}
		\begin{tabular}{p{0.5cm}p{0.5cm}p{0.5cm}}
			x1 & x2 & y \\
			\midrule
			1 & 1 & 0 \\
			1 & 0 & 1 \\
			0 & 1 & 1 \\
			0 & 0 & 0 \\
			% \underline{(1,2)}
		\end{tabular}
	\end{center}
	\caption[XOR truth table]{XOR truth table}
	\label{tab:event3}
\end{table}

However, a perceptron can only converge on linearly separable data. Have a look at the graph and try to fit a linear function that separates our problem in two classes. It's just not possible. Therefore, a perceptron isn’t capable of imitating the XOR function. The solution to this problem is to expand beyond the single-layer architecture of a perceptron by adding an additional layer of units. This is also know as a hidden layer. Now we come a bit closer to a "real" network. The "vanilla" one can be referred to as a multilayer perceptron (MLP).
%
%% -------------------------------------------------------------------------------
%
\section{MLP}	
%
%% -------------------------------------------------------------------------------
%
A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). This means it forwards the input through the network to output the resulting predictions. An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron and can distinguish data that is not linearly separable.
\clearpage

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[>=latex,scale=0.2]
		% \draw [help lines, black!30, step=0.5] (0,0) grid (20,50);

		% Styles for states, and state edges
		\tikzstyle{unit} = [draw, thick, fill=white, circle, minimum height=1em, minimum width=1em, node distance=1em, font={\sffamily}]
		\tikzstyle{stateEdgePortion} = [black,thick];
		\tikzstyle{stateEdge} = [stateEdgePortion,->];
		\tikzstyle{edgeLabel} = [pos=0.5, text centered, font={\sffamily\small}];

		% ACT-R
		\node[unit, name = input1, scale = .75] {x 1};
		\node[unit, name = input2, below =1cm of input1, scale = .75] {x 2};
		\node[unit, name = bias1,  below =0.5cm of input2, scale = .75] {b1};

		\node[unit, name = hidden1, below right =0.2cm and 2cm of input1, scale = 1.25] {h1};
		\node[unit, name = hidden2, below =1cm of hidden1, scale = 1.25] {h2};
		\node[unit, name = bias2,   below =0.5cm of hidden2, scale = .75] {b2};

		\node[unit, name = output, below right =0.2cm and 2cm of hidden1, scale = 1.25] {y};

		\draw 	(input1) edge [stateEdge] node[edgeLabel]{} (hidden1);
		\draw 	(input2) edge [stateEdge] node[edgeLabel]{} (hidden1);
		\draw 	(bias1)  edge [stateEdge] node[edgeLabel]{} (hidden1);

		\draw 	(input1) edge [stateEdge] node[edgeLabel]{} (hidden2);
		\draw 	(input2) edge [stateEdge] node[edgeLabel]{} (hidden2);
		\draw 	(bias1)  edge [stateEdge] node[edgeLabel]{} (hidden2);

		\draw 	(hidden2) edge [stateEdge] node[edgeLabel]{} (output);
		\draw 	(hidden1) edge [stateEdge] node[edgeLabel]{} (output);
		\draw 	(bias2)   edge [stateEdge] node[edgeLabel]{} (output);

		\end{tikzpicture}
	\end{center}
	\caption[MLP]{MLP} 
	\label{fig:mlp}
\end{figure}

Figure above shows a MLP with $x$ denoting the input, $y$ the output, $w_{ij}^{(l)}$ the weights, and $b$ the bias term. Since MLPs are fully connected, each node in one layer connects with a certain weight to every node in the following layer.

The weight $ w_{12}^{(1)}$ is the weight of the 1st layer $^{(1)}$ and connects the 1st neuron from the 1st layer to 2nd neuron in the next layer $_{(1,2)}$. \\

But what happens inside the neuron? There can be two activations: Linear \& non-linear!

$$ z = x_{1}w_{11}^{(1)} + x_{2}w_{21}^{(1)} + b_{1}$$

as a linear activation, and the sigmoid function


$$ a_{1,1}^{(1)} = \sigma(z) $$

$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$

as a non-linear activation function. \\

Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron. A neural network without an activation function is essentially just a linear regression model. Thus, the activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks.

No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer.

MLP can be seen as a very shallow ANN.
%
%% -------------------------------------------------------------------------------
%
\section{How it works}	
%
%% -------------------------------------------------------------------------------
%
Learning of the MLP occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. Thus, we need to know the expected result and have the learning is supervised. The change of the weights is carried out through an algorithm called backpropagation, a generalization of the least mean squares algorithm in the linear perceptron.


In general, there are four (or five) stages of an ANN

\begin{enumerate}
	\item Initialize weights „somehow“, i.e. randomly – as this is only done once, it is sometimes counted as a proper “step” and sometimes not. Regardless of the terminology, weights have to be initialized
	\item Forward-Pass of the inputs
	\item Calculation of the loss/ cost function to determine the prediction
	\item Backpropagation to check the influence of each weight in the prediction
	\item Weight update such that the loss decreases in future forward steps
\end{enumerate}
%
%% -------------------------------------------------------------------------------
%
\section{The math behind it}	
%
%% -------------------------------------------------------------------------------
%
Now the math is the more tricky part. So far, we have only done a forward-pass and probably got some wrong predictions. Similarly, we have calculated each activation separately. What is the big picture?

\begin{align*}
	\hat{y} = a^{(2)} 	&= \sigma(z^{(2)}) \\
						&= \sigma(a^{(1)}w^{(2)}+b^{(2)}) \\
						&= \sigma(\sigma(z^{(2)})w^{(2)}+b^{(2)}) \\
						&= \sigma(\sigma(xw^{(1)}+b^{(1)})w^{(2)}+b^{(2)}) \\
\end{align*}

Now the cost function comes into play to see how good this prediction is. \\

\paragraph{Loss function}
To optimize the network we need a function that specifies the error of our prediction towards the expected output. Typically, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply “loss”. The cost or loss function has an important job in that it must faithfully distill all aspects of the model down into a single number in such a way that improvements in that number are a sign of a better model.

A well known is the mean squared error (MSE):

$$ L = \frac{1}{2}(y_{true}-\hat{y})^2 $$

Ok, now that we have the loss of our trained network, what do we do with it? The idea is to take small steps towards the minimum of the loss function.
This is achieved by updating the weights with a fraction of the (negative) gradient. \\

\paragraph{Gradient Descent}
The “gradient” in gradient descent refers to an error gradient. The gradient descent algorithm seeks to change the weights so that the next evaluation reduces the loss of the model, meaning the optimization algorithm is navigating down the gradient (or slope) of error. 

$$ \Delta L $$

Here the gradient is a vector of the partial derivatives, which points in the direction of the steepest slope. This vector is multiplied by a negative step size (or learning rate) and thus moves in the direction of the towards the minimum. Now suppose we want to calculate $\frac{\partial c}{\partial a}$, that is, the gradient from c to a. This gradient represents the influence of the operator a on the result c: If a changes, so does c, where the gradient indicates how much. \\

\paragraph{Learning rate}

We want to go in small steps towards the minimum (alpha), a learning rate
not to small as we do not want to take forever.

The aim is to update the weights with only a fraction of the (negative) gradient. Trade off between speed of training and „closeness to minimum“ achieved
In practice: Lot’s of training strategies (continuous decay of learning rate, decay on plateau, manual tuning, optimizers like RMSprop) \\

\paragraph{Backpropagation}

With such a small graph, we could compute $\frac{\partial c}{\partial a}$ in one go, by determining the derivative of c with respect to a. However, this would be
impractical for more extensive graphs. Instead of a naive direct computation of the gradient with respect to each weight individually, we determine the gradient using backpropagation. The backpropagation algorithm works by computing the gradient one layer at a time by the chain rule and iterating backward from the last layer to avoid redundant calculations of intermediate terms. Using the chain rule makes it suitable for graphs of arbitrary size and for training multilayer networks. According to this rule, to calculate $\frac{\partial c}{\partial a}$, we need to do the following:

\begin{enumerate}
	\item we traverse the graph backwards from c to a.
	\item we compute the local gradient for each intervening operation, that is, the derivative of the output of that operation after its input.
	\item we multiply all local gradients.
\end{enumerate}

% You will probably never have to implement the backpropagation algorithm yourself in real-world projects as modern ML libraries already bring their own ready-made implementations. Its good to understand how it works though, as it helps when something is failing.

\subsection{A practical example}

This example is done for the above MLP and shows the backpropagation step to weight 2.

$$\frac{\partial L}{\partial w^{(2)}} = \frac{\partial L}{\partial a^{(2)}} * \frac{\partial a^{(2)}}{\partial z^{(2)}} * \frac{\partial z^{(2)}}{\partial w^{(2)}}$$

Remember, as we want to decrease the gradient we still need to have the derivatives. This means, in order to backpropagate to the second weight (as in this example), we need the derivative of the cost function $\frac{\partial L}{\partial a^{(2)}}$, if the activation (sigmoid) function, $\frac{\partial a^{(2)}}{\partial z^{(2)}}$, and the the linear function of the inputs $\frac{\partial z^{(2)}}{\partial w^{(2)}}$. \\

Let‘s start with the cost-function: $L = \frac{1}{2}(y_{true}-\hat{y})^2$

\begin{align*}
	\frac{\partial L}{\partial \hat{y}}	&= \frac{1}{2}*2*(y_{true}-\hat{y})*(-1) \\
										&= (y - \hat{y}) \text{  with  } \hat{y} = a^2 \\
										&= (y - a^{(2)})
\end{align*}

Now lets get the derivative of the (sigmoid) activation function $\hat{y} = a^{(2)} = \sigma(z^{(2)})$ with $\sigma(z^{(2)}) = \sigma(1 - \sigma)$ and $a = \sigma$ follows:

$$\frac{\partial a^{(2)}}{\partial z^{(2)}} = (a^{(2)}(1-a^{(2)}))$$

This leaves us with the derivative of the linear neuron input: $z^{(2)} = a^{(1)}w^{(2)}+b^{(2)}$

$$\frac{\partial z^{(2)}}{\partial w^{(2)}} = a^{(1)}$$


Now this was only a backpropagation to the second weight. If we want to go back to the bottom of the net to calculate the influence of w1 to the output, we have for the weights:

$$\frac{\partial L}{\partial w^{(1)}} = \frac{\partial L}{\partial a^{(2)}} * \frac{\partial a^{(2)}}{\partial z^{(2)}} * \frac{\partial z^{(2)}}{\partial a^{(1)}} * \frac{\partial a^{(1)}}{\partial z^{(1)}} * \frac{\partial z^{(1)}}{\partial w^{(1)}}$$

and for the bias:

$$\frac{\partial L}{\partial b^{(1)}} = \frac{\partial L}{\partial a^{(2)}} * \frac{\partial a^{(2)}}{\partial z^{(2)}} * \frac{\partial z^{(2)}}{\partial a^{(1)}} * \frac{\partial a^{(1)}}{\partial z^{(1)}} * \frac{\partial z^{(1)}}{\partial b^{(1)}}$$



This leaves us with the calculated gradients, respectively the influence of $w^{(2)}$ on the output $a^{(2)}$. Finally we need to update the weights to minimize the loss of the next evaluation of the mode. This is done by multiplying the calculated gradient with the learning rate and adding the current weight, like:

$$w^{(2)}_{new} = \alpha * \frac{\partial L}{\partial w^{(2)}} + w^{(2)}$$

with $\alpha$ as the learning rate. Unwinding the function gives us:

\begin{align*}
	w^{(2)}_{new}	&= \alpha * \frac{\partial L}{\partial w^{(2)}} + w^{(2)} \\
					&= \alpha * \frac{\partial L}{\partial a^{(2)}} * \frac{\partial a^{(2)}}{\partial z^{(2)}} * \frac{\partial z^{(2)}}{\partial w^{(2)}} + w^{(2)} \\
					&= \alpha * (y - a^{(2)}) * (a^{(2)}(1-a^{(2)})) * a^{(1)} + w^{(2)} \\
\end{align*}

Now this has to be done to update the weight $w^{(1)}$ and the biases $b^{(2)}$ and $b^{(1)}$ as well. And this is only for two layers - image how much work it is to calculate it with even more!
%
%% -------------------------------------------------------------------------------
%
\section{Hyperparameters}	
%
%% -------------------------------------------------------------------------------
%
In the above example as well as calculations, we took some things for granted. However, there are multiple possibilities for different problems and sometimes finding the best trained networks means adjusting the hyperparamters
%
%% -------------------------------------------------------------------------------
%
\subsection{Activation functions}	
%
%% -------------------------------------------------------------------------------
%

\paragraph{Tanh}

\paragraph{ReLU}

Rectified Non-Linear unit (ReLU), which combats the vanishing gradient problem occurring in sigmoids. ReLU is easier to compute and generates sparsity (not always beneficial).

\paragraph{Leaky ReLU}

\paragraph{Softmax}

%
%% -------------------------------------------------------------------------------
%
\subsection{Loss functions}	
%
%% -------------------------------------------------------------------------------
Loss is the prediction error of Neural Net and Loss Function. It's a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results. We want our loss function do minimize. The Loss is later used to calculate the gradients. 

In calculating the error of the model during the optimization process, a loss function must be chosen.

This can be a challenging problem as the function must capture the properties of the problem and be motivated by concerns that are important to the project and stakeholders.

%
\paragraph{Binary cross entropy}

\paragraph{Categorical (Multiclass) cross entropy}
%
%% -------------------------------------------------------------------------------
%
\subsection{Optimizers}	
%
%% -------------------------------------------------------------------------------
%
optimizer for gradient descent optimization problem and is directly influences by the learning rate. keep in mind that gradients are used to update the weights of the Neural Net.

\paragraph{SDG}

SDG: gradient can be approximated with small number of batches, not the entire dataset (otherwise computationally expensive) 

\paragraph{RmsProp}

RmsProp: adapts learning rate in each step, by weighting with GD-values. Large GD-values get penalized => adapted speed

\paragraph{Adam}

Adam: Includes updated learning rate and smoothened Gradient Descent direction (accumulates direction from previous steps) 


%
%% -------------------------------------------------------------------------------
%
\section{Coding an ANN}	
%
%% -------------------------------------------------------------------------------
%

Deep learning extends the topic by adding more layers. the more layers, the more advances probleems. However, going deep is also expensive and sometimes not the best choice.

How to battle overfitting and achieve generalization?

Regularization of the model!
For Neural Networks: 
Add penalties for large weights in the loss function. L1- or L2-Norm can be used for the weights
Dropout: Train multiple architectures at the same time by randomly “dropping out” nodes in the network


Now let's create the XOR problem we had in the initial chapter with the initial setting.
optimizer adam, activation function sigmoid and loss mean squared error
%
% % For both models, mean accuracy is computed across pilots.  \citet{Klaproth.2019}
%
%
\clearpage
%
%% -------------------------------------------------------------------------------
%
\subsection{...plain and custom}	
%
%% -------------------------------------------------------------------------------
%
\vspace{-1cm}
\begin{algorithm}
	\begin{lstlisting}[language=Python]
	import numpy as np 

	def sigmoid (x):
	    return 1/(1 + np.exp(-x))

	def sigmoid_derivative(x):
	    return x * (1 - x)

	# Input datasets
	inputs = np.array([[0,0],[0,1],[1,0],[1,1]])
	y_true = np.array([[0],[1],[1],[0]])

	epochs = 10000
	lr = 0.1
	input_size, hidden_size, output_size = 2, 2, 1

	# Random weights and bias initialization
	w_1 = np.random.uniform(size=(input_size, hidden_size))
	b_1 = np.random.uniform(size=(1, hidden_size))
	w_2 = np.random.uniform(size=(hidden_size, output_size))
	b_2 = np.random.uniform(size=(1, output_size))

	# Training the network
	for i in range(epochs):
			# Forward propagation
	    z_1 = np.dot(inputs, w_1) + b_1
	    a_1 = sigmoid(z_1)
	    z_2 = np.dot(a_1, w_2) + b_2
	    a_2 = sigmoid(z_2)
	    
	    y_hat = a_2
	    
	    # Calculation of Loss (cost function)
	    L = np.mean(0.5 * np.square(y_true - y_hat))

			# Backpropagation
	    d_y_hat = y_true - y_hat
	    d_z_2 = d_y_hat * sigmoid_derivative(a_2)
	    d_a_1 = d_z_2.dot(w_2.T)
	    d_z_1 = d_a_1 * sigmoid_derivative(a_1) 
		# d_z_1 is the long dot product in 1.5

		# Updating Weights and Biases
	    w_2 += a_1.T.dot(d_z_2) * lr
	    b_2 += np.sum(d_z_2, axis=0, keepdims=True) * lr
	    w_1 += inputs.T.dot(d_z_1) * lr
	    b_1 += np.sum(d_z_1, axis=0, keepdims=True) * lr

	print(f"Output after training 10,000 epochs: {a_2}")

\end{lstlisting}
\caption[Custom implementation of the \textit{XOR} Problem]{Custom implementation of the \textit{XOR} Problem.}
\label{alg:xor_ann_own}	
\end{algorithm}
%\vspace{0.5cm}

%
\clearpage


%
%% -------------------------------------------------------------------------------
%
\subsection{...with Keras}	
%
%% -------------------------------------------------------------------------------
%

%
% Schaubild mit dem CM strategien und Modellierungen
%
\vspace{0.5cm}
\begin{algorithm}
	\begin{lstlisting}[language=Python]
	import numpy as np
	from tensorflow import keras
	from tensorflow.keras import layers

	# Input datasets
	# y_true is denoted as y_train 
	# inputs are denoted as x_train
	x_train = np.array([[0,0],[0,1],[1,0],[1,1]])
	y_train = np.array([[0],[1],[1],[0]])

	# hyperparameters
	epochs = 10000
	lr = 0.1

	# model size
	input_shape, hidden_shape, output_shape = 2, 2, 1

	model = keras.models.Sequential(
		[
			keras.Input(shape = input_shape),
			layers.Dense(units = hidden_shape, activation = 'sigmoid'),
			layers.Dense(units = output_shape, activation = 'sigmoid')
		]
	)

	print(model.summmary())

	model.compile(loss = 'mean_squared_error', 
								optimizer = 'adam',
				  			metrics = ['mean_squared_error']
				  			)

	# train the model according to y_true
	model.fit(x_train, y_train, epochs = epochs)

	predictions = model.predict(x_train)
	print(predictions)
\end{lstlisting}
\caption[Keras implementation of the \textit{XOR} Problem]{Exemplary implementation of the \textit{XOR} Problem by implementing an ANN using Keras.}
\label{alg:xor_ann_keras}	
\end{algorithm}
\vspace{0.5cm}
%
\vspace{2em}
%

%
%% -------------------------------------------------------------------------------
%
\subsection{...for images}	
%
%% -------------------------------------------------------------------------------
%

basically the same. However, the data are somewhat different. We now have an array. we have to flatten first. example we have images of numbers 0 to 9 and what to classify which number it is. This already tells us the number of output layers.

input, hidden layers, 


\vspace{0.5cm}
\begin{algorithm}
	\begin{lstlisting}[language=Python]
	from tensorflow import keras
	from tensorflow.keras import layers
	import matplotlib.pyplot as plt
	from collections import Counter

	# load the data
	(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

	# check the shape of the training data and how many labels there are
	print(X_train.shape)
	# (60000, 28, 28)
	label_len = len([*Counter(y_train)])
	# 10

	# reshape image to vector and normalize between 0 and 1
	img_size = 28*28
	X_train_flat = X_train.reshape(X_train.shape[0], img_size) / 255
	X_test_flat = X_test.reshape(X_test.shape[0], img_size) / 255
	
	epochs=10
	lr = 0.001

	input_shape, hidden_shape, output_shape = img_size, 128, label_len

	model = keras.models.Sequential(
	    [
	        keras.Input(shape=input_shape),
	        layers.Dense(units=hidden_shape, activation='relu'),
	        layers.Dense(units=output_shape, activation='softmax'),
	    ]
	)

	model.compile(  loss = 'sparse_categorical_crossentropy', 
	                optimizer = keras.optimizers.Adam(lr), 
	                metrics = ['mean_squared_error','accuracy']
	             )
	
	model.fit(X_train_flat, y_train, epochs = epochs)

	predictions = model.predict(X_test_flat)
	print(f"Predictions: {predictions}")

	# plot loss and accuracy
	plt.plot(history.history['accuracy'])
	plt.plot(history.history['loss'])
	plt.legend(['accuracy','loss'], loc='upper right')  
	plt.show()
\end{lstlisting}
\caption[Keras implementation for \textit{MNIST} classification]{Keras implementation for \textit{MNIST} classification}
\label{alg:mnist_ann_keras}	
\end{algorithm}
\vspace{0.5cm}



https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d

https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234

https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226

https://towardsdatascience.com/entropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65

https://towardsdatascience.com/understanding-maximum-likelihood-estimation-fa495a03017a

https://towardsdatascience.com/the-five-discrete-distributions-every-statistician-should-know-131400f77782

https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23

https://towardsdatascience.com/importance-of-loss-function-in-machine-learning-eddaaec69519

https://towardsdatascience.com/tagged/loss-function

https://towardsdatascience.com/backpropagation-for-people-who-are-afraid-of-math-936a2cbebed7
%