[["index.html", "MLOps Engineering Building, Deploying, and Managing Machine Learning Workflows with Airflow and MLflow on Kubernetes. Summary", " MLOps Engineering Building, Deploying, and Managing Machine Learning Workflows with Airflow and MLflow on Kubernetes. Sebastian Blum 2023-03-09 Summary "],["preamble.html", "Preamble", " Preamble This project started out of an interest in multiple domains. At first, I was working on a Kubeflow platform at the time and haven’t had much experience in the realm of MLOps. I was starting with K8s and Terraform and was interested to dig deeper. I did so by teaching myself and I needed a project. What better also to do than to build “my own” MLOps plattform. I used Airflow since it is widely uses for workflow management and I also wanted to use it from the perspective of a data scientist, meaning to actually build some pipelines with it. This idea of extending the project to actually have a running use case expanded this work to include MLFlow for model tracking. Topics The overall aim is to build and create a MLOps architecture based on Airflow running on AWS EKS. Ideally, this architecture is create using terraform. Model tracking might be done using MLFlow, Data tracking using DVC. Further mentioned might be best practices in software development, CI/CD, Docker, and pipelines. I might also include a small Data Science use case utilizing the Airflow Cluster we built. A work in progress This project / book / tutorial / whatever this is or will be startet by explaining the concept of Kubernetes. The plan is to continuously update it by further sections. Since there is no deadline, there is no timeline, and I am also not sure whether there will exist something final to be honest. This document is written during my journey in the realm of MLOps. It is therefore in a state of continuous development. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction “MLOps Engineering: Building, Deploying, and Managing Machine Learning Workflows with Airflow and MLflow on Kubernetes” is a comprehensive guide to building and deploying machine learning models using Airflow and MLflow on an Kubernetes cluster running in a cloud environment. The book is designed for Data Scientists, Data Engineers and DevOps practitioners who are looking to dive into the realm of machine learning operations (MLOps) and automate the deployment of their models in a production environment. The book begins by introducing the concepts of MLOps and the challenges associated with building, deploying, and managing machine learning models in production. It then covers the basics of Airflow, a powerful tool for building and managing machine learning workflows, and MLflow, a platform for managing the machine learning lifecycle. Next, the book covers the basics of Kubernetes, an open-source system for automating the deployment, scaling, and management of containerized applications, and Terraform, a tool for provisioning and managing infrastructure as code. Finally, the book covers the process of deploying Airflow and MLflow on an Kubernetes cluster in a cloud environment using Terraform. Hencefore it introduces an exemplary implementation of the previously introduced tools. A machine learning workflow using Airflow is set up on the deployed infrastructure, including data preprocessing, model training, and model deployment, as well as tracking the experiment and deploying the model into production using MLFlow. "],["machine-learning-workflow.html", "1.1 Machine Learning Workflow", " 1.1 Machine Learning Workflow A machine learning workflow typically involves several stages. These stages are closely related and sometimes overlap as some stages may involve multiple iterations. In the following, the machine learning workflow is broken down to five different stages to make things easier, and give an overview. Bild nicht gefunden: images/01-Introduction/machine-learning-workflow.png 1. Data Preparation In the first stage, data used to train a machine learning model is collected, cleaned, and preprocessed. Preprocessing includes tasks to remove missing or duplicate data, normalize data, and split data into a training and testing set. 2. Model Building In the second stage, a machine learning model is selected and trained using the prepared data. This includes tasks such as selecting an appropriate algorithm as a machine learning model, training the model, and tuning the model’s parameters to improve its performance. 3. Model Evaluation Afterward, the performance of the trained model is evaluated using the test data set. This includes tasks such as measuring the accuracy and other performance metrics, comparing the performance of different models, and identifying potential issues with the model. 4. Model Deployment Finally, the selected and optimized model is deployed to a production environment where it can be used to make predictions on new data. This stage includes tasks like scaling the model to handle large amounts of data, and deploying the model to different environments to be used in different contexts 5. Model Monitoring and Maintenance It is important to monitor the model performance and update the model as needed, once the model is deployed. This includes tasks such as collecting feedback from the model, monitoring the model’s performance metrics, and updating the model as necessary. Each stage is often handled by the same tool or platform which makes a clear differentiation across stages and tools fairly difficult. Further, some machine learning workflows will not have all the steps, or they might have some variations. A machine learning workflow is thereby not a walk in the park and the actual model code is just a small piece of the work. Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex. (D. Sculley, et al., 2015) Working with and developing machine learning models, monitoring their performance, and continuously retraining it on new data with possible alternative models can be challenging and involves the right tools. 1.1.1 ML Worflow Tools There are several machine learning workflow tools that can help streamline the process of building and deploying machine learning models. By Integrating them into the machine learning workflow there can be three main processes and functionalities of tools derived: (1) Workflow Management, (2) Model Tracking, and (3) Model Serving. All three of these processes are closely related to each other and are often handled by the same tool or platform. 1.1.1.1 Workflow Management Workflow Management is the process of automating and streamlining the stages involved in building, training, and deploying machine learning models. This includes tasks such as data preprocessing, model training, and model deployment. Workflow management tools allow for the coordination of all the necessary stages in the machine learning pipeline, and the tracking of the pipeline’s progress. Apache Airflow is an open-source platform for workflow management and is widely used to handle ETL-processes of data, or automate the training and deployment of machine learning models. 1.1.1.2 Model Tracking Model Tracking is the process of keeping track of different versions of a machine learning model, including their performance, and the parameters and data used to train it. Model Tracking tools are often used at the development and testing stages of the machine learning workflow. During development, tracking allows to keep track of the different versions of a model, compare their performance and learning parameters, and finally help to select the best version of the model to deploy. Model tracking also allows to check the performance of a machine learning model during testing, and to assure it meets industry requirements. MLflow is an open-source platform to manage the machine learning lifecycle, including experiment tracking, reproducibility, and deployment. Similarly, DVC (Data Version Control) is a tool that allows to version control, manage, and track not only models but also data. 1.1.1.3 Model Serving Model Serving refers to the process of deploying a machine learning model in a production environment, so it can be used to make predictions on new data. This includes tasks such as scaling the model to handle large amounts of data, deploying the model to different environments, and monitoring the performance of the deployed model. Model serving tools are specifically used at the deployment stage of the machine learning workflow and can handle the necessary tasks mentioned beforehand. There are multiple tools that integrate the funtionality of serving models, each different in its specific use cases, for example TensorFlow, Kubernetes, DataRobot, or also the already mentioned tools MLflow and Airflow. 1.1.2 Developing Machine Learning Models In the development phase of a machine learning model, many stages of the machine learning workflow are carried out manually. For instance, testing machine learning code is often done in a notebook or script, rather than through an automated pipeline. Similarly, deploying the model is typically a manual process, and only involves serving the model for inference, rather than deploying an entire machine learning system. This manual deployment process can result in infrequent deployments and the management of only a few models that do not change frequently. Additionally, the deployment process is usually not handled by Data Scientists but is instead managed by an operations team. This can create a disconnection between the development and production stages, and once deployed, there is typically no monitoring of the model’s performance, meaning no feedback loop is established for retraining. Performing stages manually and managing models in production at scale can become exponentially more challenging. To overcome these issues, it is recommended to adopt a unifying framework for production, known as MLOps. This framework will shift the focus from managing existing models to developing new ones, making the process more efficient and effective. "],["machine-learning-operations-mlops.html", "1.2 Machine Learning Operations (MLOps)", " 1.2 Machine Learning Operations (MLOps) Machine Learning Operations (MLOps) combines principles from developing and deploying machine learning models to enable an efficient operation and management of machine learning models in a production environment. The goal of MLOps is to automate and streamline the machine learning workflow as much as possible, while also ensuring that each machine learning model is performing as expected. This allows organizations to move quickly and efficiently from prototyping to production. The infrastructure involved in MLOps includes both, hardware and software components. Hardware components aim to make the deployment of machine learning models scalable. This includes servers and storage devices for data storage and model deployment, as well as specialized hardware such as GPUs for training and inferencing. Software components include version control systems, continuous integration and continuous deployment (CI/CD) tools, containerization and orchestration tools, and monitoring and logging tools. There are several cloud-based platforms that provide pre-built infrastructure to manage and deploy models, automate machine learning workflows, and monitor and optimize the performance of models in production. For example AWS SageMaker, Azure Machine Learning, and Google Cloud AI Platform. Overall, the key concepts and best practices of MLOps, and the use of tools and technologies to support the MLOps process are closely related to regular DevOps principles which are a standard for the operation of software in the industry. 1.2.1 ML + (Dev)-Ops Machine Learning Operations (MLOps) and Development Operations (DevOps) both aim to improve the efficiency and effectiveness of software development and deployment. Both practices share major similarities, but there are some key differences that set them apart. The goal of DevOps is to automate and streamline the process of building, testing, and deploying software, to make the management of the software lifecycle as quick and efficient as possible. This can be achieved by using tools, such as: Containerization: Tools such as Docker and Kubernetes are used to package and deploy applications and services in a consistent and reproducible manner. Version Control: Tools such as Git, SVN, and Mercurial are used to track and manage changes to code and configuration files, allowing teams to collaborate and roll back to previous versions if necessary. Continuous Integration and Continuous Deployment (CI/CD): Tools such as Jenkins, Travis CI, and Github Actions are used to automate the building, testing, and deployment of code changes. Monitoring and Logging: Tools such as Prometheus, Grafana, and ELK are used to monitor the health and performance of applications and services, as well as to collect and analyze log data. Cloud Management Platforms: AWS, Azure, and GCP are used to provision, manage, and scale infrastructure and services in the cloud. Scaling and provisioning infrastructure can be automated by using Infrastructure as Code (IaC) Tools like Terraform. DevOps focuses on the deployment and management of software in general (or traditional sofware), while MLOps focuses specifically on the deployment and management of machine learning models in a production environment. The goal is basically the same as in DevOps, yet deploying a machine learning model. While this is achieved by the same tools and best practices used in DevOps, deploying machine learning models (compared to software) adds a lot of complexity to the process. Traditional vs ML software Machine learning models are not just lines of code, but also require large amounts of data, and specialized hardware, to function properly. Further, machine learning models and their complex algorithms might need to change when there is a shift in new data. This process of ensuring that machine learning models are accurate and reliable with new data leads to additional challenges. Another key difference is that MLOps places a great emphasis on model governance, which ensures that machine learning models are compliant with relevant regulations and standards. The above list of tools within DevOps can be extended to the following for MLOps. Machine Learning Platforms: Platforms such as TensorFlow, PyTorch, and scikit-learn are used to develop and train machine learning models. Experiment Management Tools: Tools such as MLflow, Weights &amp; Biases, and Airflow are used to track, version, and reproduce experiments, as well as to manage the model lifecycle. Model Deployment and Management Tools: Tools such as TensorFlow Serving, Clipper, and Seldon Core are used to deploy and manage machine learning models in production. Data Versioning and Management Tools: Tools such as DVC (Data Version Control) and Pachyderm are used to version and manage the data used for training and evaluating models. Automated Model Evaluation and Deployment Tools: Tools such as AlgoTrader and AlgoHub are used to automate the evaluation of machine learning models and deploy the best-performing models to production. It’s important to note that the specific tools used in MLOps and DevOps may vary depending on the organization’s needs. Some of the tools are used in both but applied differently in each, e.g. container orchestration tools like Kubernetes. The above lists do also not claim to be complete and there are of course multiple more tools. 1.2.2 MLOps Lifecycle Incorporating the tools introduced by DevOps and MLOps can extend the machine learning workflow outlined in the previous section, resulting in a complete MLOps lifecycle that covers each stage of the machine learning process while integrating automation practices. Integrating MLOps into machine learning projects introduces additional complexity into the workflow. Although the development stage can be carried out on a local machine, subsequent stages are typically executed within a cloud platform. Additionally, the transition from one stage to another is automated using tools like CI/CD, which automate testing and deployment. MLOps also involves integrating workflow management tools and model tracking to enable monitoring and ensure reproducible model training. This enables proper versioning of code, data, and models, providing a comprehensive overview of the project. 1.2.3 MLOps Engineering MLOps Engineering is the discipline that applies the previously mentioned software engineering principles to create the necessary development and production environment for machine learning models. It usually comines the skills and expertise of Data Scientists and -Engineers, Machine Learning Engineers, and DevOps engineers to ensure that machine learning models are deployed and managed efficiently and effectively. One of the key aspects of MLOps Engineering is infrastructure management. The infrastructure refers to the hardware, software and networking resources that are needed to support the machine learning models in production. The underlying infrastructure is crucial for the success of MLOps as it ensures that the models are running smoothly, are highly available and are able to handle the load of the incoming requests. It also helps to prevent downtime, ensure data security and guarantee the scalability of the models. MLOps Engineering is responsible for designing, building, maintaining and troubleshooting the infrastructure that supports machine learning models in production. This includes provisioning, configuring, and scaling the necessary resources like cloud infrastructure, Kubernetes clusters, machine learning platforms, and model serving infrastructure. It is useful to use configuration management and automation tools like Infrastructure as Code (e.g. Terraform) to manage the infrastructure in a consistent, repeatable and version controlled way. This allows to easily manage and scale the infrastructure as needed, and roll back to previous versions if necessary. It is important to note that one person can not exceed on all of the above mentioned tasks of designing, building, maintaining, and troubleshooting. Developing the infrastructure for machine learning models in production usually requires multiple people working with different and specialized skillsets and roles. "],["roles-and-tasks-in-mlops.html", "1.3 Roles and Tasks in MLOps", " 1.3 Roles and Tasks in MLOps The MLOps lifecycle typically involves several key roles and responsibilities, each with their own specific tasks and objectives. Roles and their operating areas within the MLOps lifecycle 1.3.1 Data Engineer A Data Engineer designs, builds, maintains, and troubleshoots the infrastructure and systems that support the collection, storage, processing, and analysis of large and complex data sets. Tasks include designing and building data pipelines, managing data storage, optimizing data processing, ensuring data quality, and monitoring the necessary data pipelines and systems. They usually work closely with Data Scientists to understand their data needs and ensure the infrastructure supports their requirements. 1.3.2 Data Scientist Data Scientists are usually responsible for developing and testing machine learning models within the development stage. Their work typically involves investigating large amounts of data, the necessary preprocessing steps, and choosing a suitable machine learning algorithm that solves the business need. They also develop a functioning ML model respective to the data. 1.3.3 ML Engineer Machine Learning (ML) Engineers work closely with Data Scientists to develop and deploy machine learning models in a production environment. They are responsible for creating and maintaining the infrastructure and tools needed to run machine learning models at scale and in production. They are also responsible for the day-to-day management and monitoring of machine learning models in a production environment. They use tools and technologies to track model performance and make sure that models are running smoothly and produce accurate results. This also includes taking measures in case of data or model shifts1. 1.3.4 MLOps Engineer The MLOps Engineer is responsible to create and maintain the infrastructure and tooling needed to train and deploy models, to monitor the performance of deployed models. They also implement processes for collaboration and version control. Overall, this includes tasks such as setting up and configuring the necessary hardware and software environments, creating and maintaining documentation, and implementing automated testing and deployment processes. An MLOps Engineer must be able to navigate this complexity and ensure that the models are deployed and managed in a way that is both efficient and effective. 1.3.5 DevOps Engineer DevOps Engineers are responsible for automating and streamlining the process of building, testing, and deploying software. They use best practices from software development and operations, such as version control, continuous integration and delivery (CI/CD), and monitoring and observability, to ensure that the software is performing as expected in production. 1.3.6 Additional roles &amp; function The previous roles only show a small portion of all contributors within data projects. Additional roles within an industry context might also include Model Governance, which is responsible to ensure that models are accurate, reliable, and compliant with relevant regulations and standards, or Business Stakeholders who provide feedback and requirements for the models in a business context. It’s also worth noting that some of these roles may overlap and different organizations may have different ways of structuring their teams and responsibilities. The most important thing is to have clear communication and collaboration between all the different teams, roles, and stakeholders involved in the MLOps lifecycle. Model shift refers to the change of a deployed model compared to the model developed in a previous stage, while data shift refers to a change in the distribution of input data compared to the data used during development and testing.↩︎ "],["mlops-engineering-with-airflow-and-mlflow-on-kubernetes.html", "1.4 MLOps Engineering with Airflow and MLflow on Kubernetes", " 1.4 MLOps Engineering with Airflow and MLflow on Kubernetes MLOps platforms can be set up in various ways to apply MLOps practices to the machine learning workflow. (1) SaaS tools provide an integrated development and management experience, with an aim to offer an end-to-end process. (2) Custom-made platforms offer high flexibility and can be tailored to specific needs. However, integrating multiple different services requires significant engineering effort. (3) Many cloud providers offer a mix of SaaS and custom-tailored platforms, providing a relatively well-integrated experience while remaining open enough to integrate other services. This work presents an example of an MLOps platform using Airflow and MLflow for management during the machine learning lifecycle. Both tools will be deployed on a Kubernetes cluster using Terraform, incorporating best practices such as CI/CD and automation. This project involves building a custom-tailored MLOps platform focused on MLOps engineering, as the entire infrastructure will be set up from scratch. However, it will also incorporate the work done by data and machine learning scientists since basic machine learning models will be implemented and run on the platform. Henceforth, the following chapters give an introductory tutorial on each of the previously introduced tools. A machine learning workflow using Airflow is set up on the deployed infrastructure, including data preprocessing, model training, and model deployment, as well as tracking the experiment and deploying the model into production using MLFlow. Architecture Overview The necessary AWS infrastructure is set up using Terraform. This includes creating an AWS EKS cluster and the associated ressources like a virtual private cloud (VPC), subnets, security groups, IAM roles, as well as further AWS ressources needed to deploy Airflow and MLflow. Once the EKS cluster is set up, Kubernetes can be used to deploy and manage applications on the cluster. Helm, a package manager for Kubernetes, is used to manage the deployment of Airflow and MLflow. The EKS cluster allows for easy scalability and management of the platforms. The code is made public on a Github repository and Github Actions is used for automating the deployment of the infrastructure using CI/CD principles. Once the infrastructure is set up, machine learning models can be deployed to the EKS cluster as Kubernetes pods, using Airflows scheduling processes. Airflow’s ability to scan local directories or Git repositories will be used to import the relevant machine learning code from second Github repository. Similarly, to building Airflow workflows, the machine learning code will also include using the MLFlow API to allow for model tracking. Github Actions is used as a CI/CD pipeline to automatically build, test, and deploy machine learning models to this repository similarly as it is used in the repository for the infrastructure. Whereas the deployment of the infrastructure would be taken care of by MLOps-, DevOps-, and Data Engineers, the development of the Airflow workflows including MLFlow would be taken care of by Data Scientist and ML Engineers. "],["ops-tools-principles.html", "1.5 Ops Tools &amp; Principles", " 1.5 Ops Tools &amp; Principles MLOps integrates a range of DevOps techniques and tools to enhance the development and deployment of machine learning models. By promoting cooperation between development and operations teams, MLOps strives to improve communication, enhance efficiency, and reduce delays in the development process. Advanced version control systems can be employed to achieve these objectives. Automation plays a significant role in achieving these goals. For instance, CI/CD pipelines streamline repetitive tasks like building, testing, and deploying software. The management of infrastructure can also be automated, by using infrastructure as code to facilitate an automated provisioning, scaling, and management of infrastructure. To enhance flexibility and scalability in the operational process, containers and microservices are used to package and deploy software. Finally, monitoring and logging tools are used to track the performance of deployed and containerized software and address any issues that arise. 1.5.1 Containerization Containerization is an essential component in operations as it enables deploying and running applications in a standardized, portable, and scalable way. This is achieved by packaging an application and its dependencies into a container image, which contains all the necessary code, runtime, system tools, libraries, and settings needed to run the application, isolated from the host operating system. Containers are lightweight, portable, and can run on any platform that supports containerization, such as Docker or Kubernetes. All of this makes them beneficial compared to deploying an application on a virtual machine or traditionally directly on a machine. Virtual machines would emulate an entire computer system and require a hypervisor to run, which introduces additional overhead. Similarly, a traditional deployment involves installing software directly onto a physical or virtual machine without the use of containers or virtualization. Not to mention the lack of portability of both. The concept of container images is analogous to shipping containers in the physical world. Like shipping containers can be loaded with different types of cargo, a container image can be used to create different containers with various applications and configurations. Both the physical containers and container images are standardized, just like blueprints, enabling multiple operators to work with them. This allows for the deployment and management of applications in various environments and cloud platforms, making containerization a versatile solution. Containerization offers several benefits for MLOps teams. By packaging the machine learning application and its dependencies into a container image, reproducibility is achieved, ensuring consistent results across different environments and facilitating troubleshooting. Containers are portable which enables easy movement of machine learning applications between various environments, including development, testing, and production. Scalability is also a significant advantage of containerization, as scaling up or down compute resources in an easy fashion allows to handle large-scale machine learning workloads and adjust to changing demand quickly. Additionally, containerization enables version control of machine learning applications and their dependencies, making it easier to track changes, roll back to previous versions, and maintain consistency across different environments. To effectively manage model versions, simply saving the code into a version control system is insufficient. It’s crucial to include an accurate description of the environment, which encompasses Python libraries, their versions, system dependencies, and more. Virtual machines (VMs) can provide this description, but container images have become the preferred industry standard due to their lightweight nature. Finally, containerization facilitates integration with other DevOps tools and processes, such as CI/CD pipelines, enhancing the efficiency and effectiveness of MLOps operations. 1.5.2 Version Control Version control is a system that records changes to a file or set of files over time, to be able to recall specific versions later. It is an essential tool for any software development project as it allows multiple developers to work together, track changes, and easily rollback in case of errors. There are two main types of version control systems: centralized and distributed. Centralized Version Control Systems (CVCS) : In a centralized version control system, there is a single central repository that contains all the versions of the files, and developers must check out files from the repository in order to make changes. Examples of CVCS include Subversion and Perforce. Distributed Version Control Systems (DVCS) : In a distributed version control system, each developer has a local copy of the entire repository, including all the versions of the files. This allows developers to work offline, and it makes it easy to share changes with other developers. Examples of DVCS include Git, Mercurial and Bazaar Version control is a vital component of software development that offers several benefits. First, it keeps track of changes made to files, enabling developers to revert to a previous version in case something goes wrong. Collaboration is also made easier with version control, as it allows multiple developers to work on a project simultaneously and share changes with others. In addition, it provide backup capabilities by keeping a history of all changes, allowing you to retrieve lost files. Version control also allows auditing of changes, tracking who made a specific change, when, and why. Finally, it enables developers to create branches of a project, facilitating simultaneous work on different features without affecting the main project, with merging later. Versioning all components of a machine learning project, such as code, data, and models, is essential for reproducibility and managing models in production. While versioning code-based components is similar to typical software engineering projects, versioning machine learning models and data requires specific version control systems. There is no universal standard for versioning machine learning models, and the definition of “a model” can vary depending on the exact setup and tools used. Popular tools such as Azure ML, AWS Sagemaker, Kubeflow, and MLflow offer their own mechanisms for model versioning. For data versioning, there are tools like Data Version Control (DVC) and Git Large File Storage (LFS). The de-facto standard for code versioning is Git. The code-versioning system Github is used for this project, which will be depicted in more detail in the following. 1.5.2.1 Github GitHub provides a variety of branching options to enable flexible collaboration workflows. Each branch serves a specific purpose in the development process, and using them effectively can help teams collaborate more efficiently and effectively. Main Branch: The main branch is the default branch in a repository. It represents the latest stable version and production-ready state of a codebase, and changes to the code are merged into the main branch as they are completed and tested. Feature Branch: A feature branch is used to develop a new feature or functionality. It is typically created off the main branch, and once the feature is completed, it can be merged back into the main branch. Hotfix Branch: A hotfix branch is used to quickly fix critical issues in the production code. It is typically created off the main branch, and once the hotfix is completed, it can be merged back into the main branch. Release Branch: A release branch is a separate branch that is created specifically for preparing a new version of the software for release. Once all the features and fixes for the new release have been added and tested, the release branch is merged back into the main branch, and a new version of the software is tagged and released. 1.5.2.2 Git lifecycle After a programmer has made changes to their code, they would typically use Git to manage those changes through a series of steps. First, they would use the command git status to see which files have been changed and are ready to be committed. They would then stage the changes they want to include in the commit using the command git add &lt;FILE-OR-DIRECTORY&gt;, followed by creating a new commit with a message describing the changes using git commit -m \"MESSAGE\". After committing changes locally, the programmer may want to share those changes with others. They would do this by pushing their local commits to a remote repository using the command git push. Once the changes are pushed, others can pull those changes down to their local machines and continue working on the project by using the command git pull. If the programmer is collaborating with others, they may need to merge their changes with changes made by others. This can be done using the git merge &lt;BRANCH-NAME&gt; command, which combines two branches of development history. The programmer may need to resolve any conflicts that arise during the merge. If the programmer encounters any issues or bugs after pushing their changes, they can use Git to revert to a previous version of the code by checking out an older commit using the command git checkout. Git’s ability to track changes and revert to previous versions makes it an essential tool for managing code in collaborative projects. While automating the code review process is generally viewed as advantageous, it is still typical to have a manual code review as the final step before approving a pull or merge request to be merged into the main branch. It is considered a best practice to mandate a manual approval from one or more reviewers who are not the authors of the code changes. 1.5.3 CI/CD Continuous Integration (CI) and Continuous Delivery / Continuous Delivery (CD) are related software development practices that work together to automate and streamline the software development and deployment process of code changes to production. Deploying new software and models without CI/CD often requires a lot of implicit knowledge and manual steps. Continuous Integration (CI): is a software development practice that involves frequently integrating code changes into a shared central repository. The goal of CI is to catch and fix integration errors as soon as they are introduced, rather than waiting for them to accumulate over time. This is typically done by running automated tests and builds, to catch any errors that might have been introduced with new code changes, for example when merging a Git feature branch into the main branch. Continuous Delivery (CD): is the practice that involves automating the process of building, testing, and deploying software to a production-like environment. The goal is to ensure that code changes can be safely and quickly deployed to production. This is typically done by automating the deployment process and by testing the software in a staging environment before deploying it to production. Continuous Deployment (CD): is the practice of automatically deploying code changes to production once they pass automated tests and checks. The goal is to minimize the time it takes to get new features and bug fixes into the hands of end-users. In this process, the software is delivered directly to the end-user without manual testing and verification. The terms Continuous Delivery and Continuous Deployment are often used interchangeably, but they have distinct meanings. Continuous delivery refers to the process of building, testing, and running software on a production-like environment, while continuous deployment refers specifically to the process of running the new version on the production environment itself. However, fully automated deployments may not always be desirable or feasible, depending on the organization’s business needs and the complexity of the software being deployed. While continuous deployment builds on continuous delivery, the latter can offer significant value on its own. CI/CD integrates the principles of continuous integration and continuous delivery in a seamless workflow, allowing teams to catch and fix issues early and quickly deliver new features to users. The pipeline is often triggered by a code commit. Ideally, a Data Scientist would push the changes made to the code at each incremental step of development to a share repository, including metadata and documentation. This code commit would trigger the CI/CD pipeline to build, test, package, and deploy the model software. In contrast to the local development, the CI/CD steps will test the model changes on the full dataset and aiming to deploy for production. CI and CD practices help to increase the speed and quality of software development, by automating repetitive tasks and catching errors early, reducing the time and effort required to release new features, and increasing the stability of the deployed software. Examples for CI/CD Tools that enable automated testing with already existing build servers are for example GitHub Actions, Gitlab CI/CD, AWS Code Build, or Azure DevOps The following code snippet shows an exemplary GitHub Actions pipeline to test, build and push a Docker image to the DockerHub registry. The code is structured in three parts. At first, the environment variables are defined under env. Two variables are defined here which are later called with by the command env.VARIABLE. The second part defines when the pipeline is or should be triggered. The exampele shows three possibilites to trigger a pipelines, when pushing on the master branch push, when a pull request to the master branch is granted pull_request, or when the pipeline is triggered manually via the Github interface workflow_dispatch. The third part of the code example introduces the actual jobs and steps performed by the pipeline. The pipeline consists of two jobs pytest and docker. The first represents the CI part of the pipeline. The run environment of the job is set up and the necessary requirements are installed. Afterward unit tests are run using the pytest library. If the pytest job was successful, the docker job will be triggered. The job builds the Dockerfile and pushes it automatically to the specified Dockerhub repository specified in tags. The step introduces another variable just like the env.Variable before, the secrets.. Secrets are a way by Github to safely store classified information like username and passwords. They can be set up using the Github Interface and used in the Github Actions CI using secrets.SECRET-NAME. name: Docker CI base env: DIRECTORY: base DOCKERREPO: seblum/mlops-public on: push: branches: master paths: $DIRECTORY/** pull_request: branches: [ master ] workflow_dispatch: jobs: pytest: runs-on: ubuntu-latest defaults: run: working-directory: ./${{ env.DIRECTORY }} steps: - uses: actions/checkout@v3 - name: Set up Python uses: actions/setup-python@v4 with: python-version: &#39;3.x&#39; - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt pip install pytest pip install pytest-cov - name: Test with pytest run: | pytest test_app.py --doctest-modules --junitxml=junit/test-results.xml --cov=com --cov-report=xml --cov-report=html docker: needs: pytest runs-on: ubuntu-latest steps: - name: Set up QEMU uses: docker/setup-qemu-action@v2 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v2 - name: Login to DockerHub uses: docker/login-action@v2 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build and push uses: docker/build-push-action@v3 with: file: ./${{ env.DIRECTORY }}/Dockerfile push: true tags: ${{ env.DOCKERREPO }}:${{ env.DIRECTORY }} 1.5.4 Infrastructure as code Infrastructure as Code (IaC) is a software engineering approach that enables the automation of infrastructure provisioning and management using machine-readable configuration files rather than manual processes or interactive interfaces. This means that the infrastructure is defined using code, instead of manually setting up servers, networks, and other infrastructure components. This code can be version controlled, tested, and deployed just like any other software code. It also allows to automate the process of building and deploying infrastructure resources, enabling faster and more reliable delivery of services, as well as ensuring to provide the same environment every time. It also comes with the benefit of an increased scalability, improved security, and better visibility into infrastructure changes. It is recommended to utilize infrastructure-as-code to deploy an MLOps platform. Popular tools for implementing IaC are for example Terraform, CloudFormation, and Ansible. Chapter 6 gives a more detailed description and a tutorial on how to use Infrastructure as code using Terraform. "],["airflow.html", "Chapter 2 Airflow", " Chapter 2 Airflow Apache Airflow is an open-source platform to develop, schedule and monitor workflows. Airflow comes with a web user interface that aims to make managing workflows as easy as possible and provides a good overview of each workflow over time and the ability to inspect logs and manage tasks, for example retrying a task in case of failure. web-interface_overview However, the philosophy of Airflow is to define workflows as code, so coding will always be required. Thus, Airflow can also be referred to as a “Workflows as code”-tool that allows for a dynamic, extensible, and flexible management of its workflows. The Airflow platform contains different operators to easily extend and connect with many other technologies. Being able to manage a workflow for all stages of the training of ML models, and the possibility to combine Airflow with other tools like MLflow for model tracking, make Apache Airflow a great tool to incorporate in an MLOps architecture. The aim of this chapter is to give a tutorial on how to use Airflow from a user perspective, as well as give a short overview of its deployment. Airflow can be deployed in multiple ways, starting from a single processing unit on a local machine to a distributed setup with multiple compute resources for large workflows in a production setting. A detailed description of what an Airflow deployment involves is shown in the section Airflow Infrastructure. The usage tutorial is based on the local installation of Airflow. Please refer to the prerequisits on what is needed to follow through. Prerequisites The main prerequisites to follow this tutorial to have an Apache Airflow instance installed. The official documentation gives a good overview on how to do. It is sufficient to run Airflow on a local deployment and there is no need to to set up a complex Airflow deployment on a cluster or else. Further needed is intermediate knowledge of the programming language Python and basic knowledge of bash. "],["core-components.html", "2.1 Core Components", " 2.1 Core Components Airflow serves as a batch-oriented workflow orchestration plattform. Workflows vary in their levels of complexity and in general it is a term with various meaning depending on context. When working with Airflow, a workflow usually describes a set of steps to accomplish a given data engineering tasks, e.g. downloading files, copying data, filtering information, or writing to a database, etc. An exemplary use case might be to set up an ETL pipeline that extracts data from multiple sources, the transformation of the data, as well as loading them into a machine learning model. Even the training itself of a ML model can triggered via Airflow. Another workflow step might involve the generation of a report or backups. Even though Airflow can implement programs from any language, the workflows are written and defined as Python code. Airflow allows to access each workflow via the previously mentioned web user interface, or via a command-line interface. Writing workflows in code has the benefit to use version control systems like Git to ensure roll backs to previous versions as well as to develop a workflow with a team of developers simultaneously. Using Git also allows to include further DevOps principles such as pipelines, and testing and validating the codes functionality. 2.1.1 DAGs A workflows in Airflow is implemented as a DAG, a Directed Acyclic Graph. A graph describes the actual set of components of a workflow. It is directed because it has an inherent flow representing dependencies between its components. It is acyclic as it does not loop or repeat. The DAG object is needed to nest the separate tasks intp a workflow. A workflow specified in code, e.g. python, is often also referred to as a pipeline. This terminology can be used synonymosly when working with Airflow. The following code snippet depicts how to define a DAG object in python code. The dag_id string is a unique identifier to the DAG object. The default_args dictionary consists of additional parameters that can be specified. There are only shown two additional parameters. There are a lot more though which can be seen in the official documentation. from airflow.models import DAG from pendulum import datetime # Using extra arguments allows to customize in a clear structure # e.g. when setting the time zone in a DAG. default_args = { &#39;start_date&#39;: datetime(2023, 1, 1, tz=&quot;Europe/Amsterdam&quot;), &#39;schedule_interval&#39;: &#39;None&#39; } example_dag = DAG( dag_id=&#39;DAG_fundamentals&#39;, default_args=default_args ) The state of a workflow can accessed via the web user inteface, such as shown in below images. The first image shows Airflows overview of all DAGs currently “Active” and further information about them such as their “Owner”, how many “Runs” have been performaned and whether they were successful, and much more. The second image depicts a detailed overview of the DAG “xcom_fundamentals”. Besudes looking into the Audit Logs of the DAG and the Task Duration, it is also possible to check the Code of the DAG. web-interface_dags web-interface_dag-grid The Airflow command line interface also allows to interact with the DAGs. Below command shows its exemplary usage and how to list all active DAGs. Further examples of using the CLI in a specific context are show in the subsection about Tasks. # Create the database schema airflow db init # Print the list of active DAGs airflow dags list People sometimes think of a DAG definition file as a place where the actual data processing is done. That is not the case at all! The scripts purpose is to only define a DAG object. It needs to evaluate quickly (in seconds, not minutes) since the scheduler of Airflow will load and execute it periodically to account for changes in the DAG definition. A DAG usually consists of multiple steps it runs through, also names as tasks. Tasks themselves consist of operators. This will be outlined in the following subsections. 2.1.2 Operators An Operator represents a single predefined task in a workflow. It is basically a unit of work that Airflow has to complete. Operators usually run independently and generally do not share any information by themselves. There are different categories of operators to perform different tasks with, for example Action operators, Transfer operators, or Sensors. Action Operators execute a basic task based on the operators specifications. Examples are the BashOperator, the PythonOperator, or the EmailOperator. The operator names already suggest what kind of executions they provide; the PythonOperator runs python tasks. Transfer Operators are designed to transfer data from one place to another, for example to copy data from one cloud bucket to another. Those operators are often stateful, which means the downloaded data are first stored locally and then uploaded to a destination storage. The principle of a stateful execution defines them as an own category of operator. Finally, Sensors are a special subclass of operators that are triggered when an external event is happening, for example the creation of a file. The PythonOperator is actually declared a deprecated function. Airflow 2.0 promotes to use the @task-decorator of Taskflow to define tasks in a more pythonic context. Yet, it still works in Airflow 2.0 and is still a very good example on how operators work. 2.1.2.1 Action Operators BashOperator As its name suggests, the BashOperator executes commands in the bash shell. from airflow.operators.bash_operator import BashOperator bash_task = BashOperator( task_id=&#39;bash_example&#39;, bash_command=&#39;echo &quot;Example Bash!&quot;&#39;, dag=action_operator_fundamentals ) PythonOperator The PythonOperator expects a python callable. Airflow passes a set of keyword arguments from the op_kwargs dictionary to the callable as input. from airflow.operators.python_operator import PythonOperator def sleep(length_of_time): time.sleep(length_of_time) sleep_task = PythonOperator( task_id=&#39;sleep&#39;, python_callable=sleep, op_kwargs={&#39;length_of_time&#39;: 5}, dag=action_operator_fundamentals ) EmailOperator The EmailOperator allows to send predefined emails from an Airflow DAG run. For example, this could be used to notify if a workflow was successfull or not. The EmailOperator does require the Airflow system to be configured with email server details as a prerequisite. Please refer to the official docs on how to do this. from airflow.operators.email_operator import EmailOperator email_task = EmailOperator( task_id=&#39;email_sales_report&#39;, to=&#39;sales_manager@example.com&#39;, subject=&#39;automated Sales Report&#39;, html_content=&#39;Attached is the latest sales report&#39;, files=&#39;latest_sales.xlsx&#39;, dag=action_operator_fundamentals ) 2.1.2.2 Transfer Operators GoogleApiToS3Operator The GoogleApiToS3Operator makes requests to any Google API that supports discovery and uploads its response to AWS S3. The example below loads data from Google Sheets and saves it to an AWS S3 file. from airflow.providers.amazon.aws.transfers.google_api_to_s3 import GoogleApiToS3Operator google_sheet_id = &lt;GOOGLE-SHEET-ID &gt; google_sheet_range = &lt;GOOGLE-SHEET-RANGE &gt; s3_destination_key = &lt;S3-DESTINATION-KEY &gt; task_google_sheets_values_to_s3 = GoogleApiToS3Operator( task_id=&quot;google_sheet_data_to_s3&quot;, google_api_service_name=&quot;sheets&quot;, google_api_service_version=&quot;v4&quot;, google_api_endpoint_path=&quot;sheets.spreadsheets.values.get&quot;, google_api_endpoint_params={ &quot;spreadsheetId&quot;: google_sheet_id, &quot;range&quot;: google_sheet_range}, s3_destination_key=s3_destination_key, ) DynamoDBToS3Operator The DynamoDBToS3Operator copies the content of an AWS DynamoDB table to an AWS S3 bucket. It is also possible to specifiy criteria such as dynamodb_scan_kwargs to filter the transfered data and only replicate records according to criteria. from airflow.providers.amazon.aws.transfers.dynamodb_to_s3 import DynamoDBToS3Operator table_name = &lt;TABLE-NAME &gt; bucket_name = &lt;BUCKET-NAME &gt; backup_db = DynamoDBToS3Operator( task_id=&quot;backup_db&quot;, dynamodb_table_name=table_name, s3_bucket_name=bucket_name, # Max output file size in bytes. If the Table is too large, multiple files will be created. file_size=20, ) AzureFileShareToGCSOperator The AzureFileShareToGCSOperator transfers files from the Azure FileShare to the Google Storage. Even though the storage systems are quite similar, this operator is beneficial when the cloud provider needs to be changed. The share_name-parameter denotes the Azure FileShare share name to transfer files from. Similarly, the dest_gcsspecifies the destination bucket on the Google Cloud. from airflow.providers.google.cloud.transfers.azure_fileshare_to_gcs import AzureFileShareToGCSOperator azure_share_name = &lt;AZURE-SHARE-NAME &gt; bucket_name = &lt;BUCKET-NAME &gt; azure_directory_name = &lt;AZURE-DIRECTORY-NAME &gt; sync_azure_files_with_gcs = AzureFileShareToGCSOperator( task_id=&quot;sync_azure_files_with_gcs&quot;, share_name=azure_share_name, dest_gcs=bucket_name, directory_name=azure_directory_name, replace=False, gzip=True, google_impersonation_chain=None, ) 2.1.2.3 Sensors A sensor is a special subclass of an operator that is triggered when an external event is happening or a certain condition is be true. Such conditions are for example the creation of a file, an upload of a database record, or a certain response from a web request. It is also possible to specify further requirements to check the condition. The mode argument sets how to check for a condition. mode='poke' denotes to run a task repeatedly until it is successful (this is the default), whereas mode='reschedule' gives up a task slot and tries again later. Simultaneously, the poke_interval defines how long a sensor should wait between checks, and the timeout parameter defines how long to wait before letting a task fail. Below example shows a FileSensor that checks the creation of a file with a poke_interval defined. from airflow.contrib.sensors.file_sensor import FileSensor file_sensor_task = FileSensor(task_id=&#39;file_sense&#39;, filepath=&#39;salesdata.csv&#39;, poke_interval=30, dag=sensor_operator_fundamentals ) Other sensors are for example: * ExternalTaskSensor - waits for a task in another DAG to complete * HttpSensor - Requests a web URL and checks for content * SqlSensor - Runs a SQL query to check for content 2.1.3 Tasks To use an operator in a DAG it needs to be instantiated as a task. Tasks determine how to execute an operator’s work within the context of a DAG. The concepts of a Task and Operator are actually somewhat interchangeable as each task is actually a subclass of Airflow’s BaseOperator. However, it is useful to think of them as separate concepts. Tasks are Instances of operators and are usually assigned to a python variable. The following code instantiates the BashOperator to two different variables task_1 and task_2. The depends_on_past argument ensures that the previously scheduled task has succeeded before the current task is triggered. task_1 = BashOperator( task_id=&quot;print_date&quot;, bash_command=&quot;date&quot;, dag=task_fundamentals ) task_2 = BashOperator( task_id=&quot;set_sleep&quot;, depends_on_past=False, bash_command=&quot;sleep 5&quot;, retries=3, dag=task_fundamentals ) task_3 = BashOperator( task_id=&quot;print_success&quot;, depends_on_past=True, bash_command=&#39;echo &quot;Success!&quot;&#39;, dag=task_fundamentals ) Tasks can be referred to by their task_id either using the web interface or using the CLI within the airflow tools. airflow tasks test runs task instances locally, outputs their log to stdout (on screen), does not bother with dependencies, and does not communicate the state (running, success, failed, …) to the database. It simply allows testing a single task instance. The same accounts for airflow dags test # Run a single task with the following command airflow run &lt;dag_id&gt; &lt;task_id&gt; &lt;start_date&gt; # Run tasks locally for testing airflow tasks test &lt;dag_id&gt; &lt;task_id&gt; &lt;input-parameter&gt; # Testing the task print_date airflow tasks test task_fundamentals print_date 2015-06-01 # Testing the task sleep airflow tasks test task_fundamentals sleep 2015-06-01 2.1.3.1 Task dependencies A machine learning or data workflow usually has a specific order in which its tasks should run. Airflow alloes to define a specific order of task completion using task dependencies that are either referred to as upstream or downstream tasks. In Airflow 1.8 and later, this can be defined using the bitshift operators in python. &gt;&gt; - or the upstream operator (before) &lt;&lt; - or the downstream operator (after) An exemplary code and chaining examples of tasks would look like this: # Simply chained dependencies task_1 &gt;&gt; task_2 &gt;&gt; task_3 task-dependencies # Mixed dependencies task_1 &gt;&gt; task_3 &lt;&lt; task_2 # which is similar to task_1 &gt;&gt; task_3 task_2 &gt;&gt; task_3 # or [task_1, task_2] &gt;&gt; task_3 task-dependencies_mixed # It is also possible to define dependencies with task_1.set_downstream(task_2) task_3.set_upstream(task_1) task-dependencies_upstream # It is also possible to mix it completely wild task_1 &gt;&gt; task_3 &lt;&lt; task_2 task_1.set_downstream(task_2) task-dependencies_wild It is possible to list all tasks within a DAG using the CLI. Below commands show two approaches. # Prints the list of tasks in the &quot;task_fundamentals&quot; DAG airflow tasks list task_fundamentals # Prints the hierarchy of tasks in the &quot;task_fundamentals&quot; DAG airflow tasks list task_fundamentals --tree In general, each task of a DAG runs on a different compute resource (also called worker). It may require an extensive use of environment variables to achieve running on the same environment or with elevated privileges. This means that tasks naturally cannot cross communicate which impedes the exchange of information and data. To achieve cross communication an additional feature of Airflow needs to be used, called XCom. 2.1.4 XCom XCom (short for “cross-communication”) is a mechanism that allows information passing between tasks in a DAG. This is beneficial as by default tasks are isolated within Airflow and may run on entirely different machines. The goal of using XComs is not to pass large values like dataframes themselves. XComs are used to store all relevant metadata that are needed to pass data from one task to another, including infomration about the sender, the recipient, and the data itself. If there is the need to pass a big data frame from task_a to task_b, the data is stored in a persistent storage solution (a bucket, or database) and the information about the storage location is stored in the XCom. This means that task_a pushes the data to a storage solution and writes the information where the data is stored (e.g. a AWS S3 URI) within an XCom to the Airflow database. Afterward, task_b can access this information and retrieve the data from the external storage using the AWS . The XCom is identified by a key-value pair stored in the Airflow Metadata Database. The key of an XCom is basically its name and consists of a tuple (dag_id, task_id, execution_date,key). key within the XComs name denotes the name of the stored data and is a configurable string (by default it’s return_value). The XComs value is a json serializable. It is important to note that it is only designed for small amounts of data, depending on the attached metadata database (e.g. 2GB for SQLite, or 1GB Postgres database). XComs are explicitly pushed and pulled to/from the metadata database using the xcom_push and xcom_pull methods. While the push_*-methods upload the XCom, the pull-method downloads information from XCom. The XCom element can be viewed within Airflows web interface which is quite helpful to debug or monitor a DAG. Below example shows this mechanics. However, when looking at the push one can see a difference in their functionality. The method push_by_returning uses the operators’ auto-push functionality that pushes their results into an default XCom key (the default is return_value). Using the auto-push functionality allows to only use python return statements and is enabled by setting the do_xcom_push argument to True, which it also is by default ( @task functions do this as well). To push an XCom with a specific key, the xcom_push-method needs to be called explicitly. In order to access the xcom_push one needs to access the task instance (ti) object. It can be accessed by passing the \"ti\" parameter to the python callable of the PythonOperator. Its usage can be seen in the push method, where also a custom key is given to the XCom. Similarly, the puller function uses the xcom_pull method to pull the previously pushed values from the metadata databes. from airflow.models import DAG from pendulum import datetime from airflow.operators.python_operator import PythonOperator xcom_fundamentals = DAG( dag_id=&#39;xcom_fundamentals&#39;, start_date=datetime(2023, 1, 1, tz=&quot;Europe/Amsterdam&quot;), schedule_interval=None ) # DAG definition etc. value_1 = [1, 2, 3] value_2 = {&#39;a&#39;: &#39;b&#39;} def push(**kwargs): &quot;&quot;&quot;Pushes an XCom without a specific target&quot;&quot;&quot; kwargs[&#39;ti&#39;].xcom_push(key=&#39;value from pusher 1&#39;, value=value_1) def push_by_returning(**kwargs): &quot;&quot;&quot;Pushes an XCom without a specific target, just by returning it&quot;&quot;&quot; # Airflow does this automatically as auto-push is turned on. return value_2 def puller(**kwargs): &quot;&quot;&quot;Pull all previously pushed XComs and check if the pushed values match the pulled values.&quot;&quot;&quot; ti = kwargs[&#39;ti&#39;] # get value_1 pulled_value_1 = ti.xcom_pull(key=None, task_ids=&#39;push&#39;) # get value_2 pulled_value_2 = ti.xcom_pull(task_ids=&#39;push_by_returning&#39;) # get both value_1 and value_2 the same time pulled_value_1, pulled_value_2 = ti.xcom_pull( key=None, task_ids=[&#39;push&#39;, &#39;push_by_returning&#39;]) print(f&quot;pulled_value_1: {pulled_value_1}&quot;) print(f&quot;pulled_value_2: {pulled_value_2}&quot;) push1 = PythonOperator( task_id=&#39;push&#39;, # provide context is for getting the TI (task instance ) parameters provide_context=True, dag=xcom_fundamentals, python_callable=push, ) push2 = PythonOperator( task_id=&#39;push_by_returning&#39;, dag=xcom_fundamentals, python_callable=push_by_returning, # do_xcom_push=False ) pull = PythonOperator( task_id=&#39;puller&#39;, # provide context is for getting the TI (task instance ) parameters provide_context=True, dag=xcom_fundamentals, python_callable=puller, ) # push1, push2 are upstream to pull [push1, push2] &gt;&gt; pull 2.1.5 Scheduling A workflow can be run either triggered manually or on a scheduled basis. Each DAG maintains a state for each workflow and the tasks within the workflow, and specifies whether it is running, failed, or a success. Airflow scheduling is designed to run as a persistent service in an production environment and can be customized. When running Airflow locally, executing the airflow scheduler via the CLI will use the configuration specified in airflow.cfg and start the service. The Airflow scheduler monitors all DAGs and tasks, and triggers those task instances whose dependencies have been met. A scheduled tasks needs several attributes specified. When looking at the first example of how to define a DAG we can see that we already defined the attributes start_date, and schedule_interval. We can also add optional attributes such as end_date, and max_tries. from airflow.models import DAG default_args = { &#39;start_date&#39;: &#39;2023-01-01&#39;, # (optional) when to stop running new DAG instances &#39;end_date&#39;: &#39;2023-01-01&#39;, # (optional) how many attempts to make &#39;max_tries&#39;: 3, &#39;schedule_interval&#39;: &#39;@daily&#39; } example_dag = DAG( dag_id=&#39;scheduling_fundamentals&#39;, default_args=default_args ) 2.1.6 Taskflow Defining a DAG and using operators as shown in the previous sections is the classic approach to define a workflow in Airflow. However, Airflow 2.0 introduced the TaskFlow API which allows to work in a more pythonic way using decorators. DAGs and tasks can be created using the @dagor @task decorators. The function name itself acts as the unique identifier for the DAG or task respectively. All of the processing in a TaskFlow DAG is similar to the traditional paradigm of Airflow, but it is all abstracted from the developers. This allows developers to focus on the code. There is also no need to specify task dependencies as they are automatically generated within TaskFlow based on the functional invocation of tasks. Defining a workflow of an ETL-pipeline using the TaskFlow paradigm is shown in belows example. The pipeline invokes an extract task, sends the ordered data to a transform task for summarization, and finally invokes a load task with the previously summarized data. Its quite easy to catch that the Taskflow workflow contrasts with Airflow’s traditional paradigm in several ways. import json import pendulum from airflow.decorators import dag, task # Specify the dag using @dag # The Python function name acts as the DAG identifier # (see also https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html) @dag( schedule=None, start_date=pendulum.datetime(2021, 1, 1, tz=&quot;UTC&quot;), catchup=False, tags=[&quot;example&quot;], ) def taskflow_api_fundamentals(): # set a task using @task @task() def extract(): data_string = &#39;{&quot;1001&quot;: 301.27, &quot;1002&quot;: 433.21, &quot;1003&quot;: 502.22}&#39; order_data_dict = json.loads(data_string) return order_data_dict @task(multiple_outputs=True) def transform(order_data_dict: dict): total_order_value = 0 for value in order_data_dict.values(): total_order_value += value return {&quot;total_order_value&quot;: total_order_value} @task() def load(total_order_value: float): print(f&quot;Total order value is: {total_order_value:.2f}&quot;) # task dependencies are automatically generated order_data = extract() order_summary = transform(order_data) load(order_summary[&quot;total_order_value&quot;]) # Finally execute the DAG taskflow_api_fundamentals() Even the passing of data between tasks which might run on different workers is all handled by TaskFlow so there is no need to use XCom. However, XCom is still used behind the scenes, but all of the XCom usage passing data between tasks is abstracted away. This allows to view the XCom in the Airflow UI as before. Belows example shows the transform function written in the traditional Airflow using XCom and highlights the simplicity of using TaskFlow. def transform(**kwargs): ti = kwargs[&quot;ti&quot;] extract_data_string = ti.xcom_pull(task_ids=&quot;extract&quot;, key=&quot;order_data&quot;) order_data = json.loads(extract_data_string) total_order_value = 0 for value in order_data.values(): total_order_value += value total_value = {&quot;total_order_value&quot;: total_order_value} total_value_json_string = json.dumps(total_value) ti.xcom_push(&quot;total_order_value&quot;, total_value_json_string) As it is clearly visible, using TaskFlow is an easy approach to workflowing in Airflow. It takes away a lot of worries when it comes to building pipelines and allows for a flexible programing experience using decorators. It allows for several more functionalities, such as reusing decorated tasks in multiple DAGs, overriding task parameters like the task_id, custom XCom backends to automatically store data in e.g. AWS S3, and using TaskGroups to group multiple tasks for a better overview in the Airflow Interface. However, even though Taskflow allows for a smooth way of developing workflows, it is beneficial to learn the traditional Airflow API to understand the fundamental concepts of how to create a workflow. The following section will build up on that knowledge and depict a full machine learning workflow as an example. "],["exemplary-ml-workflow.html", "2.2 Exemplary ML workflow", " 2.2 Exemplary ML workflow Please refer to the previous section. This involves everything # build dag that includes everything from before def to_be_done(): pass "],["airflow-infrastructure.html", "2.3 Airflow infrastructure", " 2.3 Airflow infrastructure Before Data Scientists and Machine Learning Engineers can utilize the power of Airflow Workflows, Airflow obviously needs to be set up and deployed. There are multiple ways an Airflow deployment can take place. It can be run either on a single machine or in a distributed setup on a cluster of machines. As stated in the prerequisites, a local Airflow setup is on a single machine can be used for this tutorial to give an introduction on how to work with airflow. Although Airflow can be run on a single machine, it should be deployed as a distributed system to utilize its full power when working with Airflow in production. 2.3.1 Airflow as a distributed system Airflow consists of several separate architectural components. While this separation is somewhat simulated on a local deployment, each unit of Airflow can be set up separately when deploying Airflow in a distributed manner. A distributed architecture comes with benefits of availability, security, reliability, and scalability. An Airflow deployment generally consists of five different compontens: Scheduler: The schedules handles triggering scheduled workflows and submitting tasks to the executor to run. Executor: The executor handles running the tasks themselves. In a local installation of Airflow, the tasks are run by the executor itself. In a production ready deployment of Airflow the executor pushes the task execution to separate worker instance which then runs the task. Webserver: The webserver provides the web user interface of Airflow that allows to inspect, trigger, and debug DAGs and tasks. DAG Directory: The DAG directory is a directory that contains the DAG files which are read by the scheduler and executor. Metadata Database: The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, user settings, or XCom. The following graphs shows how the components build up the Airflow architecture. Architecture of Airflow as a distributed system 2.3.2 Scheduler The scheduler is basically the brain and heart of Airflow. It handles triggering and scheduling of workflows, as well as submitting tasks to the executor to run. To be able to do this, the scheduler is responsible to parse the DAG files from the DAG directory, manage the database states in the metadata database, and to communicate with the executor to schedule tasks. Since the release of Airflow 2.0 it is possible to run multiple schedulers at a time to ensure a high availability and reliability of this centerpiece of Airflow. 2.3.3 Webserver The webserver runs the web interface of Airflow and thus the user interface every Airflow user sees. This allows to inspect, trigger, and debug DAGs and tasks in Airflow (and much more!), such as seen in the previous chapters. Each user interaction and change is written to the DAG directory or the metadata database, from where the scheduler will read and act upon. 2.3.4 Executor The executor defines where and how the Airflow tasks should be executed and run. This crucial component of Airflow can be configured by the user and should be chosen to fit the users specific needs. There are several different executors, each handling the run of a task a bit differently. Choosing the right executor also relies on the underlying infrastructure Airflow is build upon. In a production ready deployment of Airflow the executor pushes the task execution to separate worker instances that run the tasks. This allows for different setups such as a Celery-like executors or an executor based on Kubernetes. The benefit of a distributed deployment is its reliability and availability, as it is possible to have many workers in different places (for example using separate virtual machines, or multiple kubernetes pods). It is further possibility to run tasks on different instances based on their needs, for example to run the training step of a machine learning model on a GPU node. The SequentialExecutor is the default executor in Airflow. This executor only runs one task instance at a time and should therefore not used in a production use case. The LocalExecutorexecutes each task in a separate process on a single machine. It’s the only non-distributed executor which is production ready and works well in relatively small deployments. If Airflow is installed locally as mentioned in the prerequisites, this executor will be used. In contrast, the CeleryExecutor uses under the hood the Celery queue system that allows users to deploy multiple workers that read tasks from the broker queue (Redis or RabbitMQ) where tasks are sent by scheduler. This enables Airflow to distribute tasks between many machines and allows users to specify what task should be executed and where. This can be useful for routing compute-heavy tasks to more resourceful workers and is the most popular production executor. The KubernetesExecutor is another widely used production-ready executor and works similarly to the CeleryExecutor. As the name already suggests it requires an underlying Kubernetes cluster that enables Airflow to spawn a new pod to run each task. Even though this is a robust method to account for machine or pod failure, the additional overhead in creating pods or even nodes can be problematic for short running tasks. The CeleryKubernetsExecutor uses both, the CeleryExecutor and KubernetesExecutor (as the name already says). It allows to distinguish whether a particular task should be executed on kubernetes or routed to the celery workers. This way users can take full advantage of horizontal auto scaling of worker pods, and to delegate computational heavy tasks to kubernetes. The additional DebugExecutor is an executor whose main purpose is to debug DAGs locally. It’s the only executor that uses a single process to execute all tasks. Examining which executor is currently set can be done by running the following command. airflow config get-value core executor 2.3.5 DAG Directory The DAG directory contains the DAG files written in python. The DAG directory is read and used by each Airflow component for a different purpose. The web interface lists all written DAGs from the directory as well as their content. The scheduler and executor run a DAG or a task based on the input read from the DAG directory. The DAG directory can be of different nature. It can be a local folder in case of a local installation, or also use a separate repository like Git where the DAG files are stored. The scheduler will recurse through the DAG Directory so it is also possible to create subfolders for example based on different projects. 2.3.6 Metadata Database The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, user settings, or XCom. It is beneficial to run the metadata database as a separate component to keep all data safe and secure in case there are erroneous other parts of the infrastructure. Such considerations account for the architectural decisions of an Airflow deployment and the metadata database itself. For example, it is possible to run Airflow and all of its components on a Kubernetes cluster. However, this is not necessarily recommended as it is prone to the cluster and its accessibility itself. It is also possible to outsource Airflow components such as the metadata database to use a clouds’ distinguished database resources for example. For example to store all metadata, in the Relational Database Service (RDS) on the AWS Cloud. "],["mlflow.html", "Chapter 3 MLflow", " Chapter 3 MLflow MLflow is an open source platform to manage the machine learning lifecycle end-to-end. This includes the experimentation phase of ML models, their development to be reproducible, their deployment, and the registration of a ML model to be served. MLflow provides four primary components to manage the ML lifecycle. They can be either used on their own or they also to work together. MLflow Tracking is used to log and compare model parameters, code versions, metrics, and artifacts of an ML code. Results can be stored to local files or to remote servers, and can be compared over multiple runs. MLflow Tracking comes with an API and a web interface to easily observe all logged parameters and artifacts. MLflow Models enables to manage and deploy machine learning models from multiple libraries. It allows to package your own ML model for later use in downstream tasks, e.g. real-time serving through a REST API. The package format defines a convention that saves the model in different “flavors” that can be interpreted by different downstream tools. MLflow Registry provides a central model store to collaboratively manage the full lifecycle of a MLflow Model, including model versioning, stage transitions, and annotations. It comes with an API and user interface for easy use of such funtionalities and each of those aspects can be checked in MLflows’ web interface. MLflow Projects packages data science code in a standard format for a reusable, and reproducible form to share your code with other data scientists or transfer it to production. A project might be a local directory or a Git repository which uses a descriptor file to specify its dependencies and entrypoints. An existing MLflow Project can be also run either locally or from a Git repository. MLflow is library-agnostic, which means one can use it with any ML library and programming language. All functions are accessible through a REST API and CLI, but quite conveniently the project comes with a Python API, R API, and a Java API already included. It is even possible to define your own plugins. The aim is to make its use as reproducible and reusable as possible so Data Scientists require minimal changes to integrate MLflow into their existing codebase. MLflow also comes with a user web interface to conveniently view and compare models and metrics. Web Interface of MLflow Prerequisites To go through this chapter it is necessary to have python and MLflow installed. One can install MLflow locally via pip install MLflow. This tutorial is based on MLflow v2.1.1. It is also recommended to have knowledge of VirtualEnv, Conda, or Docker when working with MLflow Projects. "],["core-components-1.html", "3.1 Core Components", " 3.1 Core Components The four primary components of MLflow are shown in more detail and with exemplary code in the following sections. 3.1.1 MLflow Tracking MLflow Tracking allows to log and compare parameters, code versions, metrics, and artifacts of a machine learning model. This can be easily done by minimal changes to your code using the MLflow Tracking API. The following examples depict the basic concepts and show how to use it. To use MLflow within your code it needs to be imported first. import mlflow 3.1.1.1 MLflow experiment MLflow experiments are a part of MLflow’s tracking component that allow to group runs together based on custom criteria. For example multiple model runs with different model architectures might be grouped within one experiment to make it easier for evaluation. experiment_name = &quot;introduction-experiment&quot; mlflow.set_experiment(experiment_name) 3.1.1.2 MLflow run An MLflow run is an execution environment for a piece of machine learning code. Whenever parameters or performances of a ML run or experiment should be tracked, a new MLflow run is created. This is easily done using MLflow.start_run(). Using MLflow.end_run() the run will similarly be ended. run_name = &quot;example-run&quot; mlflow.start_run() run = mlflow.active_run() print(f&quot;Active run_id: {run.info.run_id}&quot;) mlflow.end_run() It is a good practice to pass a run name to the MLflow run to identify it easily afterwards. It is also possible to use the context manager as shown below, which allows for a smoother style. run_name = &quot;context-manager-run&quot; with mlflow.start_run(run_name=run_name) as run: run_id = run.info.run_id print(f&quot;Active run_id: {run_id}&quot;) Child runs It is possible to create child runs of the current run, based on the run ID. This can be used for example to gain a better overview of multiple run. Belows code shows how to create a child run. # Create child runs based on the run ID with mlflow.start_run(run_id=run_id) as parent_run: print(&quot;parent run_id: {}&quot;.format(parent_run.info.run_id)) with mlflow.start_run(nested=True, run_name=&quot;test_dataset_abc.csv&quot;) as child_run: mlflow.log_metric(&quot;acc&quot;, 0.91) print(&quot;child run_id : {}&quot;.format(child_run.info.run_id)) with mlflow.start_run(run_id=run_id) as parent_run: print(&quot;parent run_id: {}&quot;.format(parent_run.info.run_id)) with mlflow.start_run(nested=True, run_name=&quot;test_dataset_xyz.csv&quot;) as child_run: mlflow.log_metric(&quot;acc&quot;, 0.90) print(&quot;child run_id : {}&quot;.format(child_run.info.run_id)) 3.1.1.3 Logging metrics &amp; parameters The main reason to use MLflow Tracking is to log and store parameters and metrics during a MLflow run. Parameters represent the input parameters used for training, e.g. the initial learning rate. Metrics are used to track the progress of the model training and are usually updated over the course of a model run. MLflow allows to keep track of the model’s train and validation losses and to visualize their development across the training run. Parameters and metrics can be easily logged by calling MLflow.log_param and MLflow.log_metric. One can also specify a tag to identify the run by using MLflow.set_tag. Belows example show how to use each method within a run. run_name = &quot;tracking-example-run&quot; experiment_name = &quot;tracking-experiment&quot; mlflow.set_experiment(experiment_name) with mlflow.start_run(run_name=run_name) as run: # Parameters mlflow.log_param(&quot;learning_rate&quot;, 0.01) mlflow.log_params({&quot;epochs&quot;: 0.05, &quot;final_activation&quot;: &quot;sigmoid&quot;}) # Tags mlflow.set_tag(&quot;env&quot;, &quot;dev&quot;) mlflow.set_tags({&quot;some_tag&quot;: False, &quot;project&quot;: &quot;xyz&quot;}) # Metrics mlflow.log_metric(&quot;loss&quot;, 0.001) mlflow.log_metrics({&quot;acc&quot;: 0.92, &quot;auc&quot;: 0.90}) # It is possible to log a metrics series (for example a training history) for val_loss in [0.1, 0.01, 0.001, 0.00001]: mlflow.log_metric(&quot;val_loss&quot;, val_loss) for val_acc in [0.6, 0.6, 0.8, 0.9]: mlflow.log_metric(&quot;val_acc&quot;, val_acc) run_id = run.info.run_id experiment_id = run.info.experiment_id print(f&quot;run_id: {run_id}&quot;) print(f&quot;experiment_id: {experiment_id}&quot;) It is also possible to add information after the experiment ran. One just needs to specify the run ID from the previous run to the current run. The example below shows how to do this, and uses the mlflow.client.MLflowClient. The mlflow.client module provides a Python CRUD interface, which is a lower level API directly translating to the MLflow REST API calls. It can be used similarly to the mlflow-module of the higher level API. It is mentioned here to give a hint of its existence. from mlflow.tracking import MLflowClient # add a note to the experiment MLflowClient().set_experiment_tag( experiment_id, &quot;MLflow.note.content&quot;, &quot;my experiment note&quot;) # add a note to the run MLflowClient().set_tag(run_id, &quot;MLflow.note.content&quot;, &quot;my run note&quot;) # Or we can even log further metrics by calling MLflow.start_run on a specific ID with mlflow.start_run(run_id=run_id): run = mlflow.active_run() mlflow.log_metric(&quot;f1&quot;, 0.9) print(f&quot;run_id: {run.info.run_id}&quot;) 3.1.1.4 Display &amp; View metrics How can the logged parameters and metrics be used and viewed afterwards? It is possible to give an overview of the currently stored runs using the MLflow API and printing the results. current_experiment = dict(mlflow.get_experiment_by_name(experiment_name)) mlflow_run = mlflow.search_runs([current_experiment[&#39;experiment_id&#39;]]) print(f&quot;MLflow_run: {mlflow_run}&quot;) MLflow Model Tracking CLI Run Overview Yet, viewing all the results in the web interface of MLflow gives a much better overview. By default, the tracking API writes the data to the local filesystem of the machine it’s running on under a ./mlruns directory. This directory can be accessed by the MLflow’s Tracking web interface by running MLflow ui via the command line. The web interface can be viewed in the browser under http://localhost:5000 (The port: 5000 is the MLflow default). The metrics dashboard of a run looks like the following: MLflow Model Tracking Dashboard It is also possible to configure MLflow to log to a remote tracking server. This allows to manage results on in a central place and share them across a team. To get access to a remote tracking server it is needed to set a MLflow tracking URI. This can be done multiple way. Either by setting an environment variable MLflow_TRACKING_URI to the servers URI, or by adding it to the start of our code. import mlflow mlflow.set_tracking_uri(&quot;http://YOUR-SERVER:YOUR-PORT&quot;) mlflow.set_experiment(&quot;my-experiment&quot;) 3.1.1.5 Logging artifacts Artifacts represent any kind of file to save during training, such as plots and model weights. It is possible to log such files as well, and place them within the same run as parameters and metrics. This means everything created within a ML run is saved at one point. Artifact files can be either single local files, or even full directories. The following example creates a local file and logs it to a model run. import os mlflow.set_tracking_uri(&quot;http://127.0.0.1:5000/&quot;) # Create an example file output/test.txt file_path = &quot;outputs/test.txt&quot; if not os.path.exists(&quot;outputs&quot;): os.makedirs(&quot;outputs&quot;) with open(file_path, &quot;w&quot;) as f: f.write(&quot;hello world!&quot;) # Start the run based on the run ID and log the artifact # we just created with mlflow.start_run(run_id=run_id) as run: mlflow.log_artifact( local_path=file_path, # store the artifact directly in run&#39;s root artifact_path=None ) mlflow.log_artifact( local_path=file_path, # store the artifact in a specific directory artifact_path=&quot;data/subfolder&quot; ) # get and print the URI where the artifacts have been logged to artifact_uri = mlflow.get_artifact_uri() print(f&quot;run_id: {run.info.run_id}&quot;) print(f&quot;Artifact uri: {artifact_uri}&quot;) 3.1.1.6 Autolog Previously, all the parameters, metrics, and files have been logged manually by the user. The autolog-feature of MLflow allows for automatic logging of metrics, parameters, and models without the need for an explicit log statements. This feature needs to be activated previous to the execution of a run by calling MLflow.sklearn.autolog(). import mlflow.sklearn import numpy as np from sklearn.ensemble import RandomForestRegressor params = {&quot;n_estimators&quot;: 4, &quot;random_state&quot;: 42} mlflow.sklearn.autolog() run_name = &#39;autologging model example&#39; with mlflow.start_run(run_name=run_name) as run: rfr = RandomForestRegressor( **params).fit(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]), [1, 1, 1]) print(f&quot;run_id: {run.info.run_id}&quot;) mlflow.sklearn.autolog(disable=True) Even though this is a very convenient feature, it is a good practice to log metrics manually, as this gives more control over a ML run. 3.1.2 MLflow Models MLflow Models manages and deploys models from various different ML libraries such as scikit-learn, TensorFlow, PyTorch, Spark, and many more. It includes a generic MLmodel format that acts as a standard format to package ML models so they can be used in different projects and environments. The MLmodel format defines a convention that saves the model in so called “flavors”. For example mlflow.sklearn allows to load mlflow models back into scikit-learn. The stored model can also be served easily and conveniently using these flavors as a python function either locally, in Docker-based REST servers containers, or on commercial serving platforms like AWS SageMaker or AzureML. The following example is based on the scikit-learn library. # Import the sklearn models from MLflow import mlflow.sklearn from sklearn.ensemble import RandomForestRegressor mlflow.set_tracking_uri(&quot;http://127.0.0.1:5000/&quot;) run_name = &quot;models-example-run&quot; params = {&quot;n_estimators&quot;: 4, &quot;random_state&quot;: 42} # Start an MLflow run, train the RandomForestRegressor example model, and # log its parameeters. In the end the model itself is logged and stored in MLflow run_name = &#39;Model example&#39; with mlflow.start_run(run_name=run_name) as run: rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1]) mlflow.log_params(params) mlflow.sklearn.log_model(rfr, artifact_path=&quot;sklearn-model&quot;) model_uri = &quot;runs:/{}/sklearn-model&quot;.format(run.info.run_id) model_name = f&quot;RandomForestRegressionModel&quot; print(f&quot;model_uri: {model_uri}&quot;) print(f&quot;model_name: {model_name}&quot;) Once a model is stored in the correct format it can be identified by its model_uri, loaded, and used for prediction. import mlflow.pyfunc # Load the model and use it for predictions model = mlflow.pyfunc.load_model(model_uri=model_uri) data = [[0, 1, 0]] model_pred = model.predict(data) print(f&quot;model_pred: {model_pred}&quot;) MLflow Models 3.1.3 MLflow Model Registry The MLflow Model Registry provides a central model store to manage the lifecycle of an ML Model. This allows to register MLflow models like the RandomForestRegressor from the previous section to the Model Registry and include model versioning, stage transitions, and annotations. In fact, by running MLflow.sklearn.log_model we already did exactly that. Look at how easy the MLflow API is to use. Let’s have a look at the code again. import mlflow.sklearn import mlflow.pyfunc from sklearn.ensemble import RandomForestRegressor mlflow.set_tracking_uri(&quot;http://127.0.0.1:5000/&quot;) run_name = &quot;registry-example-run&quot; params = {&quot;n_estimators&quot;: 4, &quot;random_state&quot;: 42} run_name = &#39;model registry example&#39; with mlflow.start_run(run_name=run_name) as run: rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1]) mlflow.log_params(params) # Log and store the model and the MLflow Model Registry mlflow.sklearn.log_model(rfr, artifact_path=&quot;sklearn-model&quot;) model_uri = f&quot;runs:/{run.info.run_id}/sklearn-model&quot; model_name = f&quot;RandomForestRegressionModel&quot; model = mlflow.pyfunc.load_model(model_uri=model_uri) data = [[0, 1, 0]] model_pred = model.predict(data) print(f&quot;model_pred: {model_pred}&quot;) Yet, it is also possible to register the MLflow model in the model registry by calling MLflow.register_model such as show in belows example. # The previously stated Model URI and name are needed to register a MLflow Model mv = mlflow.register_model(model_uri, model_name) print(&quot;Name: {}&quot;.format(mv.name)) print(&quot;Version: {}&quot;.format(mv.version)) print(&quot;Stage: {}&quot;.format(mv.current_stage)) Once registered to the model registry the model is versioned. This enables to load a model based on a specific version and to change a model version respectively. A registered model can be also modified to transition to another version or stage. Both use cases are shown in the example below. import mlflow.pyfunc # Load model for prediction. Keep note that we now specified the model version. model = mlflow.pyfunc.load_model( model_uri=f&quot;models:/{model_name}/{mv.version}&quot; ) # Predict based on the loaded model data = [[0, 1, 0]] model_pred = model.predict(data) print(f&quot;model_pred: {model_pred}&quot;) Let’s stage a model to 'Staging'. The for-loop below prints all registered models and shows that there is indeed a model with a 'Staging'-stage. # Transition the model to another stage from mlflow.client import MLflowClient client = MlflowClient() stage = &#39;Staging&#39; # None, Production client.transition_model_version_stage( name=model_name, version=mv.version, stage=stage ) # print registered models for rm in client.search_registered_models(): pprint(dict(rm), indent=4) 3.1.4 MLflow Projects MLflow Projects allows to package code and its dependencies as a project that can be run reproducible on other data. Each project includes a MLproject file written in the YAML syntax that defines the projects dependencies, and the commands and arguments it takes to run the project. It basically is a convention to organizes and describe the model code so other data scientists or automated tools can run it conveniently. MLflow currently supports four environments to run your code: Virtualenv, Conda, Docker Container, and system environment. A very basic MLproject file is shown below that is run in an Virtualenv name: mlprojects_tutorial # Use Virtualenv: alternatively conda_env, docker_env.image python_env: &lt;MLFLOW_PROJECT_DIRECTORY&gt;/python_env.yaml entry_points: main: parameters: alpha: {type: float, default: 0.5} l1_ratio: {type: float, default: 0.1} command: &quot;python wine_model.py {alpha} {l1_ratio}&quot; A project is run using the MLflow run command in the command line. It can run a project from either a local directory or a GitHub URI. The MLproject file shows that two parameters are passed to the MLflow run command. This is optional in this case as they have default values. It is also possible to specify extra parameters such as the experiment name or to specify the tracking uri (check the official documentation for more). Below is a possible CLI command show to run the MLflow Project. By setting the MLFLOW_TRACKING_URI environment variable it is possible to also specify an execution backend for the run. # Run the MLflow project from the current directory # The parameters are optional in this case as the MLproject file has defaults mlflow run . -P alpha=5.0 # It is also possible to specify an experiment name or to specify the # Tracking_URI, e.g. MLFLOW_TRACKING_URI=http://localhost:&lt;PORT&gt; mlflow run . --experiment-name=&quot;models-experiment&quot; # Run the MLflow project from a Github URI and use the localhost as backend MLFLOW_TRACKING_URI=http://localhost:&lt;PORT&gt; MLflow run https://github.com/seblum/mlops-practice#files/06-MLFlow/mlprojects --version=chapter/mlflow The MLflow Projects API allows to chain projects together into workflows and also supports launching multiple runs in parallel. Combining this with for example the MLflow Tracking API enables an easy way of hyperparameter tuning to develop a model with a good fit. "],["mlfflow-architecture.html", "3.2 MLFflow Architecture", " 3.2 MLFflow Architecture While MLflow can be run locally for your personal model implementation, it is usually deployed on a distributed architecture for large organizations or teams. The MLflow backend consists of three different main components, Tracking Server, Backend Store, and Artifact Store, all of which can reside on remote hosts. The MLflow client can interface with a variety of backend and artifact storage configurations. The official MLflow documentation outlines several detailed configurations. The example below depicts the main interaction between the different architectural components of a remote MLflow Tracking Server, a Postgres database for backend storage, and an S3 bucket for artifact storage. MLflow Architecture Diagram 3.2.1 MLflow Tracking Server The MLflow Tracking Server is the main component that handles the communication between the REST API to log parameters, metrics, experiments and metadata to a storage solution. The server uses both, the backend store and the artifact store to store and read data from. Although it is possible to track parameters without running a server (e.g. locally), it is recommended to create a MLflow tracking server to log your data to. Some of the functionality of the API is also available via the web interface, for example to create an experiment. Further, the Tracking web user interface allows to view runs easily in the web browser. Running the CLI command mlflow ui starts a web server on your local machine serving the MLFlow UI. Alternatively, a remote MLflow tracking server serves the same UI which can be accessed using the server’s URL http://&lt;TRACKING-SERVER-IP-ADDRESS&gt;:5000 from any machine that can connect to the tracking server. 3.2.2 MLflow Backend Store The MLflow Backend Store is where MLflow stores experiment and run data like parameters, and metrics. It is usually a relational database which means that all metadata will be stored, but no large data files. MLflow supports two types of backend stores: file store and database-backed store. By default, the backend store is set to the local file store backend at the ./mlruns directory. A database-backed store must be configures using the --backend-store-uri. MLflow supports encoded Databases like mysql, mssql, sqlite, and postgresql, and it is possible to use a variety of externally hosted metadata stores like Azure MySQL, or AWS RDS. To be able to use the MLflow Model Registry the server must use a database-backed store. 3.2.3 MLflow Artifact Store In addition to the Backend Store the Artifact Store is another storage place for the MLflow tracking server. It is the location to store large data of an ML run that are not suitable for the Backend Store or a relational database respectively. This is where MLflow users log their artifact outputs, or data and image files to. The user can access these artifacts via HTTP requests to the MLflow Tracking Server. The location to the server’s artifact store defaults to local ./mlruns directory. It is possible to specify another artifact store server using --default-artifact-root. The MLflow client caches the artifact location information on a per-run basis. It is therefore not recommended to alter a run’s artifact location before it has terminated. The Artifact Store needs to be configured when running MLflow on a distributed system. In addition to local file paths, MLflow supports to configure the following cloud storage resources as an artifact stores: Amazon S3, Azure Blob Storage, Google Cloud Storage, SFTP server, and NFS. "],["kubernetes.html", "Chapter 4 Kubernetes", " Chapter 4 Kubernetes Kubernetes (short: K8s) is greek and means pilot. K8s is an applications orchestrator that originated from Google and is open source. Beeing an application orchestrator, K8s deploys and manages application containers. It scales up and down so called Pod (A Pod manages a container) as needed and allows for zero downtime as well as the possibility of rollbacks. Thus, the meaning of pilot relates to its functionining in piloting containers. Prerequisites As a prerequisites to go through this tutorial and implement the scripts one has to have Docker installed, Kubectl to interact via the command line with K8s, as well as Minikube to run a K8s cluster on a local machine. Please refer to the corresponding sites. "],["core-components-2.html", "4.1 Core Components", " 4.1 Core Components 4.1.1 Nodes A K8s Cluster usually consistes of a set of nodes. A Node can hereby be a virtual machine (VM) in the cloud, e.g. AWS, Azure, or GCP, or a node can also be of course a physical on-premise instance. K8s distinguishes the nodes between a master node and worker nodes. The master node is basically the brain of the cluster. This is where everything is organized, handled, and managed. In comparison, a worker nodes is where the heavy lifting is happening, such as running application. Both, master and worker nodes communicate with each other via the so called kubelet. One cluster has only one master node and usually multiple worker nodes. K8s Cluster 4.1.1.1 Master &amp; Control Plane To be able to work as the brain of the cluster, the master node contains a controll plane made of several components, each of which serves a different function. Scheduler Cluster Store API Server Controller Manager Cloud Controller Manager Master Node 4.1.1.1.1 API Server The api servers serves as the connection between the frontend and the K8s controll plane. All communications, external and interal, go through it. Frontend to Kubernetes Controll Plane. It exposes a restful api on port 443 to allow communication, as well as performes authentication and authorization checks. Whenever we perform something on the K8s cluster, e.g. using a command like kubectl apply -f &lt;file.yaml&gt;, we communicate with the api server (what we do here is shown in the section about pods). 4.1.1.1.2 Cluster store The cluster store stores the configuration and state of the entire cluster. It is a distributed key-value data store and the single source of truth database of the cluster. As in the example before, whenever we apply a command like kubectl apply -f &lt;file.yaml&gt;, the file is stored on the cluster store to store the configuration. 4.1.1.1.3 Scheduler The scheduler of the control plane watches for new workloads/pods and assigns them to a node based on several scheduling factors. These factors include whether a node is healthy, whether there are enough resources available, whether the port is available, or according to affinity or anti-affinity rules. 4.1.1.1.4 Controller manager The controller manager is a daemon that manages the control loop. This means, the controller manager is basically a controller of other controllers. Each controller watches the api server for changes to their state. Whenever a current state of a controller does not match the desired state, the control manager administers the changes. These controllers are for example replicasets, endpoints, namespace, or service accounts. There is also the cloud controller manager, which is responsible to interact with the underlying cloud infrastructure. 4.1.1.2 Worker Nodes There worker nodes are the part of the cluster where the heavy lifting happens. Their VMs (or physical machines) often run linux and thus provide a suitable and running environment for each application. Worker Node A worker node consists of three main components. Kubelet Container runtime Kube proxy 4.1.1.2.1 Kubelet The kubelet is the main agent of a worker node that runs on every single node. It receives pod definitions from the API server and interacts with the container runtime to run containers associated with the corresponding pods. The kubelet also reports node and pod state to the master node. 4.1.1.2.2 Container runtime The container runtime is responsible to pull images from container registries, e.g. from DockerHub, or AWS ECR, as well as starting, and stoping containers. The container runtime thus abstracts container management for K8s and runs a Container Runtime Interface (CRI) within. 4.1.1.2.3 Kube-proxy The kube-proxy runs on every node via a DaemondSet. It is responsible for network communications by maintaining network rules to allow communication to pods from inside and outside the cluster. If two pods want to talk to each other, the kube-proxy handles their communication. Each node of the cluster gets its own unique IP adress. The kube-proxy handels the local cluster networking as well as routing the network traffic to a load balanced service. 4.1.2 Pods A pod is the smallest deployable unit in K8s (In contrast to K8s, the smallest deployable unit for docker are containers.). Therefore, a pod is a running process that runs on a clusters’ node. Within a pod, there is always one main container representing the application (in whatever language written, e.g. JS, Python, Go). There also may or may not be init containers, and/or side containers. Init containers are containers that are executed before the main container. Side containers are containers that support the main containers, e.g. a container that acts as a proxy to your main container. There may be volumes specified within a pod, which enables containers to share and store data. Pod The containers running within a pod communicate with each other using localhost and whatever port they expose. The port itself has a unique ip adress, which enables outward communication between pods. The problem is that a pods does not have a long lifetime (also denoted as ephemeral) and is disposable. This suggests to never create a pod on its own within a K8s cluster and to rather use controllers instead to deploy and maintain a pods lifecycle, e.g. controllers like Deployments. In general, managing ressources in K8s is done via an imperative or declarative management. 4.1.3 Imperative &amp; Declarative Management Imperative management means managing the pods via a CLI and specifying all necessary parameters using it. It is good for learning, troubleshooting, and experimenting on the cluster. In contrast, the declarative approach uses a yaml file to state all necessary parameters needed for a ressource, and then using the CLI to administer the changes. The declarative approach is reproducible, which means the same configuration can be applied in different environments (prod/dev). This is best practice to use when building a cluster. As stated, this differentiation does not only hold for pods, but for all ressources within a cluster. 4.1.3.1 Imperative Management # start a pod by specifying the pods name, # the container image to run, and the port exposed kubectl run &lt;pod-name&gt; --image=&quot;&lt;image-name&gt;&quot; --port=80 # run following command to test the pod specified # It will forward to localhost:8080 kubectl port-forward pod/&lt;pod-name&gt; 8080:80 4.1.3.2 Declarative Management / Configuration Declarative configuration is done using a yaml format, which works on key-value pairs. # pod.yaml apiVersion: v1 # specify which kind of configuration this is # lets configure a simple pod kind: Pod # metadata will be explained later on in more detail metadata: name: hello-world labels: name: hello-world spec: # remember: a pod is a selection of one or more containers # we could therefore specify multiple containers containers: # specify the container name - name: hello # specify which container image should be pulled image: seblum/mlops-public:cat-v1 # ressource configurations will be handled later as well ressources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # specify the port on which the container should be exposed # similar to the imperative approach ports: ContainerPorts: 80 Appyl this declarative configuration using the following kubectl command via the CLI. kubectl apply -f &quot;file-name.yaml&quot; # similar to before, run following to test your pod on localhost:8080 kubectl port-forward pod/&lt;pod-name&gt; 8080:80 4.1.3.3 Kubectl One word to interacting with the cluster using the CLI. In general, kubectl is used to interact with the K8s cluster. This allows to run and apply pod configurations such as seen before, as well as the already shown port forwarding. We can also inspect the cluster, see what ressources are running on which nodes, see their configurations, and watch their logs. A small selection of commands are shown below. # forward the pods to localhost:8080 kubectl port-forward &lt;ressource&gt;/&lt;pod-name&gt; 8080:80 # show all pods currently running in the cluster kubectl get pods # delete a specific pod kubectl delete pods &lt;pod-name&gt; # delete a previously applied configuration kubectl delete -f &lt;file-name&gt; # show all instances of a specific resource running on the cluster # (nodes, pods, deployments, statefulsets, etc) kubectl get &lt;resource&gt; # describe and show specific settings of a pods kubectl describe pod &lt;pod-name&gt; "],["application-deployment-design.html", "4.2 Application Deployment &amp; Design", " 4.2 Application Deployment &amp; Design 4.2.1 Deployments We should never deploy a pod using kind:Pod. Pods are ephemeral, so never treat them like pets. They do not heal on their own and if a pod is terminated, it does not restart by themselves. This is dangerous as there should always be one replica running of and application. This demands for a mechanism for the application to self heal and this is exactly where Deployments and ReplicaSets come in to solve the problem. In general, Pods should be managed through Deployments. The purpose of a Deployment is to facilitate software deployment. They manage releases of a new application, they provide zero downtime of an application and create a ReplicaSet behind the scenes. K8s will take care of the full deployment process when applying a Deployment, even if we want to make a rolling update to change the version. 4.2.1.1 ReplicaSets A ReplicaSet makes sure that a desired number of pods is running. When looking at Pods’ name of a Deployment, it usually has a random string attached. This is because a deployment can have multiple replicas and the random suffix ensures a different name after all. The way ReplicaSets work is that they implement a background control loop that checks the desired number of pods are always present on the cluster. We can specify the number of replicas by creating a yaml-file of a Deployment, similar to previous specifications done to a Pod. As a reminder, the Deployment can be applied using the kubectl apply -f as well. # deployment.yaml apiVersion: apps/v1 # specify that we want a deployment kind: Deployment metadata: name: hello-world spec: # specify number of replicas replicas: 3 selector: matchLabels: app: hello-world template: metadata: labels: app: hello-world spec: containers: - name: hello-world image: seblum/mlops-public:cat-v1 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 4.2.1.2 Rolling updates A rolling update means that a new version of the application is rolled out. In general, a basic deployments strategy will delete every single pod before it creates a new version. This is very dangerous since there is downtime. The preferred strategy is to perform a rolling update. This ensures keeping traffic to the previous version until the new one is up and running and alternates traffic until the new version is fully healthy. K8s perfoms the update of an application while the application is up and running. For example, when there are two replicasets running, one with version v1 and one with v2, K8s performs the update such that it only scales v1 down when v2 is already up and running and the traffic has been redirected to v2 as well. How do the deployments need to be configured for that? # deployment_rolling-update.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: replicas: 3 # a few new things have been added here revisionHistoryLimit: 20 # specify the deployments strategy strategy: type: RollingUpdate rollingUpdate: # only one pod at a time to become unavailable # in our case scaling down of v1 maxUnavailable: 1 # never have more than one pod above the mentioned replicas # with three replicas, there will never be 5 pods running during a rollout maxSurge: 1 selector: matchLabels: app: hello-world template: metadata: labels: app: hello-world annotations: # just an annotation the get the version change kubernetes.io/change-cause: &quot;seblum/mlops-public:cat-v2&quot; spec: containers: - name: hello-world # only change specification of the image to v2, k8s performs the update itself image: seblum/mlops-public:cat-v2 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 The changes can be applied as well using kubectl apply -f \"file-name.yaml\". Good to know, K8s is not deleting the replicasets of previous versions. They are still stored on the Cluster Store. The spec.revisionHistory: &lt;?&gt; state in the yaml denoted this. The last ten previous versions are stored on default. However, it doesn’t really make sense to keep more such for example in the previous yaml where there are the last 20 versions specified. This enables to perform Rollbacks to previous versions. To not have discrepancies in a cluster, one should always update using the declarative approach. Below stated are a number of commands that trigger and help with a rollback or with rollouts in general. # check the status of the current deployment process kubectl rollout status deployments &lt;name&gt; # pause the rollout of the deployment. kubectl rollout pause # check the rollout history of a specific deployment kubectl rollout history deployment &lt;name&gt; # undo the rollout of a deployment and switch to previous version kubectl rollout undo deployment &lt;name&gt; # goes back to a specific revision # there is a limit of history and k8s only keeps 10 previous versions kubectl rollout undo deployment &lt;name&gt; --to-revision= 4.2.2 Resource Management Besides the importance of a healthy application itself, there should be also enough resources allocated so the application can perform well, e.g. memory &amp; CPU. Yet, it should also only consume the resources needed and not block unneeded ones. It might be dangerous, as one application using a lot of ressources, leaving nothing left for other applications and eventually starving them. To prevent this from happening in K8s. there can be a minimum amount of resources defined a container needs (request) as well as the maximum amount of resources a container can have (limit). Configuring limits and requests for a container can be done within the spec for a Pod or Deployment. Actually, we have been using them all the time previously. # resource-management.yaml apiVersion: apps/v1 # specify that we want a deployment kind: Deployment metadata: name: rm-deployment spec: # specify number of replicas replicas: 3 selector: matchLabels: app: rm-deployment template: metadata: labels: app: rm-deployment spec: containers: - name: rm-deployment image: seblum/mlops-public:cat-v1 requests: memory: &quot;512Mi&quot; cpu: &quot;1000m&quot; # Limits limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 4.2.3 DaemonSets The master node of K8s decides on what worker nodes a pod is scheduled or not. However, there are times where we want to have a copy of a pod across the cluster. A DaemonSet ensures a copy of the specified Pod is exactly doing this. This can be useful for example to deploy system daemons such as log collectors and monitoring agents. DaemonSets are automatically deployed on every single node, unless specified on which node to run. They therefore do not need a specification of nodes and can scale up and down with the cluster as needed. They will automatically scheduled a pod on each new node. The given example deploys a DaemonSet to cover logging using K8s FluendID. # daemonsets.yaml apiVersion: v1 kind: Namespace metadata: name: logging --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: logging labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can&#39;t run pods - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule # specify the containers as done in Pods or Deployments volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers containers: - name: fluentd-elasticsearch # allows to collect logs from nodes image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 4.2.4 StatefulSets StatefulSets are used to deploy and manage stateful applications. Stateful applications are applications which are long lived, for example databases. Most applications of K8s are stateless as they only run for a specific task. However, a database is a state of truth and should be present at all time. StatefulSets manage the pods based on the same container specifications such as Deployments. Lets assume we have a StatefulSet with 3 replicas. Each Pod has a PV attached. 4.2.5 Jobs &amp; Cron Jobs Using the busybox image in the section about volumes we experienced that the image is very short lived. K8s is not aware of this and runs into a CrashLoopBackOff-Error. K8s will try and restart the container itself though until it BackOffs completley. Because the image is so short live, a job within the image has to be executed such as done with a shell command previously. However, what if we have a simple task that only should run like every 5 minutes, or every single day? A good idea is to use CronJobs for such tasks that start the image if needed. When comparingJobs jobs and CronJobs, jobs execute only once, whereas CronJobs execute depending on an specified expression. The following job simulates a backup to a database that runs 30 seconds in total. The part in the args specifies that the container will sleep for 20 seconds (the hypothetical backup). Afterward, the container will wait 10 seconds to shut down, as specified in ttlSecondsAfterFinished. # job.yaml apiVersion: batch/v1 kind: Job metadata: name: db-backup-job spec: # time it takes to terminate the job for one completion ttlSecondsAfterFinished: 10 template: spec: containers: - name: backup image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: - &quot;echo &#39;performing db backup...&#39; &amp;&amp; sleep 20&quot; restartPolicy: Never The CronJob below runs run every minute. Given the structure of ( * * * * * * ) - ( Minutes Hours Day-of-month Month Day-of-week Year), the cronjob expression defines as follows: # cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: db-backup-cron-job spec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: backup image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: - &quot;echo &#39;performing db backup...&#39; &amp;&amp; sleep 20&quot; restartPolicy: Never "],["services-networking.html", "4.3 Services &amp; Networking", " 4.3 Services &amp; Networking 4.3.1 Services To overall question when deploying an application is how we can access it. Each individual Pod has its own IP address. Thereby, the client can access this Pod via its IP address. Previously, we have made the app available via kubectl port-forward by forwarding the Pods’ IP to localhost. However, should only be done for testing purposes and is not a reliable and stable way to enable access. Since Pods are ephemeral, the client cannot rely on the ip address alone. For example, if an application is scaled up or down, there will be new IPs associated with each new Pod. Services Instead, Services should be used. A service is an abstract way to expose an application as a network service. The service can connect access to a pod via an interal reference, so a change of the Pods IP will not interfere with its accessibility. The service itself has a stable IP adress, a stable DNS name, and a stable port. This allows for a reliable and stable connection from the client to the service, which can then direct the traffic to the pod. There are different types of Services. ClusterIP (Default) NodePort ExternalName LoadBalancer 4.3.1.1 ClusterIP The ClusterIP is the default K8s service. This service type will be chosen if no specific type is selected. The ClusterIP is used for cluster internal access and does not allow for external communication. If one Pod wants to talk to another Pod inside the cluster, it will use ClusterIP to do so. The service will allow and send traffic to any pod that is healthy. 4.3.1.2 NodePort The NodePort service allows to open a static port simultaneously on all nodes. Its range lies between between 30.000/32.767. If a client wants to communicate with a node of the cluster, the client directly communicates with the node via its IP address. When the request reaches the port of the node, the NodePort service handles the request and forwards it to the specifically marked pod. This way, an application running on a pod can be exposed directly on a nodes’ IP under a specific port. You’ll be able to contact the NodePort Service from outside the cluster by requesting &lt;NodeIP&gt;:&lt;NodePort&gt;. Using a NodePort is beneficial for example when a request is sent to a node without a pod. The NodePort service will forward the request to a node which has a healthy associated pod running. However, only having one service specified per port is also a disadvantage. Having one ingress and multiple services is more desireable. The point of running K8s in the cloud is to scale up and down and if the NodeIP address changes, then we have a problem. So we should not aim to access a Node IP directly in the first place If applying below example using the frontend-deployment and the backend-deployment, we can access the frontend using the nodeport. Since using minikube, we can access the service by using minikube service frontend-node --url. Using the given IP adress it is possible to access the frontend using the NodePort service. We can also test the NodePort service when inside of a node. When accessing a node e.g. via minikube ssh, we can run curl localhost:PORT/ inside the node to derive the to derive the website data from the frontend. 4.3.1.3 LoadBalancer Loadbalancers are a standard way of exposing applications to the extern, for example the internet. Loadbalancers automatically distribute incoming traffic across multiple targets to balance the load in an equal level. If K8s is running on the cloud, e.g. AWS or GCP, a Network Load Balancer (NLB) is created. The Cloud Controller Manager (remember the Controller Manager of a Node) is resposible to talk to the underlying cloud provier. In Minikube, the external IP to access the application via the LoadBalancer can be exposed using the command minikube tunnel. 4.3.1.4 default kubernetes services There are also default K8s services created automatically to access K8s with the K8s API. Check the endpoints of the kubernetes service and the endpoints of the api-service pod within kube-system namespace. They should be the same. It is also possible to show all endpoints of the cluster using kubectl get endpoints 4.3.1.5 Exemplary setup of database and frontend microservices The following example show the deployment and linking of two different deployments. A frontend-deployment.yaml that pulls a container running a Streamlit App, and a database-deployment.yaml that runs a flask application exposing a dictionary as an exemplary and very basic database. The frontend accesses the flask database using a ClusterIP Service linked to the database-deployment. It also exposes an external IP via a Loadbalancer Service, so the streamlit app can be accesses via the browser and without the use of kubectl port-forward. Since minikube is a closed network, use minikube tunnel to allow access to it using the LoadBalancer. When looking at the ClusterIP service with kubectl describe service backendflask the IP address of the service to exposes, as well as the listed endpoints that connect to the database-deployments are shown. One can compare them to the IPs of the actual deployments - they are the same. # services_frontend-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 2 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: containers: - name: frontend image: seblum/mlops-public:frontend-streamlit imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # enviroment variable defined in the application and dockerfile # value is ip adress of the order env: # using the ip adress would be a bad idea. # use the service ip adress. # value: &quot;&lt;order-service-ip-adress&gt;:8081&quot; # how to do it should be this. # we reference to the order service - name: DB_SERVICE value: &quot;backendflask:5001&quot; ports: # we can actually use the actual ip of the service or # use the dns, as done in the example above. - containerPort: 8501 --- apiVersion: v1 kind: Service metadata: name: frontend-lb spec: type: LoadBalancer selector: app: frontend ports: - port: 80 targetPort: 8501 --- apiVersion: v1 kind: Service metadata: name: frontend-node spec: type: NodePort selector: app: frontend ports: - port: 80 targetPort: 8501 nodePort: 30000 # services_backend-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: backendflask spec: replicas: 2 selector: matchLabels: app: backendflask template: metadata: labels: app: backendflask spec: containers: - name: backendflask image: seblum/mlops-public:backend-flask imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 --- apiVersion: v1 kind: Service metadata: name: backendflask spec: # send traffic to any pod that matches the label type: ClusterIP # does not need to be specified selector: app: backendflask ports: # port the service is associated with - port: 5001 # port to access targeted by the access # in our case has to be the same as in backendflask. targetPort: 5000 4.3.2 Service Discovery Service Discovery is a mechanism that lets applications and microservices locate each other on a network. In fact, we have aready used Service Discovery in the previous sections, they just haven’t been mentioned yet. If a client wants to communicate with the application, it should not use the IP of an individual Pod should not use the individual pod ip. Instead, we should rely on services as they have a stable IP address. We have already seen this in the section about Services. Yet, each pod has also an individual DNS (Domain Name System). A DNS translates a domain names to an IP address, just one lookes up a number in a telephone book, so it’s much easier to reference to a resource online. This is where service Discovery enters the game. Service Discovery Whenever a service is created, it is registered in the service registry with the service name and the service IP. Most clusters use CoreDNS as a service registry (this would be the telephone book itself). When having a look at the minikube cluster one should see are core-dns service running. Now you know what it is for. Having a closer look using kubectl describe svc &lt;name&gt;, the core-dns service has only one endpoint. If you want to have an even closer look, you can dive into a pod itself and check the file /etc/resolv.conf. There you find a nameserver where the IP is the one of the core-dns. # when querying services, it necessary # to specify the corresponding namespace kubectl get service -n kube-system # command for queriying the dns nslookup &lt;podname&gt; 4.3.2.1 kube-proxy As mentioned earlier, each node has three main components: Kubelet, Container Runtime, and the Kube-Proxy. Kube-Proxy is a network proxy running on each node and is responsible for internal network communications as well as external. It also implements a controller that watches the API server for new services and endpoints. Whenever there is a new service or endpoint, the kube-proxy creates a local IPVS rule (IP Virtual Server) that tells the node to intercept traffic destined to the ClusterIP Service. IPVS is built on top of the network filter and implements a transport-layer load balancing. This gives the ability to load balance to real service as well as redirecting traffic to pods that match service label selectors. This means, kube-proxy is intercepting all the requests and makes sure that when a request to the ClusterIP service is sent using endpoints, the request is forwarded to the healthy pods behind the endpoint. "],["volume-storage.html", "4.4 Volume &amp; Storage", " 4.4 Volume &amp; Storage Since Pods are ephemeral, any data associated is deleted when a Pod or container restarts. Applications are run stateless the majority of the times, meaning the data does not needs to be kept on the node and the data is stored on an external database. However, there are times when the data wants to be kept, shared between Pods, or when it should persist on the host file system (disk). As described in the section about Pods, a Pod can contain volumes. Volumes are exactly what is needed for such tasks. They are used to store and access data which can be persistent or long lived on K8s. There are different types of volumes, e.g.: EmptyDir HostPath Volume awsElasticBlockStore: AWS EBS volumes are persistent and originally unmounted. They are read-write-once-only tough. There are multiple other types of volumes, a full list can be found here: https://kubernetes.io/docs/concepts/storage/volumes/#volume-types 4.4.1 EmptyDir Volume An EmptyDir Volume is initially empty (as the name suggests). The volume is a temporary directory that shares the pods lifetime. If the pod dies, the contents of the emptyDir are lost as well. The EmptyDir is also used to share data between containers inside a Pod during runtime. # volume_empty-dir.yaml apiVersion: apps/v1 kind: Deployment metadata: name: emptydir-volume spec: selector: matchLabels: app: emptydir-volume template: metadata: labels: app: emptydir-volume spec: # add a volume to the deployment volumes: # mimics a caching memory type - name: cache # specify the volume type and the temp directory emptyDir: {} # of course there could also be a second volume added containers: - name: container-one image: busybox # image used for testing purposes # since the testing image immediately dies, we want to # execute an own sh command to interact with the volume volumeMounts: # The name must match the name of the volume - name: cache # interal reference of the pod mountPath: /foo command: - &quot;/bin/sh&quot; args: - &quot;-c&quot; - &quot;touch /foo/bar.txt &amp;&amp; sleep 3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # create a second container with a different internal mountPath - name: container-two image: busybox volumeMounts: - name: cache mountPath: /footwo command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; As stated in the yaml, the busybox image immediately dies. If the Containers where created without the shell commands, the pod would be in a crashloopbackoff-state. To prevent the Pod to do so it is caught with the sleepcommands until it scales down. Accessing a container using kubectl exec, it can be checked whether the foo/bar.txt has been created in container-one. When checking the second container container-two, the same file should be visible as well. This is because both containers refer to the same volume. Keep in mind though that the mountPath of the container-two is different. # get in container kubectl exec -it &lt;emptydir-volume-name&gt; -c container-one -- sh # check whether bar.txt is present ls # accessing the second container, there is also a file foo/bar.txt # remember, both containers share the same volume kubectl exec -it &lt;emptydir-volume-name&gt; -c container-two -- sh ls 4.4.2 HostPath Volume THe HostPath Volume type is used when an application needs to access the underlying host file system, meaning the file system of the node. HostPath represents a pre-existing file or directory on the host machine. However, this can be quite dangerous and should be used with caution. If having the right access, the application can interfere and basically mess up the host. It is therefore recommended to set the rights to read only to prevent this from happening. # volume_hostpath.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hostpath-volume spec: selector: matchLabels: app: hostpath-volume template: metadata: labels: app: hostpath-volume spec: volumes: - name: var-log # specify the HostPath volume type hostPath: path: /var/log containers: - name: container-one image: busybox volumeMounts: - mountPath: /var/log name: var-log readOnly: true command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; Similar to the EmptyDir Volume example, you can check the implementation of the HostPath Volume by accessing the volume. When comparing the file structures of the hostpath-volume deployment and the directory path: /var/log on the node the deployment is running, they should be the same. All the changes made to either on of them will make the changes available on the other. By making changes via the pod we can directly influence the Node. Again, this is why it is important to keep it read-only. # access the kubectl exec -it &lt;hostpath-volume-name&gt; -- sh # ssh into node minikube ssh &lt;node&gt; 4.4.3 Persistent Volumes Persistent Volumes allow to store data beyond a Pods lifecycle. If a Pod fails, dies or moves to a different node, the data is still intact and can be shared between pods. Persistent Volume types are implemented as plugins that K8s can support(a full list can be found online). Different types of Persistent Volumes are: NFS Local Cloud Network storage (AWS EBS, Azure File Storage, Google Persistent Disk) The following example show how the usage of Persistent Volumes works on the AWS cloud. K8s is running on an AWS EKS Cluster and AWS EBS Volumes attached to it. The Container storage interface (CSI) of K8s to use Persistent Volumes is implemented by the EBS provider, e.g. the aws-ebs-plugin. This enables a the use of Persistent Volumes in the EKS cluster. Therefore, a Persistent Volume (PV) is rather the mapping between the storage provider (EBS) and the K8s cluster, than a volume itself. The storage class of a Persistent Volume can be configured to the specific needs. Should the storage be fast or slow, or do we want to have each as a storage? Or might there be other parameters to configure the storage? If a Pods or Deployments want to consume storage of the PV, they need to get access to the PV. This is done via a so called persistent volume claim (PVC). All of this is part of a Persistent Volume Subsystem. The Persistent Volume Subsystem provides an API for users and administrators. The API abstracts details of how storage is provided from how it is consumed. Again, the provisioning of storage is done via a PV and the consumption via a PCV. Persistent Volume Subsystem Listed below are again the three main components when dealing with Persistent Volumes in K8s Persistent Volume: is a storage resource provisioned by an administrator PVC: is a user’s request for and claim to a persistent volume. Storage Class: describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned. So how are Persistent Volumes specified in our deployments yamls? As there are kind:Pod ressources, there can similarly kind:PersistentVolume and kind:PersistentVolumeClaim be specified. At first, a PV is created. As we run on minikube and not on the cloud, a local storage in the node is allocated. Second, a PVC is created requesting a certain amount of that storage. This PVC is then linked in the specifications of a Deployment to allow its containers to utilized the storage. Before applying the yaml-files we need to allocate the local storage by claiming storage on the node and set the paths specified in the yamls. To do this, we ssh into the node using minikube ssh. We can then create a specific path on the node such as /mnt/data/. We might also create a file in it to test accessing it when creating a PVC to a Pod. Since we do not know yet on what node the Pod is scheduled, we should create the directory on both nodes. Below are all steps listed again. # ssh into node minikube ssh # create path sudo mkdir /mnt/data # create a file with text sudo sh -c &quot;echo &#39;this is a pvc test&#39; &gt; /mnt/data/index.html&quot; # do this on both nodes as pod can land on either one of them Afterward we can apply the yaml files and create a PV, PVC, and the corresponding Deployment utilizing the PVC. The yaml code below shows this process. # volume_persistent-volume.yaml apiVersion: v1 kind: PersistentVolume metadata: name: mypv spec: # specifiy the capacity of the PersistentVolume capacity: storage: &quot;100Mi&quot; volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: manual hostPath: path: &quot;/mnt/data&quot; # specify the hostPath on the node # that&#39;s the path we specified on our node --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mypvc spec: resources: requests: # we request the same as the PV is specified # so we basically request everything storage: &quot;100Mi&quot; volumeMode: Filesystem storageClassName: &quot;manual&quot; accessModes: - ReadWriteOnce --- apiVersion: apps/v1 kind: Deployment metadata: name: pv-pvc-deployment spec: selector: matchLabels: app: pv-pvc template: metadata: labels: app: pv-pvc spec: volumes: - name: data # define the use of the PVC by specifying the name # specify the pod/deployment can use the PVC persistentVolumeClaim: claimName: mypvc containers: - name: pv-pvc image: nginx:latest volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; # since the PVC is stated, the container needs to # mount inside it # name is equal to the pvc name specified name: data resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: pv-pvc spec: type: LoadBalancer selector: app: pv-pvc ports: - port: 80 targetPort: 80 By accessing a pod using kubectl exec -it &lt;persistent-volume-name&gt; -- sh we can check whether the path is linked using the PVC. Now, the end result may seem the same as what we did with the HostPath Volume. But it actually is not, it just looks like it since both, the PersistentVolume and the HostPath connect to the Host. Yet, the locally mounted path would be somewhere else when running in the cloud. The PV configuration would point to another storage source instead of a local file system, for example an attached EBS of EFS storage. Since we also created a LoadBalancer service, we can run minikube tunnel to expose the application deplyment under localhost:80. It should show the input of the index.html file we created on the storage. "],["environment-configuration-security.html", "4.5 Environment, Configuration &amp; Security", " 4.5 Environment, Configuration &amp; Security 4.5.1 Namespaces Namespaces allow to organize resources in the cluster, which makes it more overseeable when there are multiple resources for different needs. Maybe we want to organize by team, department, or according to a development environment (dev/prod), etc. By default, K8s will use the default-namespace for resources that have not been specified otherwise. Similarly, kubectl interacts with the default namespace as well. Yet, there are already different namespace in a basic K8s cluster default - The default namespace for objects with no other namespace kube-system - The namespace for objects created by the Kubernetes system kube-public - This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. kube-node-lease - This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales. Of course, there is also the possibility of creating ones own namespace and using it by attaching a e.g. Deployment to it, such as seen in the following example. # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring --- apiVersion: apps/v1 kind: Deployment metadata: name: monitoring-deployment namespace: monitoring spec: replicas: 1 selector: matchLabels: app: monitoring-deployment template: metadata: labels: app: monitoring-deployment spec: containers: - name: monitoring-deployment image: &quot;grafana/grafana:latest&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 When creating a Service, a corresponding DNS entry is created as well, such as seen in the Services section when calling backendflask directly. This entry is created according to the namespace which is denoted to the service. This can be useful when using the same configuration across multiple namespaces such as development, staging, and production. It is also possible to reach across namespaces. One needs to use the fully qualified domain name (FQDN) tough, such as &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local. 4.5.2 Labels, Selectors and Annotations In the previous sections we already made use of labels, selectors, and annotations, e.g. when matching the ClusterIP service to the back-deployments. Labels are a key-value pair that can be attached to objects such as Pods, Deployments, Replicaset, Services, etc. Overall, they are used to organize and select objects. Annotations are an unstructured key-value mapping stored with a resource that may be set by external tools to store and retrieve any metadata. In contrast to labels and selectors, annotations are not used for querying purposes but rather to attach arbitrary non-identifying metadata. These data are used to assist tools and libraries to work with the K8s ressource, for example to pass configuration around between systems, or to send values so external tools can perform more informed decisions based on the annotations provided. Selectors are used to filter K8s objects based on a set of labels. A selector basically simply uses a boolean language to select pods. The selector matches the labels under a an all or nothing principle, meaning everything specified in the selector must be fulfilled by the labels. However, this works not the other way around. If there are multiple labels specified and the selector matches only one of them, the selector will match the ressource itself. How a selector matches the labels can be tested using the kubectl commands as seen below. # Show all pods including their labels kubectl get pods --show-labels # Show only pods that match the specified selector key-value pairs kubectl get pods --selector=&quot;key=value&quot; kubectl get pods --selector=&quot;key=value,key2=value2&quot; # in short one can also write kubectl get pods -l key=value # or also look for multiple kubectl get pods -l &#39;key in (value1, value2)&#39; When using ReplicaSets in a Deployment, their selector matches the labels to a specific pod (check e.g. the section describing Deployments). Any Pods matching the label of the selector will be created according to the specified replicas. Of course, there can also be multiple labels specified. The same principle accounts when working with Services. Below example shows two different Pods and two NodePort services. Each service matches to a Pod based on their selector-label relationship. Have a look at their specific settings using kubectl. The Nodeport Service labels-and-selectors-2 has no endpoints, as it is a all-or-none-principle and none of the created Pods matches the label environment=dev. In contrast, even though the Pod cat-v1 has multiple labels specified app: cat-v1; version: one, the NodePort Service labels-and-selectors is linked to it. It is also linked to the second Pod cat-v2. # labels.yaml apiVersion: v1 kind: Pod metadata: name: cat-v1 labels: app: cat-v1 version: one spec: containers: - name: cat-v1 image: &quot;seblum/mlops-public:cat-v1&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; --- apiVersion: v1 kind: Pod metadata: name: cat-v2 labels: app: cat-v1 spec: containers: - name: cat-v2 image: &quot;seblum/mlops-public:cat-v2&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; --- apiVersion: v1 kind: Service metadata: name: labels-and-selectors spec: type: NodePort selector: app: cat-v1 ports: - port: 80 targetPort: 5000 --- apiVersion: v1 kind: Service metadata: name: labels-and-selectors-2 spec: type: NodePort selector: app: cat-v1 environment: dev ports: - port: 80 targetPort: 5000 4.5.3 ConfigMaps When building software, the same container image should be used for development, testing, staging, and production stage. Thus, container images should be reusable. What usually changes are only the configuration settings of the application. ConfigMaps allow to store such configurations as a simple mapping of key-value pairs. Most of the time, the configuration within a config map is injected using environment variables and volumes. However, ConfigMaps should only be used to store configuration files, not sensitive data, as they do not secure them. Besides allow for an easy change of variables, another benefit of using ConfigMaps is that changes in the configuration are not disruptive, meaning the application can still run while the configuration changes without affecting the application. However, one needs to keep in mind that change made to ConfigMaps and environment variables will not be reflected on already and currently running containers. The following example creates two different ConfigMaps. The first one includes three environment variables as data. The second one include a more complex configuration of an nginx server. # configmaps.yaml apiVersion: v1 kind: ConfigMap metadata: name: app-properties data: app-name: kitty app-version: 1.0.0 team: engineering --- apiVersion: v1 kind: ConfigMap metadata: name: nginx-conf data: # configuration in .conf nginx.conf: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location /health { access_log off; return 200 &quot;healthy\\n&quot;; } } Additionally, a Deployment is created which uses both ConfigMaps. A ConfigMap is declared under spec.volumes as well. It is also possible to state a reference to both ConfigMaps simultaneously. The Deployment creates two containers. The first container mounts each ConfigMap as a Volume. Container two uses environment variables to access and configure the key-value pairs of the ConfigMaps and store them on the container. # configmaps_deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: config-map spec: selector: matchLabels: app: config-map template: metadata: labels: app: config-map spec: volumes: # specify ConfigMap nginx-conf - name: nginx-conf configMap: name: nginx-conf # specify ConfigMap app-properties - name: app-properties configMap: name: app-properties # if both configmaps shall be mounted under one directory, # we need to use projected - name: config projected: sources: - configMap: name: nginx-conf - configMap: name: app-properties containers: - name: config-map-volume image: busybox volumeMounts: - mountPath: /etc/cfmp/ngnix # is defined here in the nginx-volume to mount name: nginx-conf # everything from that configMap is mounted as a file # the file content is the value themselves - mountPath: /etc/cfmp/properties name: app-properties - mountPath: etc/cfmp/config name: config command: - &quot;/bin/sh&quot; - &quot;-c&quot; args: - &quot;sleep 3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; - name: config-map-env image: busybox resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # as previously, keep the busybox container alive command: - &quot;/bin/sh&quot; - &quot;-c&quot; args: - &quot;env &amp;&amp; sleep 3600&quot; env: # environment variables to read in from config map # for every data key-value pair in config Map, an own # environment variable is created, which gets # the value from the corresponding key - name: APP_VERSION valueFrom: configMapKeyRef: name: app-properties key: app-version - name: APP_NAME valueFrom: configMapKeyRef: name: app-properties key: app-name - name: TEAM valueFrom: configMapKeyRef: name: app-properties key: team # reads from second config map - name: NGINX_CONF valueFrom: configMapKeyRef: name: nginx-conf key: nginx.conf We can check for the attached configs by accessing the containers via the shell, similar to what we did in the section about Volumes. In the container config-map-volume, the configs are saved under the respective mountPath of the volume. In the config-map-env, the configs are stored as environment variables. # get in container -volume or -env kubectl exec -it &lt;config-map-name&gt; -c &gt;container-name&lt; -- sh # check subdirectories ls # print environment variables printenv 4.5.4 Secrets Secrets, as the name suggests, store and manage sensitive information. However, secrets are actually not secrets in K8s. They can quite easily decoded using kubectl describe on a secret and decode it using the shell command echo &lt;password&gt; | base64 -d. Thus, sensitive information like database password should never be stored in secrets. There are much better ressources to store such data, for example a Vault on the cloud provider itself. However, secret can be used so that you don’t need to include confidential data in your application code. Since they are stored and created independently of the Pods that use them, there is less risk of being exposed during the workflow of creating, viewing, and editing Pods. It is possible to create secrets using imperative approach as shown below. # create the two secrets db-password and api-token kubectl create secret generic mysecret-from-cli --from-literal=db-password=123 --from-literal=api-token=token # output the new secret as yaml kubectl get secret mysecret -o yaml # create a file called secret with a file-password in it echo &quot;super-save-password&quot; &gt; secret # create a secret from file kubectl create secret generic mysecret-from-file --from-file=secret Similar to ConfigMaps, secrets are accessed via an environment variable or a volume. # secrets.yaml apiVersion: apps/v1 kind: Deployment metadata: name: secrets spec: selector: matchLabels: app: secrets template: metadata: labels: app: secrets spec: volumes: # get the secret from a volume - name: secret-vol secret: # the name of the secret we created earlier secretName: mysecret-from-cli containers: - name: secrets image: busybox volumeMounts: - mountPath: /etc/secrets name: secret-vol env: # nane of the secret in the container - name: CUSTOM_SECRET # get the secret from an environment variable valueFrom: secretKeyRef: # name and key of the secret we created earlier name: mysecret-from-file key: secret command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; 4.5.4.1 Exemplary use case of secrets When pulling from a private dockerhub repository, applying the deployment will throw an error since there are no username and password specified. As they should not be coded into the deployment yaml itself, they can be accessed via a secret. In fact, a specific secret can be specified for docker registry. The secret can be specified using the imperative approach. kubectl create secret docker-registry docker-hub-private \\ --docker-username=YOUR_USERNAME \\ --docker-password=YOUR_PASSWORD \\ --docker-email=YOUR_EMAIL Finally, the secret is specified in the deployment configuration where it can be accessed during application. # secret_dockerhub.yaml apiVersion: apps/v1 kind: Deployment metadata: name: secret-app spec: selector: matchLabels: app: secret-app template: metadata: labels: app: secret-app spec: # specifiy the docker-registry secret to be accessed imagePullSecrets: - name: docker-hub-private containers: - name: secret-app # of course you need an own private repository # to pull and change the name accordingly image: seblum/private:cat-v1 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 "],["observability-maintenance.html", "4.6 Observability &amp; Maintenance", " 4.6 Observability &amp; Maintenance 4.6.1 Health Checks Applications running in service need to be healthy at all times so they are ready to receive traffic. K8s uses a process called health checks to test whether an application is alive. If there are any issues and the application is unhealthy, K8s will restart the process. Yet, checking only whether a process is up and running might be not sufficient. What if, e.g., a client wants to connect to a database and the connection cannot be established, even though the app is up and running? To solve more specific issues like this, health checks like a liveness probe or readiness probe can be used. If there have not been specified, K8s will use the default checks to test whether a process is running. 4.6.1.1 Liveness Probe The Kubelet of a node uses Liveness Probes to check whether an application runs fine or whether it is unable to make progress and its stuck on a broken state. For example, it could catch a deadlock, a database connection failure, etc. The Liveness Probe can restart a container accordingly. To use a Liveness Probe, an endpoint needs to be specified. The benefit of this is, that it is simple to define what it means for an application to be healthy just by defining a path. 4.6.1.2 Readiness Probe Similar to a Liveness Probe, the Readniness Probe is used by the kubelet to check when the container is ready to start accepting traffic. A Pod is considered ready when all of its containers are ready. Its configuration is also done by specifying a path to what it means the application is healthy. A lot of frameworks, like e.g. springboot, actually provide a path to use. Belows configuration shows a Deployment which includes a Liveness and a Readiness Probe. The image of the deployment is set up so its process is killed after a given number of seconds. This has been passed using environment variables such as seen in the script. Both, the Liveness and the Readiness Probe have the same parameters in the given example. initialDelaySeconds: The probe will not be called until x seconds after all the containers in the Pod are created. timeoutSeconds: The probe must respond within a x-second timeout and the HTTP status code must be equal to or greater than 200 and less than 400. periodSeconds: The probe is called every x seconds by K8s failureThreshold: The container will fail and restart if more than x consecutive probes fail # healthchecks.yaml apiVersion: apps/v1 kind: Deployment metadata: name: backendflask-healthcheck spec: replicas: 1 selector: matchLabels: app: backendflask-healthcheck template: metadata: labels: app: backendflask-healthcheck environment: test tier: backend department: engineering spec: containers: - name: backendflask-healthcheck # check whether I have to change the backend app to do this. image: &quot;seblum/mlops-public:backend-flask&quot; imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # Specification of the the Liveness Probe livenessProbe: httpGet: # path of the url path: /liveness port: 5000 # time the liveness probe starts after pod is started initialDelaySeconds: 5 timeoutSeconds: 1 failureThreshold: 3 # period of time when the checks should be performed periodSeconds: 5 # Specification of the Readiness Probe readinessProbe: httpGet: path: /readiness port: 5000 initialDelaySeconds: 10 # change to 1 seconds and see the pod not going to go ready timeoutSeconds: 3 failureThreshold: 1 periodSeconds: 5 env: # variable for the container to be killed after 60 seconds - name: &quot;KILL_IN_SECONDS&quot; value: &quot;60&quot; ports: - containerPort: 5000 --- apiVersion: v1 kind: Service metadata: name: backendflask-healthcheck spec: type: NodePort selector: app: backendflask-healthcheck ports: - port: 80 targetPort: 8080 nodePort: 30000 --- apiVersion: v1 kind: Service metadata: name: backendflask-healthcheck spec: type: ClusterIP selector: app: backendflask-healthcheck ports: - port: 80 targetPort: 8080 4.6.2 Debugging "],["helm.html", "4.7 Helm", " 4.7 Helm tbd "],["terraform.html", "Chapter 5 Terraform", " Chapter 5 Terraform Terraform is an open-source, declarative programming language developed by HashiCorp and allows to create both on-prem and cloud resources in the form of writing code. This is also know as Infrastructure as Code (IAC). There are several IaC tools in the market today and Terraform is only one of them. Yet it is a well known and established tool and during the course of this project we only focus on this. HashiCorp describes that: Terraform is an infrastructure as code (IaC) tool that allows you to build, change, and version infrastructure safely and efficiently. This includes both low-level components like compute instances, storage, and networking, as well as high-level components like DNS entries and SaaS features. This means that users are able to manage and provision an entire IT infrastructure using machine-readable definition files and thus allowing faster execution when configuring infrastructure, as well as enableing full traceability of changes. Terraform comes with several hundred different providers that can be used to provision infrastructure, such as Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, DataDog, etc. The given chapter introduces the concepts &amp; usage of Terraform which will be needed to create the introduced MLOps Airflow deployment in an automated and tracable way. We will learn how to use Terraform to provision ressources as well as to structure a Terraform Project. Prerequisites To be able to follow this tutorial, one needs to have the AWS CLI installed as well as the AWS credentials set up. Needles to say an AWS accounts needs to be present. It is also recommended to have basic knowledge of the AWS Cloud as this tutorials used the AWS infrastructure to provision cloud resources. The attached resource definitions are specified to the AWS region eu-central-1. It might be necessary to change accordingly if you are set in another region. Further, Terraform itself needs to be installed. Please refer to the corresponding sites. The scripts are run under Terraform version v1.2.4. Later releases might have breaking changes. One can check its installation via terraform version. "],["basic-usage.html", "5.1 Basic usage", " 5.1 Basic usage A Terraform project is basically just a set of files in a directory containing resource definitions of cloud ressources to be created. Those Terraform files, denoted by the ending .tf, use Terraform’s configuration language to define the specified resources. In the following example there are two definitions made: a provider and a resource. Later in this chapter we will dive deeper in the structure of the language. For now, we only need to know this script is creating a file called hello.txt that includes the text \"Hello, Terraform\". It’s our Terraform version of Hello World! provider &quot;local&quot; { version = &quot;~&gt; 1.4&quot; } resource &quot;local_file&quot; &quot;hello&quot; { content = &quot;Hello, Terraform&quot; filename = &quot;hello.txt&quot; } 5.1.1 terraform init When a project is run for the first time the terraform project needs to be initialized. This is done via the terraform init command. Terraform scans the project files in this step and downloads any required providers needed (more details to providers in a following section). In the given example this is the local procider. # Initializes the working directory which consists of all the configuration files terraform init Terraform init 5.1.2 terraform validate The terraform validate command checks the code for syntax errors. This is optional yet a way to handle initial errors or minor careless mistakes # Validates the configuration files in a directory terraform validate Terraform validate 5.1.3 terraform plan The terraform plan command verifies what action Terraform will perform and what resources will be created. This step is basically a dry run of the code to be executed. It also returns the provided values and some permission attributes which have been set. # Creates an execution plan to reach a desired state of the infrastructure terraform plan Terraform plan 5.1.4 terraform apply The command Terraform apply creates the resource specified in the .tf files. Initially, the same output as in the terraform plan step is shown (hence its dry run). The output further states which resources are added, which will be changed, and which resources will be destroyed. After confirming the actions the resource creation will be executed. Modifications to previously deployed ressources can be implemented by using terraform apply again. The output will denote that there are resources to change. # Provision the changes in the infrastructure as defined in the plan terraform apply Terraform apply 5.1.5 terraform destroy To destoy all created ressouces and to delete everything we did before, there is a terraform destroy command. # Deletes all the old infrastructure resources terraform destroy Terraform destroy "],["core-components-3.html", "5.2 Core Components", " 5.2 Core Components The following section will explain the core components and building blocks of Terraform. This will enable you to build your very first Terraform definition files. 5.2.1 Providers Terraform relies on plugins called providers to interact with Cloud providers, SaaS providers, and other APIs. Each provider adds specific resource types and/or data sources that can be managed by Terraform. For example, the aws provider shown below allows to specify resources related to the AWS Cloud such as S3 Buckets or EC3 Instances. Depending on the provider it is necessary to supply it with specific parameters. The aws provier for example needs the region as well as username and password. If nothing is specified it will automatically pull these information from the AWS CLI and the credentials specified under the directory .aws/config. It is also a best practice to specify the version of the provider, as the providers are usually maintained and updated on a regular basis. provider &quot;aws&quot; { region = &quot;us-east-1&quot; } 5.2.2 Resources A resource is the core building block when working with Terraform. It can be a \"local_file\" such as shown in the example above, or a cloud resource such as an \"aws_instance\" on aws. The resource type is followed by the custom name of the resource in Terraform. Resource definitions are usually specified in the main.tffile. Each customization and setting to a ressource is done within its resource specification. The style convention when writing Terraform code states that the resource name is named in lowercase as well as it should not repeat the resource type. An example can be seen below # Ressource type: aws_instance # Ressource name: my-instance resource &quot;aws_instance&quot; &quot;my-instance&quot; { # resource specification ami = &quot;ami-0ddbdea833a8d2f0d&quot; instance_type = &quot;t2.micro&quot; tags = { Name = &quot;my-instance&quot; ManagedBy = &quot;Terraform&quot; } } 5.2.3 Data Sources Data sources in Terraform are “read-only” resources, meaning that it is possible to get information about existing data sources but not to create or change them. They are usually used to fetch parameters needed to create resources or generally for using parameters elsewhere in Terraform configuration. A typical example is shown below as the \"aws_ami\" data source available in the AWS provider. This data source is used to recover attributes from an existing AMI (Amazon Machine Image). The example creates a data source called \"ubuntu” that queries the AMI registry and returns several attributes related to the located image. data &quot;aws_ami&quot; &quot;ubuntu&quot; { most_recent = true filter { name = &quot;name&quot; values = [&quot;ubuntu/images/hvm-ssd/ubuntu-trusty-14.04-amd64-server-*&quot;] } filter { name = &quot;virtualization-type&quot; values = [&quot;hvm&quot;] } owners = [&quot;099720109477&quot;] # Canonical } Data sources and their attributes can be used in resource definitions by prepending the data prefix to the attribute name. The following example used the \"aws_ami\" data source within an \"aws_instace\" resource. resource &quot;aws_instance&quot; &quot;web&quot; { ami = data.aws_ami.ubuntu.id instance_type = &quot;t2.micro&quot; } 5.2.4 State A Terraform state stores all details about the resources and data created within a given context. Whenever a resource is create terrafrom stores its identifier in the statefile terraform.tfstate. Providing information about already existing resources is the primary purpose of the statefile. Whenever a Terraform script is applied or whenever the resource definitions are modified, Terraform knows what to create, change, or delete based on the existing entries within the statefile. Everything specified and provisioned within Terraform will be stored in the statefile. This should be kept in mind and detain to store sensitive information such as initial passwords. Terraform uses the concept of a backend to store and retrieve its statefile. The default backend is the local backend which means to store the statefile in the project’s root folder. However, we can also configure an alternative (remote) backend to store it elsewhere. The backend can be declared within a terraform block in the project files. The given example stores the statefile in an AWS S3 Bucket callen some-bucket. Keep in mind this needs access to an AWS account and also needs the AWS provider of terraform. terraform { backend &quot;s3&quot; { bucket = &quot;some-bucket&quot; key = &quot;some-storage-key&quot; region = &quot;us-east-1&quot; } } "],["modules.html", "5.3 Modules", " 5.3 Modules A Terraform module allows to reuse resources in multiple places throughout the project. They act as a container to package resource configurations. Much like in standard programming languages, Terraform code can be organized across multiple files and packages instead of having one single file containing all the code. Wrapping code into a module not only allows to reuse it throughout the project, but also in different environments, for example when deploying a dev and a prod infrastructure. Both environments can reuse code from the same module, just with different settings. A Terraform module is build as a directory containing one or more resource definition files. Basically, when putting all our code in a single directory, we already have a module. This is exactly what we did in our previous examples. However, terraform does not include subdirectories on its own. Subdirectories must be called explicitly using a terraform moduleparameter. The example below references a module located in a ./network subdirectory and passes two parameters to it. # main.tf module &quot;network&quot; { source = &quot;./networking&quot; create_public_ip = true environment = &quot;prod&quot; } Each module consists of a similar file structure as the root directory. This includes a main.tf where all resources are specified, as well as files for different data sources such as variables.tf and outputs.tf. However, providers are usually configured only in the root module and are not reused in modules. Note that there are different approaches on where to specify the providers. They are either specified in the main.tf or a separate providers.tf. It does not make a difference for Terraform as it does not distinguish between the resource definition files. It is merely a strategy to keep code and project in a clean and consistent structure. root │ main.tf │ variables.tf │ outputs.tf │ └── networking │ main.tf │ variables.tf │ outputs.tf 5.3.1 Input Variables Each module can have multiple Input Variables. Input Variables serve as parameters for a Terraform module so users can customize behavior without editing the source. In the previous example of importing a network module, there have been two input variables specified, create_public_ip and environment. Input variables are usually specified in the variables.tf file. # variables.tf variable &quot;instance_name&quot; { type = string default = &quot;awesome-instance&quot; description = &quot;Name of the aws instance to be created&quot; } Each variable has a type (e.g. string, map, set, boolen) and may have a default value and description. Any variable that has no default must be supplied with a value when calling the module reference. This means that variables defined at the root module need values assigned to as a requirement so Terraform will not fail. This can be done by different resources, for example a variable’s default value via the command line using the terraform apply -var=\"variable=value\"option via environment variables starting with TF_VAR_; Terraform will check them automatically a .tfvars file where the variable values are specified; Terraform can load variable definitions from these files automatically (please check online resources for further insights) Variables can be used in expressions using the var.prefix such as shown in below example. We use the resource configuration of the previous example to create an aws_instance but this time its name is provided by an input variable. # main.tf resource &quot;aws_instance&quot; &quot;awesome-instance&quot; { ami = &quot;ami-0ddbdea833a8d2f0d&quot; instance_type = &quot;t2.micro&quot; tags = { Name = var.instance_name } } 5.3.2 Output Variables Similar to Input variables, a terraform module has output variables. As their name states, output variables return values of a Terraform module and are denoted in the outputs.tf file as expected. A module’s consumer has no access to any resources or data created within the module itself. However, sometimes a modules attrivutes are needed for another module or resource. Output variables address this issue by exposing a defined subset of the created resources. The example below defines an output value instance_address containing the IP address of an EC2 instance the we create with a module. Any module that reference this module can use the instance_address value by referencing it via module.module_name.instance_address # outputs.tf output &quot;instance_address&quot; { value = aws_instance.awesome-instance.private_ip description = &quot;Web server&#39;s private IP address&quot; } outputs 5.3.3 Local Variables Additionally to Input variables and output variables a module provides the use of local variables. Local values are basically just a convenience feature to assign a shorter name to an expression and work like standard variables. This means theor scope is also limited to the module they are declared in. Using local variables reduces code repetitions which can be especially valuable when dealing with output variables from a module. # main.tf locals { vpc_id = module.network.vpc_id } module &quot;network&quot; { source = &quot;./network&quot; } module &quot;service1&quot; { source = &quot;./service1&quot; vpc_id = local.vpc_id } module &quot;service2&quot; { source = &quot;./service2&quot; vpc_id = local.vpc_id } "],["additional-tips-tricks.html", "5.4 Additional tips &amp; tricks", " 5.4 Additional tips &amp; tricks Of course there is much more to Terraform than these small examples can provide. Yet there are also some contrains when working with Terraform or declarative languages in general. Typically they do not have for-loops or other traditional procedural logic built into the language to repeat a piece of logic or conditional if-statements to configure reasources on demand. However, there are ways there are some ways to deal with this issue and to create multiple respurces without copy and paste. Terraform comes with different looping constructs, each used slightly different. The count and for_each meta arguments enable us to create multiple instances of a resource. 5.4.1 count Count can be used to loop over any resource and module. Every Terraform resource has a meta-parameter count one can use. Count is the simplest, and most limited iteration construct and all it does is to define how many copies to create of a resource. When creating multiple instance with one specification, the problem is that each instance must have a unique name, otherwise Terraform would cause an error. Therefore we need to index the meta-parameter just like doing it in a for-loop to give each resource a unique name. The example below shows how to do this on an AWS IAM user. resource &quot;aws_iam_user&quot; &quot;example&quot; { count = 2 name = &quot;neo.${count.index}&quot; } count variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;snake&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.user_names) # returns the number of items in the given array name = var.user_names[count.index] } count list After using count on a resource it becomes an array of resources rather than one single resource. The same hold when using count on modules. When adding count to a module it turns it into an array of modules. This can round into problems because the way Terraform identifies each resource within the array is by its index. Now, when removing an item from the middle of the array, all items after it shift one index back. This will result in Terraform deleting every resource after that item and then re-creating these resources again from scratch So after running terraform plan with just three names, Terraform’s internal representation will look like this: variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.user_names) # returns the number of items in the given array name = var.user_names[count.index] } count index deletion Count as conditional Count can also be used as a form of a conditional if statement. This is possible as Terraform supports conditional expressions. If count is set to one 1, one copy of that resource is created; if set to 0, the resource is not created at all. Writing this as a conditional expression could look something like the follow, where var.enable_autoscaling is a boolean variable either set to True or False. resource &quot;example-1&quot; &quot;example&quot; { count = var.enable_autoscaling ? 1 : 0 name = var.user_names[count.index] } 5.4.2 for-each The for_each expression allows to loop over lists, sets, and maps to create multiple copies of a resource just like the count meta. The main difference between them is that count expects a non-negative number, whereas for_each only accepts a list or map of values. Using the same example as above it would look like this: variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;snake&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { for_each = toset(var.user_names) name = each.value } output &quot;all_users&quot; { value = aws_iam_user.example } Using a map of resource with the for_each meta rather than an array of resources as with count has the benefit to remove items from the middle of the collection safely and without re-creating the resources following the deleted item. Of course, the same can also be done for modules. module &quot;users&quot; { source = &quot;./iam-user&quot; for_each = toset(var.user_names) user_name = each.value } 5.4.3 for Terraform also offers a similar functionality as python list comprehension in the form of a for expression. This should not be confused with the for-each expression seen above. The basic syntax is shown below to convert the list of names of previous examples in var.names to uppercase: output &quot;upper_names&quot; { value = [for name in var.names : upper(name)] } output &quot;short_upper_names&quot; { value = [for name in var.names : upper(name) if length(name) &lt; 5] } Using for to loop over lists and maps within a string can be used similarly. This allows us to use control statements directly withing strings using a syntax similar to string interpolation. output &quot;for_directive&quot; { value = &quot;%{ for name in var.names }${name}, %{ endfor }&quot; } 5.4.4 Workspaces Terraform workspaces allow us to keep multiple state files for the same project. When we run Terraform for the first time in a project, the generated state file will go into the default workspace. Later, we can create a new workspace with the terraform workspace new command, optionally supplying an existing state file as a parameter. workspaces "],["exemplary-deployment.html", "5.5 Exemplary Deployment", " 5.5 Exemplary Deployment Building a VPC with EC2 &amp; S3 Finally, we want to put everything together and provision our own cloud infrastructure using Terraform. We will create a AWS Virtual private Cloud with EC2 Instances running on it, and S3 Buckets attached to the them. We will use for_each and count to create multiple instaces. The Terraform infrastructure is separated into three modules VPS, EC2, and S3. root │ main.tf │ variables.tf │ outputs.tf │ └── networking │ │ main.tf │ │ variables.tf │ │ outputs.tf │ └── ec2 │ │ main.tf │ │ variables.tf │ │ outputs.tf │ └── s3 │ main.tf │ variables.tf │ outputs.tf "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
