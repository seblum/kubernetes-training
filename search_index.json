[["index.html", "MLOps Engineering Building, Deploying, and Managing Machine Learning Workflows with Airflow and MLflow on Kubernetes. Summary", " MLOps Engineering Building, Deploying, and Managing Machine Learning Workflows with Airflow and MLflow on Kubernetes. Sebastian Blum 2024-02-03 Summary MLOps Engineering: Building, Deploying, and Managing Machine Learning Workflows with Airflow and MLflow on Kubernetes” is a comprehensive guide to understanding the principles and roles of MLOps, as well as operational principles such as CI/CD and versioning. Through the use of an exemplary ML platform utilizing Airflow, MLflow, and JupyterHub on AWS EKS, readers will learn how to deploy this platform using infrastructure as code Terraform. The book is structured with chapters that introduce the various tools such as Airflow, MLflow, Kubernetes, and Terraform, and also includes a use case that demonstrates the utilization of the platform through Airflow workflows and MLflow tracking. Whether you are a data scientist, engineer, or developer, this book provides the necessary knowledge and skills to effectively implement MLOps practices and platforms in your organization. "],["preamble.html", "Preamble", " Preamble This project started out of an interest in multiple domains. At first, I was working on a Kubeflow platform at the time and haven’t had much experience in the realm of MLOps. I was starting with K8s and Terraform and was interested to dig deeper. I did so by teaching myself and I needed a project. What better also to do than to build “my own” MLOps plattform. I used Airflow since it is widely uses for workflow management and I also wanted to use it from the perspective of a data scientist, meaning to actually build some pipelines with it. This idea of extending the project to actually have a running use case expanded this work to include MLFlow for model tracking. Topics The overall aim is to build and create a MLOps architecture based on Airflow running on AWS EKS. Ideally, this architecture is create using terraform. Model tracking might be done using MLFlow, Data tracking using DVC. Further mentioned might be best practices in software development, CI/CD, Docker, and pipelines. I might also include a small Data Science use case utilizing the Airflow Cluster we built. The book contains two parts with distinct focuses. The first part comprises Chapters 3: Airflow, 4: MLflow, 5: Kubernetes, and 6: Terraform, which consist of tutorials on the specific tools aforementioned. These chapters also serve as prerequisites for the subsequent part. Among these tutorials, the chapters dedicated to Airflow and MLflow are oriented towards Data Scientists, providing insights into their usage. The chapters centered around Kubernetes and Terraform target Data- and MLOps Engineers, offering detailed guidance on deploying and managing these tools. The second part, comprising Chapters 7: ML Platform Design, 8: Platform Deployment, and 9: Use Case Development, delves into an exemplary machine learning Platform. This part of the book demands a strong background in engineering due to its complexity. While these chapters cover the essential tools introduced in the previous part, they may not explore certain intricate aspects used like OAuth authentication and networking details in great depth. Moreover, it is crucial to note that the ML Platform example presented is not intended for production deployment, as there should be significant security concerns considered. Instead, its main purpose is to serve as an informative illustration of ML platforms and MLOps engineering principles. Chapter 1: Introduction and 2: Ops Tools and Principles serve as an introduction to the domain of MLOps A work in progress This project / book / tutorial / whatever this is or will be, startet by explaining the concept of Kubernetes. The plan is to continuously update it by further chapters. Since there is no deadline, there is no timeline, and I am also not sure whether there will exist something final to be honest. This document is written during my journey in the realm of MLOps. It is therefore in a state of continuous development. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Imagine being able to effortlessly deploy, manage, and monitor your machine learning models with ease. No more headaches from version control issues, data drift, and model performance degradation. That’s the power of MLOps. “MLOps Engineering: Building, Deploying, and Managing Machine Learning Workflows with Airflow and MLflow on Kubernetes” takes you on a journey through the principles, practices, and platforms of MLOps. You’ll learn how to create an end-to-end pipeline for machine learning projects, using cutting-edge tools and techniques like Kubernetes, Terraform, and GitOps, and working with tools to ease your machine learning workflow such as Apache Airflow and MLflow Tracking. Before we begin, let’s have a more closer look on what MLOps actually is, what principles it incorporates, and how it distinguished from traditional DevOps. "],["machine-learning-workflow.html", "1.1 Machine Learning Workflow", " 1.1 Machine Learning Workflow A machine learning workflow typically involves several stages. These stages are closely related and sometimes overlap as some stages may involve multiple iterations. In the following, the machine learning workflow is broken down to five different stages to make things easier, and give an overview. ML lifecycle 1. Data Preparation In the first stage, data used to train a machine learning model is collected, cleaned, and preprocessed. Preprocessing includes tasks to remove missing or duplicate data, normalize data, and split data into a training and testing set. 2. Model Building In the second stage, a machine learning model is selected and trained using the prepared data. This includes tasks such as selecting an appropriate algorithm as a machine learning model, training the model, and tuning the model’s parameters to improve its performance. 3. Model Evaluation Afterward, the performance of the trained model is evaluated using the test data set. This includes tasks such as measuring the accuracy and other performance metrics, comparing the performance of different models, and identifying potential issues with the model. 4. Model Deployment Finally, the selected and optimized model is deployed to a production environment where it can be used to make predictions on new data. This stage includes tasks like scaling the model to handle large amounts of data, and deploying the model to different environments to be used in different contexts 5. Model Monitoring and Maintenance It is important to monitor the model performance and update the model as needed, once the model is deployed. This includes tasks such as collecting feedback from the model, monitoring the model’s performance metrics, and updating the model as necessary. Each stage is often handled by the same tool or platform which makes a clear differentiation across stages and tools fairly difficult. Further, some machine learning workflows will not have all the steps, or they might have some variations. A machine learning workflow is thereby not a walk in the park and the actual model code is just a small piece of the work. Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex. (D. Sculley, et al., 2015) Working with and developing machine learning models, monitoring their performance, and continuously retraining it on new data with possible alternative models can be challenging and involves the right tools. 1.1.1 ML Worflow Tools There are several machine learning workflow tools that can help streamline the process of building and deploying machine learning models. By Integrating them into the machine learning workflow there can be three main processes and functionalities of tools derived: (1) Workflow Management, (2) Model Tracking, and (3) Model Serving. All three of these processes are closely related to each other and are often handled by the same tool or platform. 1.1.1.1 Workflow Management Workflow Management is the process of automating and streamlining the stages involved in building, training, and deploying machine learning models. This includes tasks such as data preprocessing, model training, and model deployment. Workflow management tools allow for the coordination of all the necessary stages in the machine learning pipeline, and the tracking of the pipeline’s progress. Apache Airflow is an open-source platform for workflow management and is widely used to handle ETL-processes of data, or automate the training and deployment of machine learning models. 1.1.1.2 Model Tracking Model Tracking is the process of keeping track of different versions of a machine learning model, including their performance, and the parameters and data used to train it. Model Tracking tools are often used at the development and testing stages of the machine learning workflow. During development, tracking allows to keep track of the different versions of a model, compare their performance and learning parameters, and finally help to select the best version of the model to deploy. Model tracking also allows to check the performance of a machine learning model during testing, and to assure it meets industry requirements. MLflow is an open-source platform to manage the machine learning lifecycle, including experiment tracking, reproducibility, and deployment. Similarly, DVC (Data Version Control) is a tool that allows to version control, manage, and track not only models but also data. 1.1.1.3 Model Serving Model Serving refers to the process of deploying a machine learning model in a production environment, so it can be used to make predictions on new data. This includes tasks such as scaling the model to handle large amounts of data, deploying the model to different environments, and monitoring the performance of the deployed model. Model serving tools are specifically used at the deployment stage of the machine learning workflow and can handle the necessary tasks mentioned beforehand. There are multiple tools that integrate the funtionality of serving models, each different in its specific use cases, for example KF Serve, BentoML, Seldon, AWS Sagemaker or also the already mentioned tools like MLflow. 1.1.1.4 Integrated development environment An Integrated Development Environment (IDE) is a software application that provides a comprehensive and unified workspace for software developers and programmers to write, edit, debug, and manage code. It is not solely used for the development of machine learning models but rather for all software, or code related work. It typically combines a text editor with features such as code completion, syntax highlighting, version control integration, debugging tools, and project management functionalities. IDEs streamline the software development process by offering a cohesive environment where developers can efficiently write and test code, navigate through projects, access documentation, and collaborate with team members. It is thereby necessary to include an IDE in our list as ML workflow tools as it is a necessary component when developing machine learning models in a collaborative and scalable way. A well known and established tool is JupyterHub which allows organizations to provide a shared and collaborative environment for data scientists, researchers, and developers. It enables the deployment of interactive Jupyter notebooks to create, edit, and execute Jupyter notebooks within a centralized infrastructure to develop data driven code. 1.1.2 Developing Machine Learning Models In the development phase of a machine learning model, many stages of the machine learning workflow are carried out manually. For instance, testing machine learning code is often done in a notebook or script, rather than through an automated pipeline. Similarly, deploying the model is typically a manual process, and only involves serving the model for inference, rather than deploying an entire machine learning system. This manual deployment process can result in infrequent deployments and the management of only a few models that do not change frequently. Additionally, the deployment process is usually not handled by Data Scientists but is instead managed by an operations team. This can create a disconnection between the development and production stages, and once deployed, there is typically no monitoring of the model’s performance, meaning no feedback loop is established for retraining. Performing stages manually and managing models in production at scale can become exponentially more challenging. To overcome these issues, it is recommended to adopt a unifying framework for production, known as MLOps. This framework will shift the focus from managing existing models to developing new ones, making the process more efficient and effective. "],["machine-learning-operations-mlops.html", "1.2 Machine Learning Operations (MLOps)", " 1.2 Machine Learning Operations (MLOps) Machine Learning Operations (MLOps) combines principles from developing and deploying machine learning models to enable an efficient operation and management of machine learning models in a production environment. The goal of MLOps is to automate and streamline the machine learning workflow as much as possible, while also ensuring that each machine learning model is performing as expected. This allows organizations to move quickly and efficiently from prototyping to production. The infrastructure involved in MLOps includes both, hardware and software components. Hardware components aim to make the deployment of machine learning models scalable. This includes servers and storage devices for data storage and model deployment, as well as specialized hardware such as GPUs for training and inferencing. Software components include version control systems, continuous integration and continuous deployment (CI/CD) tools, containerization and orchestration tools, and monitoring and logging tools. There are several cloud-based platforms that provide pre-built infrastructure to manage and deploy models, automate machine learning workflows, and monitor and optimize the performance of models in production. For example AWS SageMaker, Azure Machine Learning, and Google Cloud AI Platform. Overall, the key concepts and best practices of MLOps, and the use of tools and technologies to support the MLOps process are closely related to regular DevOps principles which are a standard for the operation of software in the industry. 1.2.1 ML + (Dev)-Ops Machine Learning Operations (MLOps) and Development Operations (DevOps) both aim to improve the efficiency and effectiveness of software development and deployment. Both practices share major similarities, but there are some key differences that set them apart. The goal of DevOps is to automate and streamline the process of building, testing, and deploying software, to make the management of the software lifecycle as quick and efficient as possible. This can be achieved by using tools, such as: Containerization: Tools such as Docker and Kubernetes are used to package and deploy applications and services in a consistent and reproducible manner. Version Control: Tools such as Git, SVN, and Mercurial are used to track and manage changes to code and configuration files, allowing teams to collaborate and roll back to previous versions if necessary. Continuous Integration and Continuous Deployment (CI/CD): Tools such as Jenkins, Travis CI, and Github Actions are used to automate the building, testing, and deployment of code changes. Monitoring and Logging: Tools such as Prometheus, Grafana, and ELK are used to monitor the health and performance of applications and services, as well as to collect and analyze log data. Cloud Management Platforms: AWS, Azure, and GCP are used to provision, manage, and scale infrastructure and services in the cloud. Scaling and provisioning infrastructure can be automated by using Infrastructure as Code (IaC) Tools like Terraform. DevOps focuses on the deployment and management of software in general (or traditional sofware), while MLOps focuses specifically on the deployment and management of machine learning models in a production environment. The goal is basically the same as in DevOps, yet deploying a machine learning model. While this is achieved by the same tools and best practices used in DevOps, deploying machine learning models (compared to software) adds a lot of complexity to the process. Traditional vs ML software Machine learning models are not just lines of code, but also require large amounts of data, and specialized hardware, to function properly. Further, machine learning models and their complex algorithms might need to change when there is a shift in new data. This process of ensuring that machine learning models are accurate and reliable with new data leads to additional challenges. Another key difference is that MLOps places a great emphasis on model governance, which ensures that machine learning models are compliant with relevant regulations and standards. The above list of tools within DevOps can be extended to the following for MLOps. Machine Learning Platforms: Platforms such as TensorFlow, PyTorch, and scikit-learn are used to develop and train machine learning models. Experiment Management Tools: Tools such as MLflow, Weights &amp; Biases, and Airflow are used to track, version, and reproduce experiments, as well as to manage the model lifecycle. Model Deployment and Management Tools: Tools such as TensorFlow Serving, AWS Sagemaker, and Seldon Core are used to deploy and manage machine learning models in production. Data Versioning and Management Tools: Tools such as DVC (Data Version Control) and Pachyderm are used to version and manage the data used for training and evaluating models. Automated Model Evaluation and Deployment Tools: Tools such as AlgoTrader and AlgoHub are used to automate the evaluation of machine learning models and deploy the best-performing models to production. It’s important to note that the specific tools used in MLOps and DevOps may vary depending on the organization’s needs. Some of the tools are used in both but applied differently in each, e.g. container orchestration tools like Kubernetes. The above lists do also not claim to be complete and there are of course multiple more tools. 1.2.2 MLOps Lifecycle Incorporating the tools introduced by DevOps and MLOps can extend the machine learning workflow outlined in the previous section, resulting in a complete MLOps lifecycle that covers each stage of the machine learning process while integrating automation practices. Integrating MLOps into machine learning projects introduces additional complexity into the workflow. Although the development stage can be carried out on a local machine, subsequent stages are typically executed within a cloud platform. Additionally, the transition from one stage to another is automated using tools like CI/CD, which automate testing and deployment. MLOps also involves integrating workflow management tools and model tracking to enable monitoring and ensure reproducible model training. This enables proper versioning of code, data, and models, providing a comprehensive overview of the project. 1.2.3 MLOps Engineering MLOps Engineering is the discipline that applies the previously mentioned software engineering principles to create the necessary development and production environment for machine learning models. It usually comines the skills and expertise of Data Scientists and -Engineers, Machine Learning Engineers, and DevOps engineers to ensure that machine learning models are deployed and managed efficiently and effectively. One of the key aspects of MLOps Engineering is infrastructure management. The infrastructure refers to the hardware, software and networking resources that are needed to support the machine learning models in production. The underlying infrastructure is crucial for the success of MLOps as it ensures that the models are running smoothly, are highly available and are able to handle the load of the incoming requests. It also helps to prevent downtime, ensure data security and guarantee the scalability of the models. MLOps Engineering is responsible for designing, building, maintaining and troubleshooting the infrastructure that supports machine learning models in production. This includes provisioning, configuring, and scaling the necessary resources like cloud infrastructure, Kubernetes clusters, machine learning platforms, and model serving infrastructure. It is useful to use configuration management and automation tools like Infrastructure as Code (e.g. Terraform) to manage the infrastructure in a consistent, repeatable and version controlled way. This allows to easily manage and scale the infrastructure as needed, and roll back to previous versions if necessary. It is important to note that one person can not exceed on all of the above mentioned tasks of designing, building, maintaining, and troubleshooting. Developing the infrastructure for machine learning models in production usually requires multiple people working with different and specialized skillsets and roles. "],["roles-and-tasks-in-mlops.html", "1.3 Roles and Tasks in MLOps", " 1.3 Roles and Tasks in MLOps The MLOps lifecycle typically involves several key roles and responsibilities, each with their own specific tasks and objectives. Roles and their operating areas within the MLOps lifecycle 1.3.1 Data Engineer A Data Engineer designs, builds, maintains, and troubleshoots the infrastructure and systems that support the collection, storage, processing, and analysis of large and complex data sets. Tasks include designing and building data pipelines, managing data storage, optimizing data processing, ensuring data quality, and monitoring the necessary data pipelines and systems. They usually work closely with Data Scientists to understand their data needs and ensure the infrastructure supports their requirements. 1.3.2 Data Scientist Data Scientists are usually responsible for developing and testing machine learning models within the development stage. Their work typically involves investigating large amounts of data, the necessary preprocessing steps, and choosing a suitable machine learning algorithm that solves the business need. They also develop a functioning ML model respective to the data. 1.3.3 ML Engineer Machine Learning (ML) Engineers work closely with Data Scientists to develop and deploy machine learning models in a production environment. They are responsible for creating and maintaining the infrastructure and tools needed to run machine learning models at scale and in production. They are also responsible for the day-to-day management and monitoring of machine learning models in a production environment. They use tools and technologies to track model performance and make sure that models are running smoothly and produce accurate results. This also includes taking measures in case of data or model shifts1. 1.3.4 MLOps Engineer The MLOps Engineer is responsible to create and maintain the infrastructure and tooling needed to train and deploy models, to monitor the performance of deployed models. They also implement processes for collaboration and version control. Overall, this includes tasks such as setting up and configuring the necessary hardware and software environments, creating and maintaining documentation, and implementing automated testing and deployment processes. An MLOps Engineer must be able to navigate this complexity and ensure that the models are deployed and managed in a way that is both efficient and effective. 1.3.5 DevOps Engineer DevOps Engineers are responsible for automating and streamlining the process of building, testing, and deploying software. They use best practices from software development and operations, such as version control, continuous integration and delivery (CI/CD), and monitoring and observability, to ensure that the software is performing as expected in production. 1.3.6 Additional roles &amp; function The previous roles only show a small portion of all contributors within data projects. Additional roles within an industry context might also include Model Governance, which is responsible to ensure that models are accurate, reliable, and compliant with relevant regulations and standards, or Business Stakeholders who provide feedback and requirements for the models in a business context. It’s also worth noting that some of these roles may overlap and different organizations may have different ways of structuring their teams and responsibilities. The most important thing is to have clear communication and collaboration between all the different teams, roles, and stakeholders involved in the MLOps lifecycle. Model shift refers to the change of a deployed model compared to the model developed in a previous stage, while data shift refers to a change in the distribution of input data compared to the data used during development and testing.↩︎ "],["ops-tools-and-principles.html", "Chapter 2 Ops Tools and Principles", " Chapter 2 Ops Tools and Principles MLOps integrates a range of DevOps techniques and tools to enhance the development and deployment of machine learning models. By promoting cooperation between development and operations teams, MLOps strives to improve communication, enhance efficiency, and reduce delays in the development process. Advanced version control systems can be employed to achieve these objectives. Automation plays a significant role in achieving these goals. For instance, CI/CD pipelines streamline repetitive tasks like building, testing, and deploying software. The management of infrastructure can also be automated, by using infrastructure as code to facilitate an automated provisioning, scaling, and management of infrastructure. To enhance flexibility and scalability in the operational process, containers and microservices are used to package and deploy software. Finally, monitoring and logging tools are used to track the performance of deployed and containerized software and address any issues that arise. "],["containerization.html", "2.1 Containerization", " 2.1 Containerization Containerization is an essential component in operations as it enables deploying and running applications in a standardized, portable, and scalable way. This is achieved by packaging an application and its dependencies into a container image, which contains all the necessary code, runtime, system tools, libraries, and settings needed to run the application, isolated from the host operating system. Containers are lightweight, portable, and can run on any platform that supports containerization, such as Docker or Kubernetes. All of this makes them beneficial compared to deploying an application on a virtual machine or traditionally directly on a machine. Virtual machines would emulate an entire computer system and require a hypervisor to run, which introduces additional overhead. Similarly, a traditional deployment involves installing software directly onto a physical or virtual machine without the use of containers or virtualization. Not to mention the lack of portability of both. The concept of container images is analogous to shipping containers in the physical world. Like shipping containers can be loaded with different types of cargo, a container image can be used to create different containers with various applications and configurations. Both the physical containers and container images are standardized, just like blueprints, enabling multiple operators to work with them. This allows for the deployment and management of applications in various environments and cloud platforms, making containerization a versatile solution. Containerization offers several benefits for MLOps teams. By packaging the machine learning application and its dependencies into a container image, reproducibility is achieved, ensuring consistent results across different environments and facilitating troubleshooting. Containers are portable which enables easy movement of machine learning applications between various environments, including development, testing, and production. Scalability is also a significant advantage of containerization, as scaling up or down compute resources in an easy fashion allows to handle large-scale machine learning workloads and adjust to changing demand quickly. Additionally, containerization enables version control of machine learning applications and their dependencies, making it easier to track changes, roll back to previous versions, and maintain consistency across different environments. To effectively manage model versions, simply saving the code into a version control system is insufficient. It’s crucial to include an accurate description of the environment, which encompasses Python libraries, their versions, system dependencies, and more. Virtual machines (VMs) can provide this description, but container images have become the preferred industry standard due to their lightweight nature. Finally, containerization facilitates integration with other DevOps tools and processes, such as CI/CD pipelines, enhancing the efficiency and effectiveness of MLOps operations. "],["version-control.html", "2.2 Version Control", " 2.2 Version Control Version control is a system that records changes to a file or set of files over time, to be able to recall specific versions later. It is an essential tool for any software development project as it allows multiple developers to work together, track changes, and easily rollback in case of errors. There are two main types of version control systems: centralized and distributed. Centralized Version Control Systems (CVCS) : In a centralized version control system, there is a single central repository that contains all the versions of the files, and developers must check out files from the repository in order to make changes. Examples of CVCS include Subversion and Perforce. Distributed Version Control Systems (DVCS) : In a distributed version control system, each developer has a local copy of the entire repository, including all the versions of the files. This allows developers to work offline, and it makes it easy to share changes with other developers. Examples of DVCS include Git, Mercurial and Bazaar Version control is a vital component of software development that offers several benefits. First, it keeps track of changes made to files, enabling developers to revert to a previous version in case something goes wrong. Collaboration is also made easier with version control, as it allows multiple developers to work on a project simultaneously and share changes with others. In addition, it provide backup capabilities by keeping a history of all changes, allowing you to retrieve lost files. Version control also allows auditing of changes, tracking who made a specific change, when, and why. Finally, it enables developers to create branches of a project, facilitating simultaneous work on different features without affecting the main project, with merging later. Versioning all components of a machine learning project, such as code, data, and models, is essential for reproducibility and managing models in production. While versioning code-based components is similar to typical software engineering projects, versioning machine learning models and data requires specific version control systems. There is no universal standard for versioning machine learning models, and the definition of “a model” can vary depending on the exact setup and tools used. Popular tools such as Azure ML, AWS Sagemaker, Kubeflow, and MLflow offer their own mechanisms for model versioning. For data versioning, there are tools like Data Version Control (DVC) and Git Large File Storage (LFS). The de-facto standard for code versioning is Git. The code-versioning system Github is used for this project, which will be depicted in more detail in the following. 2.2.1 Github GitHub provides a variety of branching options to enable flexible collaboration workflows. Each branch serves a specific purpose in the development process, and using them effectively can help teams collaborate more efficiently and effectively. Main Branch: The main branch is the default branch in a repository. It represents the latest stable version and production-ready state of a codebase, and changes to the code are merged into the main branch as they are completed and tested. Feature Branch: A feature branch is used to develop a new feature or functionality. It is typically created off the main branch, and once the feature is completed, it can be merged back into the main branch. Hotfix Branch: A hotfix branch is used to quickly fix critical issues in the production code. It is typically created off the main branch, and once the hotfix is completed, it can be merged back into the main branch. Release Branch: A release branch is a separate branch that is created specifically for preparing a new version of the software for release. Once all the features and fixes for the new release have been added and tested, the release branch is merged back into the main branch, and a new version of the software is tagged and released. 2.2.2 Git lifecycle After a programmer has made changes to their code, they would typically use Git to manage those changes through a series of steps. First, they would use the command git status to see which files have been changed and are ready to be committed. They would then stage the changes they want to include in the commit using the command git add &lt;FILE-OR-DIRECTORY&gt;, followed by creating a new commit with a message describing the changes using git commit -m \"MESSAGE\". After committing changes locally, the programmer may want to share those changes with others. They would do this by pushing their local commits to a remote repository using the command git push. Once the changes are pushed, others can pull those changes down to their local machines and continue working on the project by using the command git pull. If the programmer is collaborating with others, they may need to merge their changes with changes made by others. This can be done using the git merge &lt;BRANCH-NAME&gt; command, which combines two branches of development history. The programmer may need to resolve any conflicts that arise during the merge. If the programmer encounters any issues or bugs after pushing their changes, they can use Git to revert to a previous version of the code by checking out an older commit using the command git checkout. Git’s ability to track changes and revert to previous versions makes it an essential tool for managing code in collaborative projects. While automating the code review process is generally viewed as advantageous, it is still typical to have a manual code review as the final step before approving a pull or merge request to be merged into the main branch. It is considered a best practice to mandate a manual approval from one or more reviewers who are not the authors of the code changes. "],["cicd.html", "2.3 CI/CD", " 2.3 CI/CD Continuous Integration (CI) and Continuous Delivery / Continuous Delivery (CD) are related software development practices that work together to automate and streamline the software development and deployment process of code changes to production. Deploying new software and models without CI/CD often requires a lot of implicit knowledge and manual steps. Continuous Integration (CI): is a software development practice that involves frequently integrating code changes into a shared central repository. The goal of CI is to catch and fix integration errors as soon as they are introduced, rather than waiting for them to accumulate over time. This is typically done by running automated tests and builds, to catch any errors that might have been introduced with new code changes, for example when merging a Git feature branch into the main branch. Continuous Delivery (CD): is the practice that involves automating the process of building, testing, and deploying software to a production-like environment. The goal is to ensure that code changes can be safely and quickly deployed to production. This is typically done by automating the deployment process and by testing the software in a staging environment before deploying it to production. Continuous Deployment (CD): is the practice of automatically deploying code changes to production once they pass automated tests and checks. The goal is to minimize the time it takes to get new features and bug fixes into the hands of end-users. In this process, the software is delivered directly to the end-user without manual testing and verification. The terms Continuous Delivery and Continuous Deployment are often used interchangeably, but they have distinct meanings. Continuous delivery refers to the process of building, testing, and running software on a production-like environment, while continuous deployment refers specifically to the process of running the new version on the production environment itself. However, fully automated deployments may not always be desirable or feasible, depending on the organization’s business needs and the complexity of the software being deployed. While continuous deployment builds on continuous delivery, the latter can offer significant value on its own. CI/CD integrates the principles of continuous integration and continuous delivery in a seamless workflow, allowing teams to catch and fix issues early and quickly deliver new features to users. The pipeline is often triggered by a code commit. Ideally, a Data Scientist would push the changes made to the code at each incremental step of development to a share repository, including metadata and documentation. This code commit would trigger the CI/CD pipeline to build, test, package, and deploy the model software. In contrast to the local development, the CI/CD steps will test the model changes on the full dataset and aiming to deploy for production. CI and CD practices help to increase the speed and quality of software development, by automating repetitive tasks and catching errors early, reducing the time and effort required to release new features, and increasing the stability of the deployed software. Examples for CI/CD Tools that enable automated testing with already existing build servers are for example GitHub Actions, Gitlab CI/CD, AWS Code Build, or Azure DevOps Github Actions GitHub Actions is an automation and workflow orchestration tool provided by GitHub. It allows developers to automate various tasks, processes directly within their GitHub repositories by providing a CI/CD framework for building, sharing, and executing automation workflows. The following code snippet shows an exemplary GitHub Actions pipeline to test, build and push a Docker image to the DockerHub registry. The code is structured in three parts. At first, the environment variables are defined under env. Two variables are defined here which are later called with by the command env.VARIABLE. The second part defines when the pipeline is or should be triggered. The exampele shows three possibilites to trigger a pipelines, when pushing on the master branch push, when a pull request to the master branch is granted pull_request, or when the pipeline is triggered manually via the Github interface workflow_dispatch. The third part of the code example introduces the actual jobs and steps performed by the pipeline. The pipeline consists of two jobs pytest and docker. The first represents the CI part of the pipeline. The run environment of the job is set up and the necessary requirements are installed. Afterward unit tests are run using the pytest library. If the pytest job was successful, the docker job will be triggered. The job builds the Dockerfile and pushes it automatically to the specified Dockerhub repository specified in tags. The step introduces another variable just like the env.Variable before, the secrets.. Secrets are a way by Github to safely store classified information like username and passwords. They can be set up using the Github Interface and used in the Github Actions CI using secrets.SECRET-NAME. name: Docker CI base env: DIRECTORY: base DOCKERREPO: seblum/mlops-public on: push: branches: master paths: $DIRECTORY/** pull_request: branches: [ master ] workflow_dispatch: jobs: pytest: runs-on: ubuntu-latest defaults: run: working-directory: ./${{ env.DIRECTORY }} steps: - uses: actions/checkout@v3 - name: Set up Python uses: actions/setup-python@v4 with: python-version: &#39;3.x&#39; - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt pip install pytest pip install pytest-cov - name: Test with pytest run: | pytest test_app.py --doctest-modules --junitxml=junit/test-results.xml --cov=com --cov-report=xml --cov-report=html docker: needs: pytest runs-on: ubuntu-latest steps: - name: Set up QEMU uses: docker/setup-qemu-action@v2 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v2 - name: Login to DockerHub uses: docker/login-action@v2 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build and push uses: docker/build-push-action@v3 with: file: ./${{ env.DIRECTORY }}/Dockerfile push: true tags: ${{ env.DOCKERREPO }}:${{ env.DIRECTORY }} "],["infrastructure-as-code.html", "2.4 Infrastructure as code", " 2.4 Infrastructure as code Infrastructure as Code (IaC) is a software engineering approach that enables the automation of infrastructure provisioning and management using machine-readable configuration files rather than manual processes or interactive interfaces. This means that the infrastructure is defined using code, instead of manually setting up servers, networks, and other infrastructure components. This code can be version controlled, tested, and deployed just like any other software code. It also allows to automate the process of building and deploying infrastructure resources, enabling faster and more reliable delivery of services, as well as ensuring to provide the same environment every time. It also comes with the benefit of an increased scalability, improved security, and better visibility into infrastructure changes. It is recommended to utilize infrastructure-as-code to deploy an ML platform. Popular tools for implementing IaC are for example Terraform, CloudFormation, and Ansible. Chapter 6 gives a more detailed description and a tutorial on how to use Infrastructure as code using Terraform. "],["airflow.html", "Chapter 3 Airflow", " Chapter 3 Airflow Apache Airflow is an open-source platform to develop, schedule and monitor workflows. Airflow comes with a web user interface that aims to make managing workflows as easy as possible and provides a good overview of each workflow over time and the ability to inspect logs and manage tasks, for example retrying a task in case of failure. However, the philosophy of Airflow is to define workflows as code, so coding will always be required. Thus, Airflow can also be referred to as a “Workflows as code”-tool that allows for a dynamic, extensible, and flexible management of its workflows. The Airflow platform contains different operators to easily extend and connect with many other technologies. Being able to manage a workflow for all stages of the training of ML models, and the possibility to combine Airflow with other tools like MLflow for model tracking, make Apache Airflow a great tool to incorporate in an MLOps architecture. The aim of this chapter is to give a tutorial on how to use Airflow from a user perspective, as well as give a short overview of its deployment. Airflow can be deployed in multiple ways, starting from a single processing unit on a local machine to a distributed setup with multiple compute resources for large workflows in a production setting. A detailed description of what an Airflow deployment involves is shown in the section Airflow Infrastructure. The usage tutorial is based on the local installation of Airflow. Please refer to the prerequisits on what is needed to follow through. Prerequisites The main prerequisites to follow this tutorial to have an Apache Airflow instance installed. The official documentation gives a good overview on how to do. It is sufficient to run Airflow on a local deployment using airflow standalone, or airflow webserver and airflow scheduler and accessing it via any browser under http://localhost:8080/. There is no need to set up a complex Airflow deployment on a cluster or else. Further needed is intermediate knowledge of the programming language Python and basic knowledge of bash. "],["core-components.html", "3.1 Core Components", " 3.1 Core Components Airflow serves as a batch-oriented workflow orchestration plattform. Workflows vary in their levels of complexity and in general it is a term with various meaning depending on context. When working with Airflow, a workflow usually describes a set of steps to accomplish a given data engineering tasks, e.g. downloading files, copying data, filtering information, or writing to a database, etc. An exemplary use case might be to set up an ETL pipeline that extracts data from multiple sources, the transformation of the data, as well as loading them into a machine learning model. Even the training itself of a ML model can triggered via Airflow. Another workflow step might involve the generation of a report or backups. Even though Airflow can implement programs from any language, the workflows are written and defined as Python code. Airflow allows to access each workflow via the previously mentioned web user interface, or via a command-line interface. Writing workflows in code has the benefit to use version control systems like Git to ensure roll backs to previous versions as well as to develop a workflow with a team of developers simultaneously. Using Git also allows to include further DevOps principles such as pipelines, and testing and validating the codes functionality. 3.1.1 DAGs A workflows in Airflow is implemented as a DAG, a Directed Acyclic Graph. A graph describes the actual set of components of a workflow. It is directed because it has an inherent flow representing dependencies between its components. It is acyclic as it does not loop or repeat. The DAG object is needed to nest the separate tasks intp a workflow. A workflow specified in code, e.g. Python, is often also referred to as a pipeline. This terminology can be used synonymosly when working with Airflow. The following code snippet depicts how to define a DAG object in Python code. The dag_id string is a unique identifier to the DAG object. The default_args dictionary consists of additional parameters that can be specified. There are only shown two additional parameters. There are a lot more though which can be seen in the official documentation. from airflow.models import DAG from pendulum import datetime # Using extra arguments allows to customize in a clear structure # e.g. when setting the time zone in a DAG. default_args = { &#39;start_date&#39;: datetime(2023, 1, 1, tz=&quot;Europe/Amsterdam&quot;), &#39;schedule_interval&#39;: &#39;None&#39; } example_dag = DAG( dag_id=&#39;DAG_fundamentals&#39;, default_args=default_args ) The state of a workflow can accessed via the web user inteface, such as shown in below images. The first image shows Airflows overview of all DAGs currently “Active” and further information about them such as their “Owner”, how many “Runs” have been performaned and whether they were successful, and much more. The second image depicts a detailed overview of the DAG “xcom_fundamentals”. Besudes looking into the Audit Logs of the DAG and the Task Duration, it is also possible to check the Code of the DAG. The Airflow command line interface also allows to interact with the DAGs. Below command shows its exemplary usage and how to list all active DAGs. Further examples of using the CLI in a specific context are show in the subsection about Tasks. # Create the database schema airflow db init # Print the list of active DAGs airflow dags list People sometimes think of a DAG definition file as a place where the actual data processing is done. That is not the case at all! The scripts purpose is to only define a DAG object. It needs to evaluate quickly (in seconds, not minutes) since the scheduler of Airflow will load and execute it periodically to account for changes in the DAG definition. A DAG usually consists of multiple steps it runs through, also names as tasks. Tasks themselves consist of operators. This will be outlined in the following subsections. 3.1.2 Operators An Operator represents a single predefined task in a workflow. It is basically a unit of work that Airflow has to complete. Operators usually run independently and generally do not share any information by themselves. There are different categories of operators to perform different tasks with, for example Action operators, Transfer operators, or Sensors. Action Operators execute a basic task based on the operators specifications. Examples are the BashOperator, the PythonOperator, or the EmailOperator. The operator names already suggest what kind of executions they provide; the PythonOperator runs Python tasks. Transfer Operators are designed to transfer data from one place to another, for example to copy data from one cloud bucket to another. Those operators are often stateful, which means the downloaded data are first stored locally and then uploaded to a destination storage. The principle of a stateful execution defines them as an own category of operator. Finally, Sensors are a special subclass of operators that are triggered when an external event is happening, for example the creation of a file. NOTE: The PythonOperator is actually declared a deprecated function. Airflow 2.0 promotes to use the @task-decorator of Taskflow to define tasks in a more pythonic context. Yet, it still works in Airflow 2.0 and is still a very good example on how operators work. 3.1.2.1 Action Operators BashOperator As its name suggests, the BashOperator executes commands in the bash shell. from airflow.operators.bash_operator import BashOperator bash_task = BashOperator( task_id=&#39;bash_example&#39;, bash_command=&#39;echo &quot;Example Bash!&quot;&#39;, dag=action_operator_fundamentals ) PythonOperator The PythonOperator expects a Python callable. Airflow passes a set of keyword arguments from the op_kwargs dictionary to the callable as input. from airflow.operators.python_operator import PythonOperator def sleep(length_of_time): time.sleep(length_of_time) sleep_task = PythonOperator( task_id=&#39;sleep&#39;, python_callable=sleep, op_kwargs={&#39;length_of_time&#39;: 5}, dag=action_operator_fundamentals ) EmailOperator The EmailOperator allows to send predefined emails from an Airflow DAG run. For example, this could be used to notify if a workflow was successfull or not. The EmailOperator does require the Airflow system to be configured with email server details as a prerequisite. Please refer to the official docs on how to do this. from airflow.operators.email_operator import EmailOperator email_task = EmailOperator( task_id=&#39;email_sales_report&#39;, to=&#39;sales_manager@example.com&#39;, subject=&#39;automated Sales Report&#39;, html_content=&#39;Attached is the latest sales report&#39;, files=&#39;latest_sales.xlsx&#39;, dag=action_operator_fundamentals ) 3.1.2.2 Transfer Operators GoogleApiToS3Operator The GoogleApiToS3Operator makes requests to any Google API that supports discovery and uploads its response to AWS S3. The example below loads data from Google Sheets and saves it to an AWS S3 file. from airflow.providers.amazon.aws.transfers.google_api_to_s3 import GoogleApiToS3Operator google_sheet_id = &lt;GOOGLE-SHEET-ID &gt; google_sheet_range = &lt;GOOGLE-SHEET-RANGE &gt; s3_destination_key = &lt;S3-DESTINATION-KEY &gt; task_google_sheets_values_to_s3 = GoogleApiToS3Operator( task_id=&quot;google_sheet_data_to_s3&quot;, google_api_service_name=&quot;sheets&quot;, google_api_service_version=&quot;v4&quot;, google_api_endpoint_path=&quot;sheets.spreadsheets.values.get&quot;, google_api_endpoint_params={ &quot;spreadsheetId&quot;: google_sheet_id, &quot;range&quot;: google_sheet_range}, s3_destination_key=s3_destination_key, ) DynamoDBToS3Operator The DynamoDBToS3Operator copies the content of an AWS DynamoDB table to an AWS S3 bucket. It is also possible to specifiy criteria such as dynamodb_scan_kwargs to filter the transfered data and only replicate records according to criteria. from airflow.providers.amazon.aws.transfers.dynamodb_to_s3 import DynamoDBToS3Operator table_name = &lt;TABLE-NAME &gt; bucket_name = &lt;BUCKET-NAME &gt; backup_db = DynamoDBToS3Operator( task_id=&quot;backup_db&quot;, dynamodb_table_name=table_name, s3_bucket_name=bucket_name, # Max output file size in bytes. If the Table is too large, multiple files will be created. file_size=20, ) AzureFileShareToGCSOperator The AzureFileShareToGCSOperator transfers files from the Azure FileShare to the Google Storage. Even though the storage systems are quite similar, this operator is beneficial when the cloud provider needs to be changed. The share_name-parameter denotes the Azure FileShare share name to transfer files from. Similarly, the dest_gcsspecifies the destination bucket on the Google Cloud. from airflow.providers.google.cloud.transfers.azure_fileshare_to_gcs import AzureFileShareToGCSOperator azure_share_name = &lt;AZURE-SHARE-NAME &gt; bucket_name = &lt;BUCKET-NAME &gt; azure_directory_name = &lt;AZURE-DIRECTORY-NAME &gt; sync_azure_files_with_gcs = AzureFileShareToGCSOperator( task_id=&quot;sync_azure_files_with_gcs&quot;, share_name=azure_share_name, dest_gcs=bucket_name, directory_name=azure_directory_name, replace=False, gzip=True, google_impersonation_chain=None, ) NOTE: The Python code for the Transfer Operators has not been tested by myself. The examples are based on the official Airflow documentation. 3.1.2.3 Sensors A sensor is a special subclass of an operator that is triggered when an external event is happening or a certain condition is be true. Such conditions are for example the creation of a file, an upload of a database record, or a certain response from a web request. It is also possible to specify further requirements to check the condition. The mode argument sets how to check for a condition. mode='poke' denotes to run a task repeatedly until it is successful (this is the default), whereas mode='reschedule' gives up a task slot and tries again later. Simultaneously, the poke_interval defines how long a sensor should wait between checks, and the timeout parameter defines how long to wait before letting a task fail. Below example shows a FileSensor that checks the creation of a file with a poke_interval defined. from airflow.contrib.sensors.file_sensor import FileSensor file_sensor_task = FileSensor(task_id=&#39;file_sense&#39;, filepath=&#39;salesdata.csv&#39;, poke_interval=30, dag=sensor_operator_fundamentals ) Other sensors are for example: * ExternalTaskSensor - waits for a task in another DAG to complete * HttpSensor - Requests a web URL and checks for content * SqlSensor - Runs a SQL query to check for content 3.1.3 Tasks To use an operator in a DAG it needs to be instantiated as a task. Tasks determine how to execute an operator’s work within the context of a DAG. The concepts of a Task and Operator are actually somewhat interchangeable as each task is actually a subclass of Airflow’s BaseOperator. However, it is useful to think of them as separate concepts. Tasks are Instances of operators and are usually assigned to a Python variable. The following code instantiates the BashOperator to two different variables task_1 and task_2. The depends_on_past argument ensures that the previously scheduled task has succeeded before the current task is triggered. task_1 = BashOperator( task_id=&quot;print_date&quot;, bash_command=&quot;date&quot;, dag=task_fundamentals ) task_2 = BashOperator( task_id=&quot;set_sleep&quot;, depends_on_past=False, bash_command=&quot;sleep 5&quot;, retries=3, dag=task_fundamentals ) task_3 = BashOperator( task_id=&quot;print_success&quot;, depends_on_past=True, bash_command=&#39;echo &quot;Success!&quot;&#39;, dag=task_fundamentals ) Tasks can be referred to by their task_id either using the web interface or using the CLI within the airflow tools. airflow tasks test runs task instances locally, outputs their log to stdout (on screen), does not bother with dependencies, and does not communicate the state (running, success, failed, …) to the database. It simply allows testing a single task instance. The same accounts for airflow dags test # Run a single task with the following command airflow run &lt;dag_id&gt; &lt;task_id&gt; &lt;start_date&gt; # Run tasks locally for testing airflow tasks test &lt;dag_id&gt; &lt;task_id&gt; &lt;input-parameter&gt; # Testing the task print_date airflow tasks test task_fundamentals print_date 2015-06-01 # Testing the task sleep airflow tasks test task_fundamentals sleep 2015-06-01 3.1.3.1 Task dependencies A machine learning or data workflow usually has a specific order in which its tasks should run. Airflow alloes to define a specific order of task completion using task dependencies that are either referred to as upstream or downstream tasks. In Airflow 1.8 and later, this can be defined using the bitshift operators in Python. &gt;&gt; - or the upstream operator (before) &lt;&lt; - or the downstream operator (after) An exemplary code and chaining examples of tasks would look like this: # Simply chained dependencies task_1 &gt;&gt; task_2 &gt;&gt; task_3 # Mixed dependencies task_1 &gt;&gt; task_3 &lt;&lt; task_2 # which is similar to task_1 &gt;&gt; task_3 task_2 &gt;&gt; task_3 # or [task_1, task_2] &gt;&gt; task_3 # It is also possible to define dependencies with task_1.set_downstream(task_2) task_3.set_upstream(task_1) # It is also possible to mix it completely wild task_1 &gt;&gt; task_3 &lt;&lt; task_2 task_1.set_downstream(task_2) It is possible to list all tasks within a DAG using the CLI. Below commands show two approaches. # Prints the list of tasks in the &quot;task_fundamentals&quot; DAG airflow tasks list task_fundamentals # Prints the hierarchy of tasks in the &quot;task_fundamentals&quot; DAG airflow tasks list task_fundamentals --tree In general, each task of a DAG runs on a different compute resource (also called worker). It may require an extensive use of environment variables to achieve running on the same environment or with elevated privileges. This means that tasks naturally cannot cross communicate which impedes the exchange of information and data. To achieve cross communication an additional feature of Airflow needs to be used, called XCom. 3.1.4 XCom XCom (short for “cross-communication”) is a mechanism that allows information passing between tasks in a DAG. This is beneficial as by default tasks are isolated within Airflow and may run on entirely different machines. The goal of using XComs is not to pass large values like dataframes themselves. XComs are used to store all relevant metadata that are needed to pass data from one task to another, including infomration about the sender, the recipient, and the data itself. If there is the need to pass a big data frame from task_a to task_b, the data is stored in a persistent storage solution (a bucket, or database) and the information about the storage location is stored in the XCom. This means that task_a pushes the data to a storage solution and writes the information where the data is stored (e.g. a AWS S3 URI) within an XCom to the Airflow database. Afterward, task_b can access this information and retrieve the data from the external storage using the AWS . The XCom is identified by a key-value pair stored in the Airflow Metadata Database. The key of an XCom is basically its name and consists of a tuple (dag_id, task_id, execution_date,key). key within the XComs name denotes the name of the stored data and is a configurable string (by default it’s return_value). The XComs value is a json serializable. It is important to note that it is only designed for small amounts of data, depending on the attached metadata database (e.g. 2GB for SQLite, or 1GB Postgres database). XComs are explicitly pushed and pulled to/from the metadata database using the xcom_push and xcom_pull methods. While the push_*-methods upload the XCom, the pull-method downloads information from XCom. The XCom element can be viewed within Airflows web interface which is quite helpful to debug or monitor a DAG. Below example shows this mechanics. However, when looking at the push one can see a difference in their functionality. The method push_by_returning uses the operators’ auto-push functionality that pushes their results into an default XCom key (the default is return_value). Using the auto-push functionality allows to only use Python return statements and is enabled by setting the do_xcom_push argument to True, which it also is by default ( @task functions do this as well). To push an XCom with a specific key, the xcom_push-method needs to be called explicitly. In order to access the xcom_push one needs to access the task instance (ti) object. It can be accessed by passing the \"ti\" parameter to the Python callable of the PythonOperator. Its usage can be seen in the push method, where also a custom key is given to the XCom. Similarly, the puller function uses the xcom_pull method to pull the previously pushed values from the metadata databes. from airflow.models import DAG from pendulum import datetime from airflow.operators.python_operator import PythonOperator xcom_fundamentals = DAG( dag_id=&#39;xcom_fundamentals&#39;, start_date=datetime(2023, 1, 1, tz=&quot;Europe/Amsterdam&quot;), schedule_interval=None ) # DAG definition etc. value_1 = [1, 2, 3] value_2 = {&#39;a&#39;: &#39;b&#39;} def push(**kwargs): &quot;&quot;&quot;Pushes an XCom without a specific target&quot;&quot;&quot; kwargs[&#39;ti&#39;].xcom_push(key=&#39;value from pusher 1&#39;, value=value_1) def push_by_returning(**kwargs): &quot;&quot;&quot;Pushes an XCom without a specific target, just by returning it&quot;&quot;&quot; # Airflow does this automatically as auto-push is turned on. return value_2 def puller(**kwargs): &quot;&quot;&quot;Pull all previously pushed XComs and check if the pushed values match the pulled values.&quot;&quot;&quot; ti = kwargs[&#39;ti&#39;] # get value_1 pulled_value_1 = ti.xcom_pull(key=None, task_ids=&#39;push&#39;) # get value_2 pulled_value_2 = ti.xcom_pull(task_ids=&#39;push_by_returning&#39;) # get both value_1 and value_2 the same time pulled_value_1, pulled_value_2 = ti.xcom_pull( key=None, task_ids=[&#39;push&#39;, &#39;push_by_returning&#39;]) print(f&quot;pulled_value_1: {pulled_value_1}&quot;) print(f&quot;pulled_value_2: {pulled_value_2}&quot;) push1 = PythonOperator( task_id=&#39;push&#39;, # provide context is for getting the TI (task instance ) parameters provide_context=True, dag=xcom_fundamentals, python_callable=push, ) push2 = PythonOperator( task_id=&#39;push_by_returning&#39;, dag=xcom_fundamentals, python_callable=push_by_returning, # do_xcom_push=False ) pull = PythonOperator( task_id=&#39;puller&#39;, # provide context is for getting the TI (task instance ) parameters provide_context=True, dag=xcom_fundamentals, python_callable=puller, ) # push1, push2 are upstream to pull [push1, push2] &gt;&gt; pull 3.1.5 Scheduling A workflow can be run either triggered manually or on a scheduled basis. Each DAG maintains a state for each workflow and the tasks within the workflow, and specifies whether it is running, failed, or a success. Airflow scheduling is designed to run as a persistent service in an production environment and can be customized. When running Airflow locally, executing the airflow scheduler via the CLI will use the configuration specified in airflow.cfg and start the service. The Airflow scheduler monitors all DAGs and tasks, and triggers those task instances whose dependencies have been met. A scheduled tasks needs several attributes specified. When looking at the first example of how to define a DAG we can see that we already defined the attributes start_date, and schedule_interval. We can also add optional attributes such as end_date, and max_tries. from airflow.models import DAG default_args = { &#39;start_date&#39;: &#39;2023-01-01&#39;, # (optional) when to stop running new DAG instances &#39;end_date&#39;: &#39;2023-01-01&#39;, # (optional) how many attempts to make &#39;max_tries&#39;: 3, &#39;schedule_interval&#39;: &#39;@daily&#39; } example_dag = DAG( dag_id=&#39;scheduling_fundamentals&#39;, default_args=default_args ) 3.1.6 Taskflow Defining a DAG and using operators as shown in the previous sections is the classic approach to define a workflow in Airflow. However, Airflow 2.0 introduced the TaskFlow API which allows to work in a more pythonic way using decorators. DAGs and tasks can be created using the @dagor @task decorators. The function name itself acts as the unique identifier for the DAG or task respectively. All of the processing in a TaskFlow DAG is similar to the traditional paradigm of Airflow, but it is all abstracted from the developers. This allows developers to focus on the code. There is also no need to specify task dependencies as they are automatically generated within TaskFlow based on the functional invocation of tasks. Defining a workflow of an ETL-pipeline using the TaskFlow paradigm is shown in belows example. The pipeline invokes an extract task, sends the ordered data to a transform task for summarization, and finally invokes a load task with the previously summarized data. Its quite easy to catch that the Taskflow workflow contrasts with Airflow’s traditional paradigm in several ways. import json import pendulum from airflow.decorators import dag, task # Specify the dag using @dag # The Python function name acts as the DAG identifier # (see also https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html) @dag( schedule=None, start_date=pendulum.datetime(2021, 1, 1, tz=&quot;UTC&quot;), catchup=False, tags=[&quot;example&quot;], ) def taskflow_api_fundamentals(): # set a task using @task @task() def extract(): data_string = &#39;{&quot;1001&quot;: 301.27, &quot;1002&quot;: 433.21, &quot;1003&quot;: 502.22}&#39; order_data_dict = json.loads(data_string) return order_data_dict @task(multiple_outputs=True) def transform(order_data_dict: dict): total_order_value = 0 for value in order_data_dict.values(): total_order_value += value return {&quot;total_order_value&quot;: total_order_value} @task() def load(total_order_value: float): print(f&quot;Total order value is: {total_order_value:.2f}&quot;) # task dependencies are automatically generated order_data = extract() order_summary = transform(order_data) load(order_summary[&quot;total_order_value&quot;]) # Finally execute the DAG taskflow_api_fundamentals() Even the passing of data between tasks which might run on different workers is all handled by TaskFlow so there is no need to use XCom. However, XCom is still used behind the scenes, but all of the XCom usage passing data between tasks is abstracted away. This allows to view the XCom in the Airflow user interface as before. Belows example shows the transform function written in the traditional Airflow using XCom and highlights the simplicity of using TaskFlow. def transform(**kwargs): ti = kwargs[&quot;ti&quot;] extract_data_string = ti.xcom_pull(task_ids=&quot;extract&quot;, key=&quot;order_data&quot;) order_data = json.loads(extract_data_string) total_order_value = 0 for value in order_data.values(): total_order_value += value total_value = {&quot;total_order_value&quot;: total_order_value} total_value_json_string = json.dumps(total_value) ti.xcom_push(&quot;total_order_value&quot;, total_value_json_string) As it is clearly visible, using TaskFlow is an easy approach to workflowing in Airflow. It takes away a lot of worries when it comes to building pipelines and allows for a flexible programing experience using decorators. It allows for several more functionalities, such as reusing decorated tasks in multiple DAGs, overriding task parameters like the task_id, custom XCom backends to automatically store data in e.g. AWS S3, and using TaskGroups to group multiple tasks for a better overview in the Airflow Interface. However, even though Taskflow allows for a smooth way of developing workflows, it is beneficial to learn the traditional Airflow API to understand the fundamental concepts of how to create a workflow. The following-section will build up on that knowledge and depict a full machine learning workflow as an example. "],["exemplary-ml-workflow.html", "3.2 Exemplary ML workflow", " 3.2 Exemplary ML workflow The following shows an exemplary Airflow DAG by building a MNIST Image Classification Workflow with Keras and the Taskflow API. The DAG automates the process of training and testing a convolutional neural network (CNN) model using the popular MNIST dataset and Keras. The DAG demonstrates how to set up a simple task flow for data preprocessing, model training, and model testing. A python file of the DAG can be found here The Airflow DAG is named \"tutorial_mnist_keras_taskflow_api\" and is designed to be informative and easy to understand. It includes three distinct tasks: preprocess_data(), train_model(data_paths_dict), and test_model(model_name). These tasks collectively create a streamlined workflow for building, training, and evaluating a deep learning model. The preprocess_data() task preprocesses the MNIST dataset, which includes loading and scaling the image data and converting class labels into categorical format. The preprocessed data is then saved as NumPy arrays for subsequent use. This task returns a dictionary containing file paths to these preprocessed data arrays. The train_model(data_paths_dict) task is responsible for building and training the Keras CNN model. It starts by loading the preprocessed data using the data paths provided in the input dictionary. The CNN architecture comprises convolutional and pooling layers, followed by flattening and dense layers. After model compilation, training begins with specified hyperparameters such as batch size and epochs. Once training is complete, the model is saved with a name, and the task returns this name as output. The test_model(model_name) task loads the trained Keras model using the provided model name and evaluates its performance on the test dataset. It calculates and prints both the test loss and accuracy, providing insights into how well the model generalizes to unseen data. The DAG execution flow is straightforward. It starts with the preprocess_data() task, which prepares the data for model training. The output of this task is then passed as an input to the train_model(data_paths_dict) task, where the CNN model is constructed and trained. Finally, the test_model(model_name) task tests the trained model and reports its performance metrics. @dag( dag_id=&quot;tutorial_mnist_keras_taskflow_api&quot;, schedule=None, start_date=pendulum.datetime(2021, 1, 1, tz=&quot;UTC&quot;), catchup=False, tags=[&quot;example&quot;, &quot;mnist&quot;, &quot;keras&quot;], ) def tutorial_mnist_keras_taskflow_api(): &quot;&quot;&quot; This Airflow DAG demonstrates a simple task flow for training and testing a Keras model on the MNIST dataset. It consists of three tasks: preprocess_data, train_model, and test_model. &quot;&quot;&quot; @task() def preprocess_data(): &quot;&quot;&quot; Preprocesses the MNIST dataset, scaling it and saving the preprocessed data as NumPy arrays. Returns: dict: A dictionary containing file paths to the preprocessed data arrays. &quot;&quot;&quot; dirname = f&quot;{os.path.dirname(__file__)}/data/&quot; print(&quot;dirname: &quot;, dirname) num_classes = 10 path = f&quot;{dirname}mnist.npz&quot; (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=path) # Scale images to the [0, 1] range x_train = x_train.astype(&quot;float32&quot;) / 255 x_test = x_test.astype(&quot;float32&quot;) / 255 # Make sure images have shape (28, 28, 1) x_train = np.expand_dims(x_train, -1) x_test = np.expand_dims(x_test, -1) print(&quot;x_train shape:&quot;, x_train.shape) print(x_train.shape[0], &quot;train samples&quot;) print(x_test.shape[0], &quot;test samples&quot;) # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) y_train_path = f&quot;{dirname}y_train.npy&quot; x_train_path = f&quot;{dirname}x_train.npy&quot; y_test_path = f&quot;{dirname}y_test.npy&quot; x_test_path = f&quot;{dirname}x_test.npy&quot; np.save(y_train_path, y_train) np.save(x_train_path, x_train) np.save(y_test_path, y_test) np.save(x_test_path, x_test) data_paths_dict = { &quot;y_train_path&quot;: y_train_path, &quot;x_train_path&quot;: x_train_path, &quot;y_test_path&quot;: y_test_path, &quot;x_test_path&quot;: x_test_path, } return data_paths_dict @task(multiple_outputs=True) def train_model(data_paths_dict: dict): &quot;&quot;&quot; Trains a Keras model on the preprocessed MNIST dataset. Args: data_paths_dict (dict): A dictionary containing file paths to the preprocessed data arrays. Returns: model_data_paths_dict: A dictionary containing file paths to the preprocessed data arrays and the model name after training. &quot;&quot;&quot; dirname = f&quot;{os.path.dirname(__file__)}/data/&quot; print(&quot;dirname: &quot;, dirname) ## Load preprocessed train data y_train = np.load(data_paths_dict.get(&quot;y_train_path&quot;)) x_train = np.load(data_paths_dict.get(&quot;x_train_path&quot;)) ## Build the model input_shape = (28, 28, 1) num_classes = 10 model = keras.Sequential( [ keras.Input(shape=input_shape), layers.Conv2D(32, kernel_size=(3, 3), activation=&quot;relu&quot;), layers.MaxPooling2D(pool_size=(2, 2)), layers.Conv2D(64, kernel_size=(3, 3), activation=&quot;relu&quot;), layers.MaxPooling2D(pool_size=(2, 2)), layers.Flatten(), layers.Dropout(0.5), layers.Dense(num_classes, activation=&quot;softmax&quot;), ] ) model.summary() ## Train the model batch_size = 128 epochs = 15 model.compile( loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;] ) model.fit( x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1 ) model_path = f&quot;{dirname}model.keras&quot; model.save(model_path) model_data_paths_dict = deepcopy(data_paths_dict) model_data_paths_dict[&quot;model_name&quot;] = model_path return model_data_paths_dict @task() def test_model(model_data_paths_dict: str): &quot;&quot;&quot; Tests a trained Keras model on the test dataset and prints the test loss and accuracy. Args: model_data_paths_dict: A dictionary containing file paths to the preprocessed data arrays and the model name after training. &quot;&quot;&quot; ## Load preprocessed test data y_test = np.load(model_data_paths_dict.get(&quot;y_test_path&quot;)) x_test = np.load(model_data_paths_dict.get(&quot;x_test_path&quot;)) model = keras.models.load_model(model_data_paths_dict.get(&quot;model_name&quot;)) score = model.evaluate(x_test, y_test, verbose=0) print(&quot;Test loss:&quot;, score[0]) print(&quot;Test accuracy:&quot;, score[1]) data_paths_dict = preprocess_data() model_data_paths_dict = train_model(data_paths_dict) test_model(model_data_paths_dict) tutorial_mnist_keras_taskflow_api() "],["airflow-infrastructure.html", "3.3 Airflow infrastructure", " 3.3 Airflow infrastructure Before Data Scientists and Machine Learning Engineers can utilize the power of Airflow Workflows, Airflow obviously needs to be set up and deployed. There are multiple ways an Airflow deployment can take place. It can be run either on a single machine or in a distributed setup on a cluster of machines. As stated in the prerequisites, a local Airflow setup is on a single machine can be used for this tutorial to give an introduction on how to work with airflow. Although Airflow can be run on a single machine, it should be deployed as a distributed system to utilize its full power when working with Airflow in production. Airflow can be configured by modifying the airflow.cfg file or by using environment variables. The airflow.cfg file is a key component, as it contains various settings and parameters that control the behavior and settings of the Airflow system. Some of the common settings that can be configured include settings that define the core behavior of Airflow, as well as settings regarding the Airflow Executor, Logging, Security, or the Scheduler. All available configuration settings can seen here. The .cfg is usually located in the “conf” directory within your Airflow installation. 3.3.1 Airflow as a distributed system Airflow consists of several separate architectural components. While this separation is somewhat simulated on a local deployment, each unit of Airflow can be set up separately when deploying Airflow in a distributed manner. A distributed architecture comes with benefits of availability, security, reliability, and scalability. An Airflow deployment generally consists of five different compontens: Scheduler: The schedules handles triggering scheduled workflows and submitting tasks to the executor to run. Executor: The executor handles running the tasks themselves. In a local installation of Airflow, the tasks are run by the executor itself. In a production ready deployment of Airflow the executor pushes the task execution to separate worker instance which then runs the task. Webserver: The webserver provides the web user interface of Airflow that allows to inspect, trigger, and debug DAGs and tasks. DAG Directory: The DAG directory is a directory that contains the DAG files which are read by the scheduler and executor. Metadata Database: The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, user settings, or XCom. The following graphs shows how the components build up the Airflow architecture. Architecture of Airflow as a distributed system 3.3.2 Scheduler The scheduler is basically the brain and heart of Airflow. It handles triggering and scheduling of workflows, as well as submitting tasks to the executor to run. To be able to do this, the scheduler is responsible to parse the DAG files from the DAG directory, manage the database states in the metadata database, and to communicate with the executor to schedule tasks. Since the release of Airflow 2.0 it is possible to run multiple schedulers at a time to ensure a high availability and reliability of this centerpiece of Airflow. 3.3.3 Webserver The webserver runs the web interface of Airflow and thus the user interface every Airflow user sees. This allows to inspect, trigger, and debug DAGs and tasks in Airflow (and much more!). Each user interaction and change is written to the DAG directory or the metadata database, from where the scheduler will read and act upon. 3.3.4 Executor The executor defines where and how the Airflow tasks should be executed and run. This crucial component of Airflow can be configured by the user and should be chosen to fit the users specific needs. There are several different executors, each handling the run of a task a bit differently. Choosing the right executor also relies on the underlying infrastructure Airflow is build upon. In a production ready deployment of Airflow the executor pushes the task execution to separate worker instances that run the tasks. This allows for different setups such as a Celery-like executors or an executor based on Kubernetes. The benefit of a distributed deployment is its reliability and availability, as it is possible to have many workers in different places (for example using separate virtual machines, or multiple kubernetes pods). It is further possibility to run tasks on different instances based on their needs, for example to run the training step of a machine learning model on a GPU node. The SequentialExecutor is the default executor in Airflow. This executor only runs one task instance at a time and should therefore not used in a production use case. The LocalExecutorexecutes each task in a separate process on a single machine. It’s the only non-distributed executor which is production ready and works well in relatively small deployments. If Airflow is installed locally as mentioned in the prerequisites, this executor will be used. In contrast, the CeleryExecutor uses under the hood the Celery queue system that allows users to deploy multiple workers that read tasks from the broker queue (Redis or RabbitMQ) where tasks are sent by scheduler. This enables Airflow to distribute tasks between many machines and allows users to specify what task should be executed and where. This can be useful for routing compute-heavy tasks to more resourceful workers and is the most popular production executor. The KubernetesExecutor is another widely used production-ready executor and works similarly to the CeleryExecutor. As the name already suggests it requires an underlying Kubernetes cluster that enables Airflow to spawn a new pod to run each task. Even though this is a robust method to account for machine or pod failure, the additional overhead in creating pods or even nodes can be problematic for short running tasks. The CeleryKubernetsExecutor uses both, the CeleryExecutor and KubernetesExecutor (as the name already says). It allows to distinguish whether a particular task should be executed on kubernetes or routed to the celery workers. This way users can take full advantage of horizontal auto scaling of worker pods, and to delegate computational heavy tasks to kubernetes. The additional DebugExecutor is an executor whose main purpose is to debug DAGs locally. It’s the only executor that uses a single process to execute all tasks. Examining which executor is currently set can be done by running the following command. airflow config get-value core executor 3.3.5 DAG Directory The DAG directory contains the DAG files written in Python. The DAG directory is read and used by each Airflow component for a different purpose. The web interface lists all written DAGs from the directory as well as their content. The scheduler and executor run a DAG or a task based on the input read from the DAG directory. The DAG directory can be of different nature. It can be a local folder in case of a local installation, or also use a separate repository like Git where the DAG files are stored. The scheduler will recurse through the DAG Directory so it is also possible to create subfolders for example based on different projects. 3.3.6 Metadata Database The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, user settings, or XCom. It is beneficial to run the metadata database as a separate component to keep all data safe and secure in case there are erroneous other parts of the infrastructure. Such considerations account for the architectural decisions of an Airflow deployment and the metadata database itself. For example, it is possible to run Airflow and all of its components on a Kubernetes cluster. However, this is not necessarily recommended as it is prone to the cluster and its accessibility itself. It is also possible to outsource Airflow components such as the metadata database to use a clouds’ distinguished database resources for example. For example to store all metadata, in the Relational Database Service (RDS) on the AWS Cloud. "],["mlflow.html", "Chapter 4 MLflow", " Chapter 4 MLflow MLflow is an open source platform to manage the machine learning lifecycle end-to-end. This includes the experimentation phase of ML models, their development to be reproducible, their deployment, and the registration of a ML model to be served. MLflow provides four primary components to manage the ML lifecycle. They can be either used on their own or they also to work together. MLflow Tracking is used to log and compare model parameters, code versions, metrics, and artifacts of an ML code. Results can be stored to local files or to remote servers, and can be compared over multiple runs. MLflow Tracking comes with an API and a web interface to easily observe all logged parameters and artifacts. MLflow Models enables to manage and deploy machine learning models from multiple libraries. It allows to package your own ML model for later use in downstream tasks, e.g. real-time serving through a REST API. The package format defines a convention that saves the model in different “flavors” that can be interpreted by different downstream tools. MLflow Registry provides a central model store to collaboratively manage the full lifecycle of a MLflow Model, including model versioning, stage transitions, and annotations. It comes with an API and user interface for easy use of such funtionalities and each of those aspects can be checked in MLflows’ web interface. MLflow Projects packages data science code in a standard format for a reusable, and reproducible form to share your code with other data scientists or transfer it to production. A project might be a local directory or a Git repository which uses a descriptor file to specify its dependencies and entrypoints. An existing MLflow Project can be also run either locally or from a Git repository. MLflow is library-agnostic, which means one can use it with any ML library and programming language. All functions are accessible through a REST API and CLI, but quite conveniently the project comes with a Python API, R API, and a Java API already included. It is even possible to define your own plugins. The aim is to make its use as reproducible and reusable as possible so Data Scientists require minimal changes to integrate MLflow into their existing codebase. MLflow also comes with a user web interface to conveniently view and compare models and metrics. Web Interface of MLflow Prerequisites To go through this chapter it is necessary to have Python and MLflow installed. One can install MLflow locally via pip install MLflow. This tutorial is based on MLflow v2.1.1. It is also recommended to have knowledge of VirtualEnv, Conda, or Docker when working with MLflow Projects. "],["core-components-1.html", "4.1 Core Components", " 4.1 Core Components The four primary components of MLflow are shown in more detail and with exemplary code in the following sections. 4.1.1 MLflow Tracking MLflow Tracking allows to log and compare parameters, code versions, metrics, and artifacts of a machine learning model. This can be easily done by minimal changes to your code using the MLflow Tracking API. The following examples depict the basic concepts and show how to use it. To use MLflow within your code it needs to be imported first. import mlflow 4.1.1.1 MLflow experiment MLflow experiments are a part of MLflow’s tracking component that allow to group runs together based on custom criteria. For example multiple model runs with different model architectures might be grouped within one experiment to make it easier for evaluation. experiment_name = &quot;introduction-experiment&quot; mlflow.set_experiment(experiment_name) 4.1.1.2 MLflow run An MLflow run is an execution environment for a piece of machine learning code. Whenever parameters or performances of a ML run or experiment should be tracked, a new MLflow run is created. This is easily done using MLflow.start_run(). Using MLflow.end_run() the run will similarly be ended. run_name = &quot;example-run&quot; mlflow.start_run() run = mlflow.active_run() print(f&quot;Active run_id: {run.info.run_id}&quot;) mlflow.end_run() It is a good practice to pass a run name to the MLflow run to identify it easily afterwards. It is also possible to use the context manager as shown below, which allows for a smoother style. run_name = &quot;context-manager-run&quot; with mlflow.start_run(run_name=run_name) as run: run_id = run.info.run_id print(f&quot;Active run_id: {run_id}&quot;) Child runs It is possible to create child runs of the current run, based on the run ID. This can be used for example to gain a better overview of multiple run. Belows code shows how to create a child run. # Create child runs based on the run ID with mlflow.start_run(run_id=run_id) as parent_run: print(&quot;parent run_id: {}&quot;.format(parent_run.info.run_id)) with mlflow.start_run(nested=True, run_name=&quot;test_dataset_abc.csv&quot;) as child_run: mlflow.log_metric(&quot;acc&quot;, 0.91) print(&quot;child run_id : {}&quot;.format(child_run.info.run_id)) with mlflow.start_run(run_id=run_id) as parent_run: print(&quot;parent run_id: {}&quot;.format(parent_run.info.run_id)) with mlflow.start_run(nested=True, run_name=&quot;test_dataset_xyz.csv&quot;) as child_run: mlflow.log_metric(&quot;acc&quot;, 0.90) print(&quot;child run_id : {}&quot;.format(child_run.info.run_id)) 4.1.1.3 Logging metrics &amp; parameters The main reason to use MLflow Tracking is to log and store parameters and metrics during a MLflow run. Parameters represent the input parameters used for training, e.g. the initial learning rate. Metrics are used to track the progress of the model training and are usually updated over the course of a model run. MLflow allows to keep track of the model’s train and validation losses and to visualize their development across the training run. Parameters and metrics can be easily logged by calling MLflow.log_param and MLflow.log_metric. One can also specify a tag to identify the run by using MLflow.set_tag. Belows example show how to use each method within a run. run_name = &quot;tracking-example-run&quot; experiment_name = &quot;tracking-experiment&quot; mlflow.set_experiment(experiment_name) with mlflow.start_run(run_name=run_name) as run: # Parameters mlflow.log_param(&quot;learning_rate&quot;, 0.01) mlflow.log_params({&quot;epochs&quot;: 0.05, &quot;final_activation&quot;: &quot;sigmoid&quot;}) # Tags mlflow.set_tag(&quot;env&quot;, &quot;dev&quot;) mlflow.set_tags({&quot;some_tag&quot;: False, &quot;project&quot;: &quot;xyz&quot;}) # Metrics mlflow.log_metric(&quot;loss&quot;, 0.001) mlflow.log_metrics({&quot;acc&quot;: 0.92, &quot;auc&quot;: 0.90}) # It is possible to log a metrics series (for example a training history) for val_loss in [0.1, 0.01, 0.001, 0.00001]: mlflow.log_metric(&quot;val_loss&quot;, val_loss) for val_acc in [0.6, 0.6, 0.8, 0.9]: mlflow.log_metric(&quot;val_acc&quot;, val_acc) run_id = run.info.run_id experiment_id = run.info.experiment_id print(f&quot;run_id: {run_id}&quot;) print(f&quot;experiment_id: {experiment_id}&quot;) It is also possible to add information after the experiment ran. One just needs to specify the run ID from the previous run to the current run. The example below shows how to do this, and uses the mlflow.client.MLflowClient. The mlflow.client module provides a Python CRUD interface, which is a lower level API directly translating to the MLflow REST API calls. It can be used similarly to the mlflow-module of the higher level API. It is mentioned here to give a hint of its existence. from mlflow.tracking import MLflowClient # add a note to the experiment MLflowClient().set_experiment_tag( experiment_id, &quot;MLflow.note.content&quot;, &quot;my experiment note&quot;) # add a note to the run MLflowClient().set_tag(run_id, &quot;MLflow.note.content&quot;, &quot;my run note&quot;) # Or we can even log further metrics by calling MLflow.start_run on a specific ID with mlflow.start_run(run_id=run_id): run = mlflow.active_run() mlflow.log_metric(&quot;f1&quot;, 0.9) print(f&quot;run_id: {run.info.run_id}&quot;) 4.1.1.4 Display &amp; View metrics How can the logged parameters and metrics be used and viewed afterwards? It is possible to give an overview of the currently stored runs using the MLflow API and printing the results. current_experiment = dict(mlflow.get_experiment_by_name(experiment_name)) mlflow_run = mlflow.search_runs([current_experiment[&#39;experiment_id&#39;]]) print(f&quot;MLflow_run: {mlflow_run}&quot;) MLflow Model Tracking CLI Run Overview Yet, viewing all the results in the web interface of MLflow gives a much better overview. By default, the tracking API writes the data to the local filesystem of the machine it’s running on under a ./mlruns directory. This directory can be accessed by the MLflow’s Tracking web interface by running MLflow ui via the command line. The web interface can be viewed in the browser under http://localhost:5000 (The port: 5000 is the MLflow default). The metrics dashboard of a run looks like the following: MLflow Model Tracking Dashboard It is also possible to configure MLflow to log to a remote tracking server. This allows to manage results on in a central place and share them across a team. To get access to a remote tracking server it is needed to set a MLflow tracking URI. This can be done multiple way. Either by setting an environment variable MLflow_TRACKING_URI to the servers URI, or by adding it to the start of our code. import mlflow mlflow.set_tracking_uri(&quot;http://YOUR-SERVER:YOUR-PORT&quot;) mlflow.set_experiment(&quot;my-experiment&quot;) 4.1.1.5 Logging artifacts Artifacts represent any kind of file to save during training, such as plots and model weights. It is possible to log such files as well, and place them within the same run as parameters and metrics. This means everything created within a ML run is saved at one point. Artifact files can be either single local files, or even full directories. The following example creates a local file and logs it to a model run. import os mlflow.set_tracking_uri(&quot;http://127.0.0.1:5000/&quot;) # Create an example file output/test.txt file_path = &quot;outputs/test.txt&quot; if not os.path.exists(&quot;outputs&quot;): os.makedirs(&quot;outputs&quot;) with open(file_path, &quot;w&quot;) as f: f.write(&quot;hello world!&quot;) # Start the run based on the run ID and log the artifact # we just created with mlflow.start_run(run_id=run_id) as run: mlflow.log_artifact( local_path=file_path, # store the artifact directly in run&#39;s root artifact_path=None ) mlflow.log_artifact( local_path=file_path, # store the artifact in a specific directory artifact_path=&quot;data/subfolder&quot; ) # get and print the URI where the artifacts have been logged to artifact_uri = mlflow.get_artifact_uri() print(f&quot;run_id: {run.info.run_id}&quot;) print(f&quot;Artifact uri: {artifact_uri}&quot;) 4.1.1.6 Autolog Previously, all the parameters, metrics, and files have been logged manually by the user. The autolog-feature of MLflow allows for automatic logging of metrics, parameters, and models without the need for an explicit log statements. This feature needs to be activated previous to the execution of a run by calling MLflow.sklearn.autolog(). import mlflow.sklearn import numpy as np from sklearn.ensemble import RandomForestRegressor params = {&quot;n_estimators&quot;: 4, &quot;random_state&quot;: 42} mlflow.sklearn.autolog() run_name = &#39;autologging model example&#39; with mlflow.start_run(run_name=run_name) as run: rfr = RandomForestRegressor( **params).fit(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]), [1, 1, 1]) print(f&quot;run_id: {run.info.run_id}&quot;) mlflow.sklearn.autolog(disable=True) Even though this is a very convenient feature, it is a good practice to log metrics manually, as this gives more control over a ML run. 4.1.2 MLflow Models MLflow Models manages and deploys models from various different ML libraries such as scikit-learn, TensorFlow, PyTorch, Spark, and many more. It includes a generic MLmodel format that acts as a standard format to package ML models so they can be used in different projects and environments. The MLmodel format defines a convention that saves the model in so called “flavors”. For example mlflow.sklearn allows to load mlflow models back into scikit-learn. The stored model can also be served easily and conveniently using these flavors as a python function either locally, in Docker-based REST servers containers, or on commercial serving platforms like AWS SageMaker or AzureML. The following example is based on the scikit-learn library. # Import the sklearn models from MLflow import mlflow.sklearn from sklearn.ensemble import RandomForestRegressor mlflow.set_tracking_uri(&quot;http://127.0.0.1:5000/&quot;) run_name = &quot;models-example-run&quot; params = {&quot;n_estimators&quot;: 4, &quot;random_state&quot;: 42} # Start an MLflow run, train the RandomForestRegressor example model, and # log its parameeters. In the end the model itself is logged and stored in MLflow run_name = &#39;Model example&#39; with mlflow.start_run(run_name=run_name) as run: rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1]) mlflow.log_params(params) mlflow.sklearn.log_model(rfr, artifact_path=&quot;sklearn-model&quot;) model_uri = &quot;runs:/{}/sklearn-model&quot;.format(run.info.run_id) model_name = f&quot;RandomForestRegressionModel&quot; print(f&quot;model_uri: {model_uri}&quot;) print(f&quot;model_name: {model_name}&quot;) Once a model is stored in the correct format it can be identified by its model_uri, loaded, and used for prediction. import mlflow.pyfunc # Load the model and use it for predictions model = mlflow.pyfunc.load_model(model_uri=model_uri) data = [[0, 1, 0]] model_pred = model.predict(data) print(f&quot;model_pred: {model_pred}&quot;) MLflow Models 4.1.3 MLflow Model Registry The MLflow Model Registry provides a central model store to manage the lifecycle of an ML Model. This allows to register MLflow models like the RandomForestRegressor from the previous section to the Model Registry and include model versioning, stage transitions, and annotations. In fact, by running MLflow.sklearn.log_model we already did exactly that. Look at how easy the MLflow API is to use. Let’s have a look at the code again. import mlflow.sklearn import mlflow.pyfunc from sklearn.ensemble import RandomForestRegressor mlflow.set_tracking_uri(&quot;http://127.0.0.1:5000/&quot;) run_name = &quot;registry-example-run&quot; params = {&quot;n_estimators&quot;: 4, &quot;random_state&quot;: 42} run_name = &#39;model registry example&#39; with mlflow.start_run(run_name=run_name) as run: rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1]) mlflow.log_params(params) # Log and store the model and the MLflow Model Registry mlflow.sklearn.log_model(rfr, artifact_path=&quot;sklearn-model&quot;) model_uri = f&quot;runs:/{run.info.run_id}/sklearn-model&quot; model_name = f&quot;RandomForestRegressionModel&quot; model = mlflow.pyfunc.load_model(model_uri=model_uri) data = [[0, 1, 0]] model_pred = model.predict(data) print(f&quot;model_pred: {model_pred}&quot;) Yet, it is also possible to register the MLflow model in the model registry by calling MLflow.register_model such as show in belows example. # The previously stated Model URI and name are needed to register a MLflow Model mv = mlflow.register_model(model_uri, model_name) print(&quot;Name: {}&quot;.format(mv.name)) print(&quot;Version: {}&quot;.format(mv.version)) print(&quot;Stage: {}&quot;.format(mv.current_stage)) Once registered to the model registry the model is versioned. This enables to load a model based on a specific version and to change a model version respectively. A registered model can be also modified to transition to another version or stage. Both use cases are shown in the example below. import mlflow.pyfunc # Load model for prediction. Keep note that we now specified the model version. model = mlflow.pyfunc.load_model( model_uri=f&quot;models:/{model_name}/{mv.version}&quot; ) # Predict based on the loaded model data = [[0, 1, 0]] model_pred = model.predict(data) print(f&quot;model_pred: {model_pred}&quot;) Let’s stage a model to 'Staging'. The for-loop below prints all registered models and shows that there is indeed a model with a 'Staging'-stage. # Transition the model to another stage from mlflow.client import MLflowClient client = MlflowClient() stage = &#39;Staging&#39; # None, Production client.transition_model_version_stage( name=model_name, version=mv.version, stage=stage ) # print registered models for rm in client.search_registered_models(): pprint(dict(rm), indent=4) 4.1.4 MLflow Projects MLflow Projects allows to package code and its dependencies as a project that can be run reproducible on other data. Each project includes a MLproject file written in the YAML syntax that defines the projects dependencies, and the commands and arguments it takes to run the project. It basically is a convention to organizes and describe the model code so other data scientists or automated tools can run it conveniently. MLflow currently supports four environments to run your code: Virtualenv, Conda, Docker Container, and system environment. A very basic MLproject file is shown below that is run in an Virtualenv name: mlprojects_tutorial # Use Virtualenv: alternatively conda_env, docker_env.image python_env: &lt;MLFLOW_PROJECT_DIRECTORY&gt;/python_env.yaml entry_points: main: parameters: alpha: {type: float, default: 0.5} l1_ratio: {type: float, default: 0.1} command: &quot;python wine_model.py {alpha} {l1_ratio}&quot; A project is run using the MLflow run command in the command line. It can run a project from either a local directory or a GitHub URI. The MLproject file shows that two parameters are passed to the MLflow run command. This is optional in this case as they have default values. It is also possible to specify extra parameters such as the experiment name or to specify the tracking uri (check the official documentation for more). Below is a possible CLI command show to run the MLflow Project. By setting the MLFLOW_TRACKING_URI environment variable it is possible to also specify an execution backend for the run. # Run the MLflow project from the current directory # The parameters are optional in this case as the MLproject file has defaults mlflow run . -P alpha=5.0 # It is also possible to specify an experiment name or to specify the # Tracking_URI, e.g. MLFLOW_TRACKING_URI=http://localhost:&lt;PORT&gt; mlflow run . --experiment-name=&quot;models-experiment&quot; # Run the MLflow project from a Github URI and use the localhost as backend MLFLOW_TRACKING_URI=http://localhost:&lt;PORT&gt; MLflow run https://github.com/mlflow/mlflow-example/MLprojects --version=chapter/mlflow The MLflow Projects API allows to chain projects together into workflows and also supports launching multiple runs in parallel. Combining this with for example the MLflow Tracking API enables an easy way of hyperparameter tuning to develop a model with a good fit. "],["mlfflow-architecture.html", "4.2 MLFflow Architecture", " 4.2 MLFflow Architecture While MLflow can be run locally for your personal model implementation, it is usually deployed on a distributed architecture for large organizations or teams. The MLflow backend consists of three different main components, Tracking Server, Backend Store, and Artifact Store, all of which can reside on remote hosts. The MLflow client can interface with a variety of backend and artifact storage configurations. The official MLflow documentation outlines several detailed configurations. The example below depicts the main interaction between the different architectural components of a remote MLflow Tracking Server, a Postgres database for backend storage, and an S3 bucket for artifact storage. MLflow Architecture Diagram 4.2.1 MLflow Tracking Server The MLflow Tracking Server is the main component that handles the communication between the REST API to log parameters, metrics, experiments and metadata to a storage solution. The server uses both, the backend store and the artifact store to store and read data from. Although it is possible to track parameters without running a server (e.g. locally), it is recommended to create a MLflow tracking server to log your data to. Some of the functionality of the API is also available via the web interface, for example to create an experiment. Further, the Tracking web user interface allows to view runs easily in the web browser. Running the CLI command mlflow ui starts a web server on your local machine serving the MLFlow user interface. Alternatively, a remote MLflow tracking server serves the same user interface which can be accessed using the server’s URL http://&lt;TRACKING-SERVER-IP-ADDRESS&gt;:5000 from any machine that can connect to the tracking server. 4.2.2 MLflow Backend Store The MLflow Backend Store is where MLflow stores experiment and run data like parameters, and metrics. It is usually a relational database which means that all metadata will be stored, but no large data files. MLflow supports two types of backend stores: file store and database-backed store. By default, the backend store is set to the local file store backend at the ./mlruns directory. A database-backed store must be configures using the --backend-store-uri. MLflow supports encoded Databases like mysql, mssql, sqlite, and postgresql, and it is possible to use a variety of externally hosted metadata stores like Azure MySQL, or AWS RDS. To be able to use the MLflow Model Registry the server must use a database-backed store. 4.2.3 MLflow Artifact Store In addition to the Backend Store the Artifact Store is another storage place for the MLflow tracking server. It is the location to store large data of an ML run that are not suitable for the Backend Store or a relational database respectively. This is where MLflow users log their artifact outputs, or data and image files to. The user can access these artifacts via HTTP requests to the MLflow Tracking Server. The location to the server’s artifact store defaults to local ./mlruns directory. It is possible to specify another artifact store server using --default-artifact-root. The MLflow client caches the artifact location information on a per-run basis. It is therefore not recommended to alter a run’s artifact location before it has terminated. The Artifact Store needs to be configured when running MLflow on a distributed system. In addition to local file paths, MLflow supports to configure the following cloud storage resources as an artifact stores: Amazon S3, Azure Blob Storage, Google Cloud Storage, SFTP server, and NFS. "],["kubernetes.html", "Chapter 5 Kubernetes", " Chapter 5 Kubernetes Kubernetes (short: K8s) is greek and means pilot. K8s is an applications orchestrator that originated from Google and is open source. Beeing an application orchestrator, K8s deploys and manages application containers. It scales up and down so called Pod (A Pod manages a container) as needed and allows for zero downtime as well as the possibility of rollbacks. Thus, the meaning of pilot relates to its functionining in piloting containers. Prerequisites As a prerequisites to go through this tutorial and implement the scripts one has to have Docker installed, Kubectl to interact via the command line with K8s, as well as Minikube to run a K8s cluster on a local machine. Please refer to the corresponding sites. All configuration files within this tutorial can be found in the github repository of this book. "],["core-components-2.html", "5.1 Core Components", " 5.1 Core Components 5.1.1 Nodes A K8s Cluster usually consistes of a set of nodes. A Node can hereby be a virtual machine (VM) in the cloud, e.g. AWS, Azure, or GCP, or a node can also be of course a physical on-premise instance. K8s distinguishes the nodes between a master node and worker nodes. The master node is basically the brain of the cluster. This is where everything is organized, handled, and managed. In comparison, a worker nodes is where the heavy lifting is happening, such as running application. Both, master and worker nodes communicate with each other via the so called kubelet. One cluster has only one master node and usually multiple worker nodes. K8s Cluster 5.1.1.1 Master &amp; Control Plane To be able to work as the brain of the cluster, the master node contains a controll plane made of several components, each of which serves a different function. Scheduler Cluster Store API Server Controller Manager Cloud Controller Manager Master Node 5.1.1.1.1 API Server The api servers serves as the connection between the frontend and the K8s controll plane. All communications, external and interal, go through it. Frontend to Kubernetes Controll Plane. It exposes a restful api on port 443 to allow communication, as well as performes authentication and authorization checks. Whenever we perform something on the K8s cluster, e.g. using a command like kubectl apply -f &lt;file.yaml&gt;, we communicate with the api server (what we do here is shown in the section about pods). 5.1.1.1.2 Cluster store The cluster store stores the configuration and state of the entire cluster. It is a distributed key-value data store and the single source of truth database of the cluster. As in the example before, whenever we apply a command like kubectl apply -f &lt;file.yaml&gt;, the file is stored on the cluster store to store the configuration. 5.1.1.1.3 Scheduler The scheduler of the control plane watches for new workloads/pods and assigns them to a node based on several scheduling factors. These factors include whether a node is healthy, whether there are enough resources available, whether the port is available, or according to affinity or anti-affinity rules. 5.1.1.1.4 Controller manager The controller manager is a daemon that manages the control loop. This means, the controller manager is basically a controller of other controllers. Each controller watches the api server for changes to their state. Whenever a current state of a controller does not match the desired state, the control manager administers the changes. These controllers are for example replicasets, endpoints, namespace, or service accounts. There is also the cloud controller manager, which is responsible to interact with the underlying cloud infrastructure. 5.1.1.2 Worker Nodes There worker nodes are the part of the cluster where the heavy lifting happens. Their VMs (or physical machines) often run linux and thus provide a suitable and running environment for each application. Worker Node A worker node consists of three main components. Kubelet Container runtime Kube proxy 5.1.1.2.1 Kubelet The kubelet is the main agent of a worker node that runs on every single node. It receives pod definitions from the API server and interacts with the container runtime to run containers associated with the corresponding pods. The kubelet also reports node and pod state to the master node. 5.1.1.2.2 Container runtime The container runtime is responsible to pull images from container registries, e.g. from DockerHub, or AWS ECR, as well as starting, and stoping containers. The container runtime thus abstracts container management for K8s and runs a Container Runtime Interface (CRI) within. 5.1.1.2.3 Kube-proxy The kube-proxy runs on every node via a DaemondSet. It is responsible for network communications by maintaining network rules to allow communication to pods from inside and outside the cluster. If two pods want to talk to each other, the kube-proxy handles their communication. Each node of the cluster gets its own unique IP adress. The kube-proxy handels the local cluster networking as well as routing the network traffic to a load balanced service. 5.1.2 Pods A pod is the smallest deployable unit in K8s (In contrast to K8s, the smallest deployable unit for docker are containers.). Therefore, a pod is a running process that runs on a clusters’ node. Within a pod, there is always one main container representing the application (in whatever language written, e.g. JS, Python, Go). There also may or may not be init containers, and/or side containers. Init containers are containers that are executed before the main container. Side containers are containers that support the main containers, e.g. a container that acts as a proxy to your main container. There may be volumes specified within a pod, which enables containers to share and store data. Pod The containers running within a pod communicate with each other using localhost and whatever port they expose. The port itself has a unique ip adress, which enables outward communication between pods. The problem is that a pods does not have a long lifetime (also denoted as ephemeral) and is disposable. This suggests to never create a pod on its own within a K8s cluster and to rather use controllers instead to deploy and maintain a pods lifecycle, e.g. controllers like Deployments. In general, managing ressources in K8s is done via an imperative or declarative management. 5.1.3 Imperative &amp; Declarative Management Imperative management means managing the pods via a CLI and specifying all necessary parameters using it. It is good for learning, troubleshooting, and experimenting on the cluster. In contrast, the declarative approach uses a yaml file to state all necessary parameters needed for a ressource, and then using the CLI to administer the changes. The declarative approach is reproducible, which means the same configuration can be applied in different environments (prod/dev). This is best practice to use when building a cluster. As stated, this differentiation does not only hold for pods, but for all ressources within a cluster. 5.1.3.1 Imperative Management # start a pod by specifying the pods name, # the container image to run, and the port exposed kubectl run &lt;pod-name&gt; --image=&quot;&lt;image-name&gt;&quot; --port=80 # run following command to test the pod specified # It will forward to localhost:8080 kubectl port-forward pod/&lt;pod-name&gt; 8080:80 5.1.3.2 Declarative Management / Configuration Declarative configuration is done using a yaml format, which works on key-value pairs. # pod.yaml apiVersion: v1 # specify which kind of configuration this is # lets configure a simple pod kind: Pod # metadata will be explained later on in more detail metadata: name: hello-world labels: name: hello-world spec: # remember: a pod is a selection of one or more containers # we could therefore specify multiple containers containers: # specify the container name - name: hello # specify which container image should be pulled image: seblum/mlops-public:cat-v1 # ressource configurations will be handled later as well ressources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # specify the port on which the container should be exposed # similar to the imperative approach ports: ContainerPorts: 80 Appyl this declarative configuration using the following kubectl command via the CLI. kubectl apply -f &quot;file-name.yaml&quot; # similar to before, run following to test your pod on localhost:8080 kubectl port-forward pod/&lt;pod-name&gt; 8080:80 5.1.3.3 Kubectl One word to interacting with the cluster using the CLI. In general, kubectl is used to interact with the K8s cluster. This allows to run and apply pod configurations such as seen before, as well as the already shown port forwarding. We can also inspect the cluster, see what ressources are running on which nodes, see their configurations, and watch their logs. A small selection of commands are shown below. # forward the pods to localhost:8080 kubectl port-forward &lt;ressource&gt;/&lt;pod-name&gt; 8080:80 # show all pods currently running in the cluster kubectl get pods # delete a specific pod kubectl delete pods &lt;pod-name&gt; # delete a previously applied configuration kubectl delete -f &lt;file-name&gt; # show all instances of a specific resource running on the cluster # (nodes, pods, deployments, statefulsets, etc) kubectl get &lt;resource&gt; # describe and show specific settings of a pods kubectl describe pod &lt;pod-name&gt; "],["application-deployment-design.html", "5.2 Application Deployment &amp; Design", " 5.2 Application Deployment &amp; Design 5.2.1 Deployments We should never deploy a pod using kind:Pod. Pods are ephemeral, so never treat them like pets. They do not heal on their own and if a pod is terminated, it does not restart by themselves. This is dangerous as there should always be one replica running of and application. This demands for a mechanism for the application to self heal and this is exactly where Deployments and ReplicaSets come in to solve the problem. In general, Pods should be managed through Deployments. The purpose of a Deployment is to facilitate software deployment. They manage releases of a new application, they provide zero downtime of an application and create a ReplicaSet behind the scenes. K8s will take care of the full deployment process when applying a Deployment, even if we want to make a rolling update to change the version. 5.2.1.1 ReplicaSets A ReplicaSet makes sure that a desired number of pods is running. When looking at Pods’ name of a Deployment, it usually has a random string attached. This is because a deployment can have multiple replicas and the random suffix ensures a different name after all. The way ReplicaSets work is that they implement a background control loop that checks the desired number of pods are always present on the cluster. We can specify the number of replicas by creating a yaml-file of a Deployment, similar to previous specifications done to a Pod. As a reminder, the Deployment can be applied using the kubectl apply -f as well. # deployment.yaml apiVersion: apps/v1 # specify that we want a deployment kind: Deployment metadata: name: hello-world spec: # specify number of replicas replicas: 3 selector: matchLabels: app: hello-world template: metadata: labels: app: hello-world spec: containers: - name: hello-world image: seblum/mlops-public:cat-v1 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 5.2.1.2 Rolling updates A rolling update means that a new version of the application is rolled out. In general, a basic deployments strategy will delete every single pod before it creates a new version. This is very dangerous since there is downtime. The preferred strategy is to perform a rolling update. This ensures keeping traffic to the previous version until the new one is up and running and alternates traffic until the new version is fully healthy. K8s perfoms the update of an application while the application is up and running. For example, when there are two replicasets running, one with version v1 and one with v2, K8s performs the update such that it only scales v1 down when v2 is already up and running and the traffic has been redirected to v2 as well. How do the deployments need to be configured for that? # deployment_rolling-update.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: replicas: 3 # a few new things have been added here revisionHistoryLimit: 20 # specify the deployments strategy strategy: type: RollingUpdate rollingUpdate: # only one pod at a time to become unavailable # in our case scaling down of v1 maxUnavailable: 1 # never have more than one pod above the mentioned replicas # with three replicas, there will never be 5 pods running during a rollout maxSurge: 1 selector: matchLabels: app: hello-world template: metadata: labels: app: hello-world annotations: # just an annotation the get the version change kubernetes.io/change-cause: &quot;seblum/mlops-public:cat-v2&quot; spec: containers: - name: hello-world # only change specification of the image to v2, k8s performs the update itself image: seblum/mlops-public:cat-v2 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 The changes can be applied as well using kubectl apply -f \"file-name.yaml\". Good to know, K8s is not deleting the replicasets of previous versions. They are still stored on the Cluster Store. The spec.revisionHistory: &lt;?&gt; state in the yaml denoted this. The last ten previous versions are stored on default. However, it doesn’t really make sense to keep more such for example in the previous yaml where there are the last 20 versions specified. This enables to perform Rollbacks to previous versions. To not have discrepancies in a cluster, one should always update using the declarative approach. Below stated are a number of commands that trigger and help with a rollback or with rollouts in general. # check the status of the current deployment process kubectl rollout status deployments &lt;name&gt; # pause the rollout of the deployment. kubectl rollout pause # check the rollout history of a specific deployment kubectl rollout history deployment &lt;name&gt; # undo the rollout of a deployment and switch to previous version kubectl rollout undo deployment &lt;name&gt; # goes back to a specific revision # there is a limit of history and k8s only keeps 10 previous versions kubectl rollout undo deployment &lt;name&gt; --to-revision= 5.2.2 Resource Management Besides the importance of a healthy application itself, there should be also enough resources allocated so the application can perform well, e.g. memory &amp; CPU. Yet, it should also only consume the resources needed and not block unneeded ones. It might be dangerous, as one application using a lot of ressources, leaving nothing left for other applications and eventually starving them. To prevent this from happening in K8s. there can be a minimum amount of resources defined a container needs (request) as well as the maximum amount of resources a container can have (limit). Configuring limits and requests for a container can be done within the spec for a Pod or Deployment. Actually, we have been using them all the time previously. # resource-management.yaml apiVersion: apps/v1 # specify that we want a deployment kind: Deployment metadata: name: rm-deployment spec: # specify number of replicas replicas: 3 selector: matchLabels: app: rm-deployment template: metadata: labels: app: rm-deployment spec: containers: - name: rm-deployment image: seblum/mlops-public:cat-v1 requests: memory: &quot;512Mi&quot; cpu: &quot;1000m&quot; # Limits limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 5.2.3 DaemonSets The master node of K8s decides on what worker nodes a pod is scheduled or not. However, there are times where we want to have a copy of a pod across the cluster. A DaemonSet ensures a copy of the specified Pod is exactly doing this. This can be useful for example to deploy system daemons such as log collectors and monitoring agents. DaemonSets are automatically deployed on every single node, unless specified on which node to run. They therefore do not need a specification of nodes and can scale up and down with the cluster as needed. They will automatically scheduled a pod on each new node. The given example deploys a DaemonSet to cover logging using K8s FluendID. # daemonsets.yaml apiVersion: v1 kind: Namespace metadata: name: logging --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: logging labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can&#39;t run pods - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule # specify the containers as done in Pods or Deployments volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers containers: - name: fluentd-elasticsearch # allows to collect logs from nodes image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 5.2.4 StatefulSets StatefulSets are used to deploy and manage stateful applications. Stateful applications are applications which are long lived, for example databases. Most applications of K8s are stateless as they only run for a specific task, like Deployments or Pods. However, a database is a state of truth and should be present at all time. StatefulSets manage the pods based on the same container specifications such as Deployments. However, unlike a Deployment, a StatefulSet ensures that each of its Pods retains a distinct identity, maintaining a persistent identifier through potential rescheduling. Lets assume we have a StatefulSet with 3 replicas that is exposed by a headless service named ngnix-service. Each Pod has a PV attached. apiVersion: v1 kind: Service metadata: name: nginx-service labels: app: nginx spec: ports: - port: 80 name: web-statefulset clusterIP: None selector: app: nginx-service --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web-statefulset spec: serviceName: &quot;nginx-service&quot; replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry.k8s.io/nginx-slim:0.8 ports: - containerPort: 80 name: web-statefulset volumeMounts: - name: data mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: data spec: accessModes: [ &quot;ReadWriteOnce&quot; ] # storageClassName: storage-class-name resources: requests: storage: 1Gi 5.2.5 Jobs &amp; Cron Jobs Using the busybox image in the section about volumes we will experience that the image is very short lived. K8s is not aware of this and runs into a CrashLoopBackOff-Error. K8s will try and restart the container itself though until it BackOffs completley. Because the image is so short live, a job within the image has to be executed such as done with a shell command previously. However, what if we have a simple task that only should run like every 5 minutes, or every single day? A good idea is to use CronJobs for such tasks that start the image if needed. When comparingJobs jobs and CronJobs, jobs execute only once, whereas CronJobs execute depending on an specified expression. The following job simulates a backup to a database that runs 30 seconds in total. The part in the args specifies that the container will sleep for 20 seconds (the hypothetical backup). Afterward, the container will wait 10 seconds to shut down, as specified in ttlSecondsAfterFinished. # job.yaml apiVersion: batch/v1 kind: Job metadata: name: db-backup-job spec: # time it takes to terminate the job for one completion ttlSecondsAfterFinished: 10 template: spec: containers: - name: backup image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: - &quot;echo &#39;performing db backup...&#39; &amp;&amp; sleep 20&quot; restartPolicy: Never The CronJob below runs run every minute. Given the structure of ( * * * * * * ) - ( Minutes Hours Day-of-month Month Day-of-week Year), the cronjob expression defines as follows: # cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: db-backup-cron-job spec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: backup image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: - &quot;echo &#39;performing db backup...&#39; &amp;&amp; sleep 20&quot; restartPolicy: Never "],["services-and-networking.html", "5.3 Services and Networking", " 5.3 Services and Networking 5.3.1 Services To overall question when deploying an application is how we can access it. Each individual Pod has its own IP address. Thereby, the client can access this Pod via its IP address. Previously, we have made the app available via kubectl port-forward by forwarding the Pods’ IP to localhost. However, should only be done for testing purposes and is not a reliable and stable way to enable access. Since Pods are ephemeral, the client cannot rely on the ip address alone. For example, if an application is scaled up or down, there will be new IPs associated with each new Pod. Services Instead, Services should be used. A service is an abstract way to expose an application as a network service. The service can connect access to a pod via an interal reference, so a change of the Pods IP will not interfere with its accessibility. The service itself has a stable IP adress, a stable DNS name, and a stable port. This allows for a reliable and stable connection from the client to the service, which can then direct the traffic to the pod. There are different types of Services. ClusterIP (Default) NodePort ExternalName LoadBalancer 5.3.1.1 ClusterIP The ClusterIP is the default K8s service. This service type will be chosen if no specific type is selected. The ClusterIP is used for cluster internal access and does not allow for external communication. If one Pod wants to talk to another Pod inside the cluster, it will use ClusterIP to do so. The service will allow and send traffic to any pod that is healthy. 5.3.1.2 NodePort The NodePort service allows to open a static port simultaneously on all nodes. Its range lies between between 30.000/32.767. If a client wants to communicate with a node of the cluster, the client directly communicates with the node via its IP address. When the request reaches the port of the node, the NodePort service handles the request and forwards it to the specifically marked pod. This way, an application running on a pod can be exposed directly on a nodes’ IP under a specific port. You’ll be able to contact the NodePort Service from outside the cluster by requesting &lt;NodeIP&gt;:&lt;NodePort&gt;. Using a NodePort is beneficial for example when a request is sent to a node without a pod. The NodePort service will forward the request to a node which has a healthy associated pod running. However, only having one service specified per port is also a disadvantage. Having one ingress and multiple services is more desireable. The point of running K8s in the cloud is to scale up and down and if the NodeIP address changes, then we have a problem. So we should not aim to access a Node IP directly in the first place If applying below example using the frontend-deployment and the backend-deployment, we can access the frontend using the nodeport. Since using minikube, we can access the service by using minikube service frontend-node --url. Using the given IP adress it is possible to access the frontend using the NodePort service. We can also test the NodePort service when inside of a node. When accessing a node e.g. via minikube ssh, we can run curl localhost:PORT/ inside the node to derive the to derive the website data from the frontend. 5.3.1.3 LoadBalancer Loadbalancers are a standard way of exposing applications to the extern, for example the internet. Loadbalancers automatically distribute incoming traffic across multiple targets to balance the load in an equal level. If K8s is running on the cloud, e.g. AWS or GCP, a Network Load Balancer (NLB) is created. The Cloud Controller Manager (remember the Controller Manager of a Node) is resposible to talk to the underlying cloud provier. In Minikube, the external IP to access the application via the LoadBalancer can be exposed using the command minikube tunnel. 5.3.1.4 default kubernetes services There are also default K8s services created automatically to access K8s with the K8s API. Check the endpoints of the kubernetes service and the endpoints of the api-service pod within kube-system namespace. They should be the same. It is also possible to show all endpoints of the cluster using kubectl get endpoints. 5.3.1.5 Exemplary setup of database and frontend microservices The following example show the deployment and linking of two different deployments. A frontend-deployment.yaml that pulls a container running a Streamlit App, and a database-deployment.yaml that runs a flask application exposing a dictionary as an exemplary and very basic database. The frontend accesses the flask database using a ClusterIP Service linked to the database-deployment. It also exposes an external IP via a Loadbalancer Service, so the streamlit app can be accesses via the browser and without the use of kubectl port-forward. Since minikube is a closed network, use minikube tunnel to allow access to it using the LoadBalancer. When looking at the ClusterIP service with kubectl describe service backendflask the IP address of the service to exposes, as well as the listed endpoints that connect to the database-deployments are shown. One can compare them to the IPs of the actual deployments - they are the same. # services_frontend-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 2 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: containers: - name: frontend image: seblum/mlops-public:frontend-streamlit imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # enviroment variable defined in the application and dockerfile # value is ip adress of the order env: # using the ip adress would be a bad idea. # use the service ip adress. # value: &quot;&lt;order-service-ip-adress&gt;:8081&quot; # how to do it should be this. # we reference to the order service - name: DB_SERVICE value: &quot;backendflask:5001&quot; ports: # we can actually use the actual ip of the service or # use the dns, as done in the example above. - containerPort: 8501 --- apiVersion: v1 kind: Service metadata: name: frontend-lb spec: type: LoadBalancer selector: app: frontend ports: - port: 80 targetPort: 8501 --- apiVersion: v1 kind: Service metadata: name: frontend-node spec: type: NodePort selector: app: frontend ports: - port: 80 targetPort: 8501 nodePort: 30000 # services_backend-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: backendflask spec: replicas: 2 selector: matchLabels: app: backendflask template: metadata: labels: app: backendflask spec: containers: - name: backendflask image: seblum/mlops-public:backend-flask imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 --- apiVersion: v1 kind: Service metadata: name: backendflask spec: # send traffic to any pod that matches the label type: ClusterIP # does not need to be specified selector: app: backendflask ports: # port the service is associated with - port: 5001 # port to access targeted by the access # in our case has to be the same as in backendflask. targetPort: 5000 5.3.2 Service Discovery Service Discovery is a mechanism that lets applications and microservices locate each other on a network. In fact, we have aready used Service Discovery in the previous sections, they just haven’t been mentioned yet. If a client wants to communicate with the application, it should not use the IP of an individual Pod should not use the individual pod ip. Instead, we should rely on services as they have a stable IP address. We have already seen this in the section about Services. Yet, each pod has also an individual DNS (Domain Name System). A DNS translates a domain names to an IP address, just one lookes up a number in a telephone book, so it’s much easier to reference to a resource online. This is where service Discovery enters the game. Service Discovery Whenever a service is created, it is registered in the service registry with the service name and the service IP. Most clusters use CoreDNS as a service registry (this would be the telephone book itself). When having a look at the minikube cluster one should see are core-dns service running. Now you know what it is for. Having a closer look using kubectl describe svc &lt;name&gt;, the core-dns service has only one endpoint. If you want to have an even closer look, you can dive into a pod itself and check the file /etc/resolv.conf. There you find a nameserver where the IP is the one of the core-dns. # when querying services, it necessary # to specify the corresponding namespace kubectl get service -n kube-system # command for queriying the dns nslookup &lt;podname&gt; 5.3.2.1 kube-proxy As mentioned earlier, each node has three main components: Kubelet, Container Runtime, and the Kube-Proxy. Kube-Proxy is a network proxy running on each node and is responsible for internal network communications as well as external. It also implements a controller that watches the API server for new services and endpoints. Whenever there is a new service or endpoint, the kube-proxy creates a local IPVS rule (IP Virtual Server) that tells the node to intercept traffic destined to the ClusterIP Service. IPVS is built on top of the network filter and implements a transport-layer load balancing. This gives the ability to load balance to real service as well as redirecting traffic to pods that match service label selectors. This means, kube-proxy is intercepting all the requests and makes sure that when a request to the ClusterIP service is sent using endpoints, the request is forwarded to the healthy pods behind the endpoint. "],["volumes-and-storage.html", "5.4 Volumes and Storage", " 5.4 Volumes and Storage Since Pods are ephemeral, any data associated is deleted when a Pod or container restarts. Applications are run stateless the majority of the times, meaning the data does not needs to be kept on the node and the data is stored on an external database. However, there are times when the data wants to be kept, shared between Pods, or when it should persist on the host file system (disk). As described in the section about Pods, a Pod can contain volumes. Volumes are exactly what is needed for such tasks. They are used to store and access data which can be persistent or long lived on K8s. There are different types of volumes, e.g.: EmptyDir HostPath Volume awsElasticBlockStore: AWS EBS volumes are persistent and originally unmounted. They are read-write-once-only tough. There are multiple other types of volumes, a full list can be found here: https://kubernetes.io/docs/concepts/storage/volumes/#volume-types 5.4.1 EmptyDir Volume An EmptyDir Volume is initially empty (as the name suggests). The volume is a temporary directory that shares the pods lifetime. If the pod dies, the contents of the emptyDir are lost as well. The EmptyDir is also used to share data between containers inside a Pod during runtime. # volume_empty-dir.yaml apiVersion: apps/v1 kind: Deployment metadata: name: emptydir-volume spec: selector: matchLabels: app: emptydir-volume template: metadata: labels: app: emptydir-volume spec: # add a volume to the deployment volumes: # mimics a caching memory type - name: cache # specify the volume type and the temp directory emptyDir: {} # of course there could also be a second volume added containers: - name: container-one image: busybox # image used for testing purposes # since the testing image immediately dies, we want to # execute an own sh command to interact with the volume volumeMounts: # The name must match the name of the volume - name: cache # interal reference of the pod mountPath: /foo command: - &quot;/bin/sh&quot; args: - &quot;-c&quot; - &quot;touch /foo/bar.txt &amp;&amp; sleep 3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # create a second container with a different internal mountPath - name: container-two image: busybox volumeMounts: - name: cache mountPath: /footwo command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; As stated in the yaml, the busybox image immediately dies. If the Containers where created without the shell commands, the pod would be in a crashloopbackoff-state. To prevent the Pod to do so it is caught with the sleepcommands until it scales down. Accessing a container using kubectl exec, it can be checked whether the foo/bar.txt has been created in container-one. When checking the second container container-two, the same file should be visible as well. This is because both containers refer to the same volume. Keep in mind though that the mountPath of the container-two is different. # get in container kubectl exec -it &lt;emptydir-volume-name&gt; -c container-one -- sh # check whether bar.txt is present ls # accessing the second container, there is also a file foo/bar.txt # remember, both containers share the same volume kubectl exec -it &lt;emptydir-volume-name&gt; -c container-two -- sh ls 5.4.2 HostPath Volume THe HostPath Volume type is used when an application needs to access the underlying host file system, meaning the file system of the node. HostPath represents a pre-existing file or directory on the host machine. However, this can be quite dangerous and should be used with caution. If having the right access, the application can interfere and basically mess up the host. It is therefore recommended to set the rights to read only to prevent this from happening. # volume_hostpath.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hostpath-volume spec: selector: matchLabels: app: hostpath-volume template: metadata: labels: app: hostpath-volume spec: volumes: - name: var-log # specify the HostPath volume type hostPath: path: /var/log containers: - name: container-one image: busybox volumeMounts: - mountPath: /var/log name: var-log readOnly: true command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; Similar to the EmptyDir Volume example, you can check the implementation of the HostPath Volume by accessing the volume. When comparing the file structures of the hostpath-volume deployment and the directory path: /var/log on the node the deployment is running, they should be the same. All the changes made to either on of them will make the changes available on the other. By making changes via the pod we can directly influence the Node. Again, this is why it is important to keep it read-only. # access the kubectl exec -it &lt;hostpath-volume-name&gt; -- sh # ssh into node minikube ssh &lt;node&gt; 5.4.3 Persistent Volumes Persistent Volumes allow to store data beyond a Pods lifecycle. If a Pod fails, dies or moves to a different node, the data is still intact and can be shared between pods. Persistent Volume types are implemented as plugins that K8s can support(a full list can be found online). Different types of Persistent Volumes are: NFS Local Cloud Network storage (AWS EBS, Azure File Storage, Google Persistent Disk) The following example show how the usage of Persistent Volumes works on the AWS cloud. K8s is running on an AWS EKS Cluster and AWS EBS Volumes attached to it. The Container storage interface (CSI) of K8s to use Persistent Volumes is implemented by the EBS provider, e.g. the aws-ebs-plugin. This enables a the use of Persistent Volumes in the EKS cluster. Therefore, a Persistent Volume (PV) is rather the mapping between the storage provider (EBS) and the K8s cluster, than a volume itself. The storage class of a Persistent Volume can be configured to the specific needs. Should the storage be fast or slow, or do we want to have each as a storage? Or might there be other parameters to configure the storage? If a Pods or Deployments want to consume storage of the PV, they need to get access to the PV. This is done via a so called persistent volume claim (PVC). All of this is part of a Persistent Volume Subsystem. The Persistent Volume Subsystem provides an API for users and administrators. The API abstracts details of how storage is provided from how it is consumed. Again, the provisioning of storage is done via a PV and the consumption via a PCV. Persistent Volume Subsystem Listed below are again the three main components when dealing with Persistent Volumes in K8s Persistent Volume: is a storage resource provisioned by an administrator PVC: is a user’s request for and claim to a persistent volume. Storage Class: describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned. So how are Persistent Volumes specified in our deployments yamls? As there are kind:Pod ressources, there can similarly kind:PersistentVolume and kind:PersistentVolumeClaim be specified. At first, a PV is created. As we run on minikube and not on the cloud, a local storage in the node is allocated. Second, a PVC is created requesting a certain amount of that storage. This PVC is then linked in the specifications of a Deployment to allow its containers to utilized the storage. Before applying the yaml-files we need to allocate the local storage by claiming storage on the node and set the paths specified in the yamls. To do this, we ssh into the node using minikube ssh. We can then create a specific path on the node such as /mnt/data/. We might also create a file in it to test accessing it when creating a PVC to a Pod. Since we do not know yet on what node the Pod is scheduled, we should create the directory on both nodes. Below are all steps listed again. # ssh into node minikube ssh # create path sudo mkdir /mnt/data # create a file with text sudo sh -c &quot;echo &#39;this is a pvc test&#39; &gt; /mnt/data/index.html&quot; # do this on both nodes as pod can land on either one of them Afterward we can apply the yaml files and create a PV, PVC, and the corresponding Deployment utilizing the PVC. The yaml code below shows this process. # volume_persistent-volume.yaml apiVersion: v1 kind: PersistentVolume metadata: name: mypv spec: # specifiy the capacity of the PersistentVolume capacity: storage: &quot;100Mi&quot; volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: manual hostPath: path: &quot;/mnt/data&quot; # specify the hostPath on the node # that&#39;s the path we specified on our node --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mypvc spec: resources: requests: # we request the same as the PV is specified # so we basically request everything storage: &quot;100Mi&quot; volumeMode: Filesystem storageClassName: &quot;manual&quot; accessModes: - ReadWriteOnce --- apiVersion: apps/v1 kind: Deployment metadata: name: pv-pvc-deployment spec: selector: matchLabels: app: pv-pvc template: metadata: labels: app: pv-pvc spec: volumes: - name: data # define the use of the PVC by specifying the name # specify the pod/deployment can use the PVC persistentVolumeClaim: claimName: mypvc containers: - name: pv-pvc image: nginx:latest volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; # since the PVC is stated, the container needs to # mount inside it # name is equal to the pvc name specified name: data resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: pv-pvc spec: type: LoadBalancer selector: app: pv-pvc ports: - port: 80 targetPort: 80 By accessing a pod using kubectl exec -it &lt;persistent-volume-name&gt; -- sh we can check whether the path is linked using the PVC. Now, the end result may seem the same as what we did with the HostPath Volume. But it actually is not, it just looks like it since both, the PersistentVolume and the HostPath connect to the Host. Yet, the locally mounted path would be somewhere else when running in the cloud. The PV configuration would point to another storage source instead of a local file system, for example an attached EBS of EFS storage. Since we also created a LoadBalancer service, we can run minikube tunnel to expose the application deplyment under localhost:80. It should show the input of the index.html file we created on the storage. "],["environment-configuration-security.html", "5.5 Environment, Configuration &amp; Security", " 5.5 Environment, Configuration &amp; Security 5.5.1 Namespaces Namespaces allow to organize resources in the cluster, which makes it more overseeable when there are multiple resources for different needs. Maybe we want to organize by team, department, or according to a development environment (dev/prod), etc. By default, K8s will use the default-namespace for resources that have not been specified otherwise. Similarly, kubectl interacts with the default namespace as well. Yet, there are already different namespace in a basic K8s cluster default - The default namespace for objects with no other namespace kube-system - The namespace for objects created by the Kubernetes system kube-public - This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. kube-node-lease - This namespace for the lease objects associated with each node which improves the performance of the node heartbeats as the cluster scales. Of course, there is also the possibility of creating ones own namespace and using it by attaching a e.g. Deployment to it, such as seen in the following example. # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring --- apiVersion: apps/v1 kind: Deployment metadata: name: monitoring-deployment namespace: monitoring spec: replicas: 1 selector: matchLabels: app: monitoring-deployment template: metadata: labels: app: monitoring-deployment spec: containers: - name: monitoring-deployment image: &quot;grafana/grafana:latest&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 5000 When creating a Service, a corresponding DNS entry is created as well, such as seen in the Services section when calling backendflask directly. This entry is created according to the namespace which is denoted to the service. This can be useful when using the same configuration across multiple namespaces such as development, staging, and production. It is also possible to reach across namespaces. One needs to use the fully qualified domain name (FQDN) tough, such as &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local. 5.5.2 Labels, Selectors and Annotations In the previous sections we already made use of labels, selectors, and annotations, e.g. when matching the ClusterIP service to the back-deployments. Labels are a key-value pair that can be attached to objects such as Pods, Deployments, Replicaset, Services, etc. Overall, they are used to organize and select objects. Annotations are an unstructured key-value mapping stored with a resource that may be set by external tools to store and retrieve any metadata. In contrast to labels and selectors, annotations are not used for querying purposes but rather to attach arbitrary non-identifying metadata. These data are used to assist tools and libraries to work with the K8s ressource, for example to pass configuration around between systems, or to send values so external tools can perform more informed decisions based on the annotations provided. Selectors are used to filter K8s objects based on a set of labels. A selector basically simply uses a boolean language to select pods. The selector matches the labels under a an all or nothing principle, meaning everything specified in the selector must be fulfilled by the labels. However, this works not the other way around. If there are multiple labels specified and the selector matches only one of them, the selector will match the ressource itself. How a selector matches the labels can be tested using the kubectl commands as seen below. # Show all pods including their labels kubectl get pods --show-labels # Show only pods that match the specified selector key-value pairs kubectl get pods --selector=&quot;key=value&quot; kubectl get pods --selector=&quot;key=value,key2=value2&quot; # in short one can also write kubectl get pods -l key=value # or also look for multiple kubectl get pods -l &#39;key in (value1, value2)&#39; When using ReplicaSets in a Deployment, their selector matches the labels to a specific pod (check e.g. the section describing Deployments). Any Pods matching the label of the selector will be created according to the specified replicas. Of course, there can also be multiple labels specified. The same principle accounts when working with Services. Below example shows two different Pods and two NodePort services. Each service matches to a Pod based on their selector-label relationship. Have a look at their specific settings using kubectl. The Nodeport Service labels-and-selectors-2 has no endpoints, as it is a all-or-none-principle and none of the created Pods matches the label environment=dev. In contrast, even though the Pod cat-v1 has multiple labels specified app: cat-v1; version: one, the NodePort Service labels-and-selectors is linked to it. It is also linked to the second Pod cat-v2. # labels.yaml apiVersion: v1 kind: Pod metadata: name: cat-v1 labels: app: cat-v1 version: one spec: containers: - name: cat-v1 image: &quot;seblum/mlops-public:cat-v1&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; --- apiVersion: v1 kind: Pod metadata: name: cat-v2 labels: app: cat-v1 spec: containers: - name: cat-v2 image: &quot;seblum/mlops-public:cat-v2&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; --- apiVersion: v1 kind: Service metadata: name: labels-and-selectors spec: type: NodePort selector: app: cat-v1 ports: - port: 80 targetPort: 5000 --- apiVersion: v1 kind: Service metadata: name: labels-and-selectors-2 spec: type: NodePort selector: app: cat-v1 environment: dev ports: - port: 80 targetPort: 5000 5.5.3 ConfigMaps When building software, the same container image should be used for development, testing, staging, and production stage. Thus, container images should be reusable. What usually changes are only the configuration settings of the application. ConfigMaps allow to store such configurations as a simple mapping of key-value pairs. Most of the time, the configuration within a config map is injected using environment variables and volumes. However, ConfigMaps should only be used to store configuration files, not sensitive data, as they do not secure them. Besides allow for an easy change of variables, another benefit of using ConfigMaps is that changes in the configuration are not disruptive, meaning the application can still run while the configuration changes without affecting the application. However, one needs to keep in mind that change made to ConfigMaps and environment variables will not be reflected on already and currently running containers. The following example creates two different ConfigMaps. The first one includes three environment variables as data. The second one include a more complex configuration of an nginx server. # configmaps.yaml apiVersion: v1 kind: ConfigMap metadata: name: app-properties data: app-name: kitty app-version: 1.0.0 team: engineering --- apiVersion: v1 kind: ConfigMap metadata: name: nginx-conf data: # configuration in .conf nginx.conf: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location /health { access_log off; return 200 &quot;healthy\\n&quot;; } } Additionally, a Deployment is created which uses both ConfigMaps. A ConfigMap is declared under spec.volumes as well. It is also possible to state a reference to both ConfigMaps simultaneously. The Deployment creates two containers. The first container mounts each ConfigMap as a Volume. Container two uses environment variables to access and configure the key-value pairs of the ConfigMaps and store them on the container. # configmaps_deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: config-map spec: selector: matchLabels: app: config-map template: metadata: labels: app: config-map spec: volumes: # specify ConfigMap nginx-conf - name: nginx-conf configMap: name: nginx-conf # specify ConfigMap app-properties - name: app-properties configMap: name: app-properties # if both configmaps shall be mounted under one directory, # we need to use projected - name: config projected: sources: - configMap: name: nginx-conf - configMap: name: app-properties containers: - name: config-map-volume image: busybox volumeMounts: - mountPath: /etc/cfmp/ngnix # is defined here in the nginx-volume to mount name: nginx-conf # everything from that configMap is mounted as a file # the file content is the value themselves - mountPath: /etc/cfmp/properties name: app-properties - mountPath: etc/cfmp/config name: config command: - &quot;/bin/sh&quot; - &quot;-c&quot; args: - &quot;sleep 3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; - name: config-map-env image: busybox resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # as previously, keep the busybox container alive command: - &quot;/bin/sh&quot; - &quot;-c&quot; args: - &quot;env &amp;&amp; sleep 3600&quot; env: # environment variables to read in from config map # for every data key-value pair in config Map, an own # environment variable is created, which gets # the value from the corresponding key - name: APP_VERSION valueFrom: configMapKeyRef: name: app-properties key: app-version - name: APP_NAME valueFrom: configMapKeyRef: name: app-properties key: app-name - name: TEAM valueFrom: configMapKeyRef: name: app-properties key: team # reads from second config map - name: NGINX_CONF valueFrom: configMapKeyRef: name: nginx-conf key: nginx.conf We can check for the attached configs by accessing the containers via the shell, similar to what we did in the section about Volumes. In the container config-map-volume, the configs are saved under the respective mountPath of the volume. In the config-map-env, the configs are stored as environment variables. # get in container -volume or -env kubectl exec -it &lt;config-map-name&gt; -c &gt;container-name&lt; -- sh # check subdirectories ls # print environment variables printenv 5.5.4 Secrets Secrets, as the name suggests, store and manage sensitive information. However, secrets are actually not secrets in K8s. They can quite easily decoded using kubectl describe on a secret and decode it using the shell command echo &lt;password&gt; | base64 -d. Thus, sensitive information like database password should never be stored in secrets. There are much better ressources to store such data, for example a Vault on the cloud provider itself. However, secret can be used so that you don’t need to include confidential data in your application code. Since they are stored and created independently of the Pods that use them, there is less risk of being exposed during the workflow of creating, viewing, and editing Pods. It is possible to create secrets using imperative approach as shown below. # create the two secrets db-password and api-token kubectl create secret generic mysecret-from-cli --from-literal=db-password=123 --from-literal=api-token=token # output the new secret as yaml kubectl get secret mysecret -o yaml # create a file called secret with a file-password in it echo &quot;super-save-password&quot; &gt; secret # create a secret from file kubectl create secret generic mysecret-from-file --from-file=secret Similar to ConfigMaps, secrets are accessed via an environment variable or a volume. # secrets.yaml apiVersion: apps/v1 kind: Deployment metadata: name: secrets spec: selector: matchLabels: app: secrets template: metadata: labels: app: secrets spec: volumes: # get the secret from a volume - name: secret-vol secret: # the name of the secret we created earlier secretName: mysecret-from-cli containers: - name: secrets image: busybox volumeMounts: - mountPath: /etc/secrets name: secret-vol env: # nane of the secret in the container - name: CUSTOM_SECRET # get the secret from an environment variable valueFrom: secretKeyRef: # name and key of the secret we created earlier name: mysecret-from-file key: secret command: - &quot;sleep&quot; - &quot;3600&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; 5.5.4.1 Exemplary use case of secrets When pulling from a private dockerhub repository, applying the deployment will throw an error since there are no username and password specified. As they should not be coded into the deployment yaml itself, they can be accessed via a secret. In fact, a specific secret can be specified for docker registry. The secret can be specified using the imperative approach. kubectl create secret docker-registry docker-hub-private \\ --docker-username=YOUR_USERNAME \\ --docker-password=YOUR_PASSWORD \\ --docker-email=YOUR_EMAIL Finally, the secret is specified in the deployment configuration where it can be accessed during application. # secret_dockerhub.yaml apiVersion: apps/v1 kind: Deployment metadata: name: secret-app spec: selector: matchLabels: app: secret-app template: metadata: labels: app: secret-app spec: # specifiy the docker-registry secret to be accessed imagePullSecrets: - name: docker-hub-private containers: - name: secret-app # of course you need an own private repository # to pull and change the name accordingly image: seblum/private:cat-v1 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 "],["observability-maintenance.html", "5.6 Observability &amp; Maintenance", " 5.6 Observability &amp; Maintenance 5.6.1 Health Checks Applications running in service need to be healthy at all times so they are ready to receive traffic. K8s uses a process called health checks to test whether an application is alive. If there are any issues and the application is unhealthy, K8s will restart the process. Yet, checking only whether a process is up and running might be not sufficient. What if, e.g., a client wants to connect to a database and the connection cannot be established, even though the app is up and running? To solve more specific issues like this, health checks like a liveness probe or readiness probe can be used. If there have not been specified, K8s will use the default checks to test whether a process is running. There are three primary types of health checks: TCP, exec, and HTTP. TCP health checks perform a GET request on container’s IP by verifying if a network socket is open and responsive. Exec health checks allow custom scripts or commands to be run within the container which can be specified using the exec field in the yaml definition file. Thirdly, a HTTP request checks the status of a web service by sending HTTP requests to specific endpoints, making them particularly suited for applications with HTTP interfaces. The choice of health check type depends on the nature of the application and the specific aspects to monitor. For the sake of this tutorial only the last type http will be covered. 5.6.1.1 Liveness Probe The Kubelet of a node uses Liveness Probes to check whether an application runs fine or whether it is unable to make progress and its stuck on a broken state. For example, it could catch a deadlock, a database connection failure, etc. The Liveness Probe can restart a container accordingly. To use a Liveness Probe, an endpoint needs to be specified. The benefit of this is, that it is simple to define what it means for an application to be healthy just by defining a path. 5.6.1.2 Readiness Probe Similar to a Liveness Probe, the Readniness Probe is used by the kubelet to check when the container is ready to start accepting traffic. A Pod is considered ready when all of its containers are ready. Its configuration is also done by specifying a path to what it means the application is healthy. A lot of frameworks, like e.g. springboot, actually provide a path to use. Belows configuration shows a Deployment which includes a Liveness and a Readiness Probe. The image of the deployment is set up so its process is killed after a given number of seconds. This has been passed using environment variables such as seen in the script. Both, the Liveness and the Readiness Probe have the same parameters in the given example. initialDelaySeconds: The probe will not be called until x seconds after all the containers in the Pod are created. timeoutSeconds: The probe must respond within a x-second timeout and the HTTP status code must be equal to or greater than 200 and less than 400. periodSeconds: The probe is called every x seconds by K8s failureThreshold: The container will fail and restart if more than x consecutive probes fail # healthchecks.yaml apiVersion: apps/v1 kind: Deployment metadata: name: backendflask-healthcheck spec: replicas: 1 selector: matchLabels: app: backendflask-healthcheck template: metadata: labels: app: backendflask-healthcheck environment: test tier: backend department: engineering spec: containers: - name: backendflask-healthcheck # check whether I have to change the backend app to do this. image: &quot;seblum/mlops-public:backend-flask&quot; imagePullPolicy: &quot;Always&quot; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; # Specification of the the Liveness Probe livenessProbe: httpGet: # path of the url path: /liveness port: 5000 # time the liveness probe starts after pod is started initialDelaySeconds: 5 timeoutSeconds: 1 failureThreshold: 3 # period of time when the checks should be performed periodSeconds: 5 # Specification of the Readiness Probe readinessProbe: httpGet: path: /readiness port: 5000 initialDelaySeconds: 10 # change to 1 seconds and see the pod not going to go ready timeoutSeconds: 3 failureThreshold: 1 periodSeconds: 5 env: # variable for the container to be killed after 60 seconds - name: &quot;KILL_IN_SECONDS&quot; value: &quot;60&quot; ports: - containerPort: 5000 --- apiVersion: v1 kind: Service metadata: name: backendflask-healthcheck spec: type: NodePort selector: app: backendflask-healthcheck ports: - port: 80 targetPort: 8080 nodePort: 30000 --- apiVersion: v1 kind: Service metadata: name: backendflask-healthcheck spec: type: ClusterIP selector: app: backendflask-healthcheck ports: - port: 80 targetPort: 8080 "],["helm.html", "5.7 Helm", " 5.7 Helm The previous sections showed the complexity of working with Kubernetes. As in other programming languages, there are easier ways to manage an application rather than writing all of the deployments and services of an application by hand. Among others, Helm is a package manager for Kubernetes that enables to template and group Kubernetes manifests as versioned packages. Helm applications are a collection of yaml and helper templates. Once an application is packaged, it can be installed onto a Kubernetes cluster using a single command. Helm applications are packaged as tar files and stored in repositories similar to registries like Docker or PyPi, and some repository registries like Artifactory automatically index Helm Charts. This ease of working allows Helm to provide a large collection of open-source charts to easily deploy applications such as PostgreSQL, Redis, Nginx, and Prometheus. 5.7.1 Helm Chart Structure As previously mentioned, a Helm package consists of a Helm Chart that includes a collection of yaml and helper templates. This chart is finally packaged as a .tar file. The following structure shows how the chart my-custom-chart is organized in the directory custom_chart. The Chart.yaml file consists of the major metadata about the chart, such as name, version, and description, as well as dependencies if multiple charts are packaged. The /templates directory contains all the Kubernetes manifests that define the behavior of the application and are deployed automatically by installing the chart. Exemplary variables able to be specified are denoted in the values.yaml file, which also incorporates default values. Just like a .gitignore file it is also possible to add a .helmignore. custom_chart/ ├── .helmignore # Contains patterns to ignore ├── Chart.yaml # Information about your chart ├── values.yaml # The default values for your templates └── templates/ # The template files └── ingress.yaml # ingress.yaml manifest, └── ... # and others... When wanting to create a own Helm Chart, there is no need to create all the files on your own. To bootstrap a Helm Chart there is the in-built command: helm create &lt;my-chart&gt; that provide all the common Kubernetes manifests (deployment.yaml, hpa.yaml , ingress.yaml , service.yaml , and serviceaccount.yaml) as well as helper templates to circumvent resource naming constraints and labels/annotations. The command will provide a scalable deployment for nginx on default, which can be simply modified to deploy a custom docker image by editing the values.yaml file. 5.7.2 Working with Helm As there is large collection of open-source and public charts already available, there is no need to create your own Helm Chart. One simply can use helms build in command to search for a specific Helm Chart, such as shown below by searching for a redis deployment, or have a look oneselves by scrolling through for example artifactory or bitnami, which provide a large collection of public charts. helm search hub redis 5.7.2.1 Adding a Helm Chart to the local setup. After finding the correct Helm Chart, it can simply be added to the local setup. Once added to the local setup, the chart is listed in the local repository and is ready to be installed. # Add a helm chart to the local setup under the name &quot;bitnami&quot; helm repo add bitnami https://charts.bitnami.com/bitnami # Search and show the local repository for all charts with the name bitnami # Once it a helm chart is listed here it can be installed helm search repo bitnami Since Helm harts are packaged as a .tarfile, they can also be downloaded locally and modified as needed. # Download the nginx-ingress-controller helm chart to local helm pull bitnami/nginx-ingress-controller --version 5.3.19 5.7.2.2 Installing a Helm Chart Once a Helm Chart is downloaded or added to the local setup, it can be installed using the helm install command followed by a custom release name, and the name of the chart to be installed. It is a best practice to update the list of charts before installing, just like when installing packages in other programming languages such as Pip and Python. # Make sure we get the latest list of charts helm repo update # Installing a Helm Chart from the local repository # helm install &lt;CUSTOM-RELEASE-NAME&gt; &lt;CHART-NAME&gt; helm install custom-bitnami bitnami/wordpress # Installing a downloaded Helm Chart from a directory helm install -f values.yaml my-custom-chart ./custom_chart Similar to installing a Helm Chart, it can also be uninstalled. # helm uninstall &lt;CUSTOM-RELEASE-NAME&gt; helm uninstall custom-bitnami helm uninstall my-custom-chart All installed and released charts can be listed using the following command. helm list "],["terraform.html", "Chapter 6 Terraform", " Chapter 6 Terraform Terraform is an open-source, declarative programming language developed by HashiCorp and allows to create both on-prem and cloud resources in the form of writing code. This is also know as Infrastructure as Code (IAC). There are several IaC tools in the market today and Terraform is only one of them. Yet it is a well known and established tool and during the course of this project we only focus on this. HashiCorp describes that: Terraform is an infrastructure as code (IaC) tool that allows you to build, change, and version infrastructure safely and efficiently. This includes both low-level components like compute instances, storage, and networking, as well as high-level components like DNS entries and SaaS features. This means that users are able to manage and provision an entire IT infrastructure using machine-readable definition files and thus allowing faster execution when configuring infrastructure, as well as enableing full traceability of changes. Terraform comes with several hundred different providers that can be used to provision infrastructure, such as Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, DataDog, etc. The given chapter introduces the concepts &amp; usage of Terraform which will be needed to create the introduced MLOps Airflow deployment in an automated and tracable way. We will learn how to use Terraform to provision ressources as well as to structure a Terraform Project. Prerequisites To be able to follow this tutorial, one needs to have the AWS CLI installed as well as the AWS credentials set up. Needles to say an AWS accounts needs to be present. It is also recommended to have basic knowledge of the AWS Cloud as this tutorials used the AWS infrastructure to provision cloud resources. The attached resource definitions are specified to the AWS region eu-central-1. It might be necessary to change accordingly if you are set in another region. Further, Terraform itself needs to be installed. Please refer to the corresponding sites. The scripts are run under Terraform version v1.2.4. Later releases might have breaking changes. One can check its installation via terraform version. "],["basic-usage.html", "6.1 Basic usage", " 6.1 Basic usage A Terraform project is basically just a set of files in a directory containing resource definitions of cloud ressources to be created. Those Terraform files, denoted by the ending .tf, use Terraform’s configuration language to define the specified resources. In the following example there are two definitions made: a provider and a resource. Later in this chapter we will dive deeper in the structure of the language. For now, we only need to know this script is creating a file called hello.txt that includes the text \"Hello, Terraform\". It’s our Terraform version of Hello World! provider &quot;local&quot; { version = &quot;~&gt; 1.4&quot; } resource &quot;local_file&quot; &quot;hello&quot; { content = &quot;Hello, Terraform&quot; filename = &quot;hello.txt&quot; } 6.1.1 terraform init When a project is run for the first time the terraform project needs to be initialized. This is done via the terraform init command. Terraform scans the project files in this step and downloads any required providers needed (more details to providers in the section about providers). In the given example this is the local provider. # Initializes the working directory which consists of all the configuration files terraform init 6.1.2 terraform validate The terraform validate command checks the code for syntax errors. This is optional yet a way to handle initial errors or minor careless mistakes # Validates the configuration files in a directory terraform validate 6.1.3 terraform plan The terraform plan command verifies what action Terraform will perform and what resources will be created. This step is basically a dry run of the code to be executed. It also returns the provided values and some permission attributes which have been set. # Creates an execution plan to reach a desired state of the infrastructure terraform plan 6.1.4 terraform apply The command Terraform apply creates the resource specified in the .tf files. Initially, the same output as in the terraform plan step is shown (hence its dry run). The output further states which resources are added, which will be changed, and which resources will be destroyed. After confirming the actions the resource creation will be executed. Modifications to previously deployed ressources can be implemented by using terraform apply again. The output will denote that there are resources to change. # Provision the changes in the infrastructure as defined in the plan terraform apply 6.1.5 terraform destroy To destoy all created ressouces and to delete everything we did before, there is a terraform destroy command. # Deletes all the old infrastructure resources terraform destroy "],["core-components-3.html", "6.2 Core Components", " 6.2 Core Components The following section will explain the core components and building blocks of Terraform. This will enable you to build your very first Terraform definition files. 6.2.1 Providers Terraform relies on plugins called providers to interact with Cloud providers, SaaS providers, and other APIs. Each provider adds specific resource types and/or data sources that can be managed by Terraform. For example, the aws provider shown below allows to specify resources related to the AWS Cloud such as S3 Buckets or EC3 Instances. Depending on the provider it is necessary to supply it with specific parameters. The aws provier for example needs the region as well as username and password. If nothing is specified it will automatically pull these information from the AWS CLI and the credentials specified under the directory .aws/config. It is also a best practice to specify the version of the provider, as the providers are usually maintained and updated on a regular basis. provider &quot;aws&quot; { region = &quot;us-east-1&quot; } 6.2.2 Resources A resource is the core building block when working with Terraform. It can be a \"local_file\" such as shown in the example above, or a cloud resource such as an \"aws_instance\" on aws. The resource type is followed by the custom name of the resource in Terraform. Resource definitions are usually specified in the main.tffile. Each customization and setting to a ressource is done within its resource specification. The style convention when writing Terraform code states that the resource name is named in lowercase as well as it should not repeat the resource type. An example can be seen below # Ressource type: aws_instance # Ressource name: my-instance resource &quot;aws_instance&quot; &quot;my-instance&quot; { # resource specification ami = &quot;ami-0ddbdea833a8d2f0d&quot; instance_type = &quot;t2.micro&quot; tags = { Name = &quot;my-instance&quot; ManagedBy = &quot;Terraform&quot; } } 6.2.3 Data Sources Data sources in Terraform are “read-only” resources, meaning that it is possible to get information about existing data sources but not to create or change them. They are usually used to fetch parameters needed to create resources or generally for using parameters elsewhere in Terraform configuration. A typical example is shown below as the \"aws_ami\" data source available in the AWS provider. This data source is used to recover attributes from an existing AMI (Amazon Machine Image). The example creates a data source called \"ubuntu” that queries the AMI registry and returns several attributes related to the located image. data &quot;aws_ami&quot; &quot;ubuntu&quot; { most_recent = true filter { name = &quot;name&quot; values = [&quot;ubuntu/images/hvm-ssd/ubuntu-trusty-14.04-amd64-server-*&quot;] } filter { name = &quot;virtualization-type&quot; values = [&quot;hvm&quot;] } owners = [&quot;099720109477&quot;] # Canonical } Data sources and their attributes can be used in resource definitions by prepending the data prefix to the attribute name. The following example used the \"aws_ami\" data source within an \"aws_instace\" resource. resource &quot;aws_instance&quot; &quot;web&quot; { ami = data.aws_ami.ubuntu.id instance_type = &quot;t2.micro&quot; } 6.2.4 State A Terraform state stores all details about the resources and data created within a given context. Whenever a resource is create terrafrom stores its identifier in the statefile terraform.tfstate. Providing information about already existing resources is the primary purpose of the statefile. Whenever a Terraform script is applied or whenever the resource definitions are modified, Terraform knows what to create, change, or delete based on the existing entries within the statefile. Everything specified and provisioned within Terraform will be stored in the statefile. This should be kept in mind and detain to store sensitive information such as initial passwords. Terraform uses the concept of a backend to store and retrieve its statefile. The default backend is the local backend which means to store the statefile in the project’s root folder. However, we can also configure an alternative (remote) backend to store it elsewhere. The backend can be declared within a terraform block in the project files. The given example stores the statefile in an AWS S3 Bucket callen some-bucket. Keep in mind this needs access to an AWS account and also needs the AWS provider of terraform. terraform { backend &quot;s3&quot; { bucket = &quot;some-bucket&quot; key = &quot;some-storage-key&quot; region = &quot;us-east-1&quot; } } "],["modules.html", "6.3 Modules", " 6.3 Modules A Terraform module allows to reuse resources in multiple places throughout the project. They act as a container to package resource configurations. Much like in standard programming languages, Terraform code can be organized across multiple files and packages instead of having one single file containing all the code. Wrapping code into a module not only allows to reuse it throughout the project, but also in different environments, for example when deploying a dev and a prod infrastructure. Both environments can reuse code from the same module, just with different settings. A Terraform module is build as a directory containing one or more resource definition files. Basically, when putting all our code in a single directory, we already have a module. This is exactly what we did in our previous examples. However, terraform does not include subdirectories on its own. Subdirectories must be called explicitly using a terraform moduleparameter. The example below references a module located in a ./network subdirectory and passes two parameters to it. # main.tf module &quot;network&quot; { source = &quot;./networking&quot; create_public_ip = true environment = &quot;prod&quot; } Each module consists of a similar file structure as the root directory. This includes a main.tf where all resources are specified, as well as files for different data sources such as variables.tf and outputs.tf. However, providers are usually configured only in the root module and are not reused in modules. Note that there are different approaches on where to specify the providers. They are either specified in the main.tf or a separate providers.tf. It does not make a difference for Terraform as it does not distinguish between the resource definition files. It is merely a strategy to keep code and project in a clean and consistent structure. root │ main.tf │ variables.tf │ outputs.tf │ └── networking │ main.tf │ variables.tf │ outputs.tf 6.3.1 Input Variables Each module can have multiple Input Variables. Input Variables serve as parameters for a Terraform module so users can customize behavior without editing the source. In the previous example of importing a network module, there have been two input variables specified, create_public_ip and environment. Input variables are usually specified in the variables.tf file. # variables.tf variable &quot;instance_name&quot; { type = string default = &quot;awesome-instance&quot; description = &quot;Name of the aws instance to be created&quot; } Each variable has a type (e.g. string, map, set, boolen) and may have a default value and description. Any variable that has no default must be supplied with a value when calling the module reference. This means that variables defined at the root module need values assigned to as a requirement so Terraform will not fail. This can be done by different resources, for example a variable’s default value via the command line using the terraform apply -var=\"variable=value\"option via environment variables starting with TF_VAR_; Terraform will check them automatically a .tfvars file where the variable values are specified; Terraform can load variable definitions from these files automatically (please check online resources for further insights) Variables can be used in expressions using the var.prefix such as shown in below example. We use the resource configuration of the previous example to create an aws_instance but this time its name is provided by an input variable. # main.tf resource &quot;aws_instance&quot; &quot;awesome-instance&quot; { ami = &quot;ami-0ddbdea833a8d2f0d&quot; instance_type = &quot;t2.micro&quot; tags = { Name = var.instance_name } } 6.3.2 Output Variables Similar to Input variables, a terraform module has output variables. As their name states, output variables return values of a Terraform module and are denoted in the outputs.tf file as expected. A module’s consumer has no access to any resources or data created within the module itself. However, sometimes a modules attrivutes are needed for another module or resource. Output variables address this issue by exposing a defined subset of the created resources. The example below defines an output value instance_address containing the IP address of an EC2 instance the we create with a module. Any module that reference this module can use the instance_address value by referencing it via module.module_name.instance_address # outputs.tf output &quot;instance_address&quot; { value = aws_instance.awesome-instance.private_ip description = &quot;Web server&#39;s private IP address&quot; } 6.3.3 Local Variables Additionally to Input variables and output variables a module provides the use of local variables. Local values are basically just a convenience feature to assign a shorter name to an expression and work like standard variables. This means theor scope is also limited to the module they are declared in. Using local variables reduces code repetitions which can be especially valuable when dealing with output variables from a module. # main.tf locals { vpc_id = module.network.vpc_id } module &quot;network&quot; { source = &quot;./network&quot; } module &quot;service1&quot; { source = &quot;./service1&quot; vpc_id = local.vpc_id } module &quot;service2&quot; { source = &quot;./service2&quot; vpc_id = local.vpc_id } "],["additional-tips-tricks.html", "6.4 Additional tips &amp; tricks", " 6.4 Additional tips &amp; tricks Of course there is much more to Terraform than these small examples can provide. Yet there are also some contrains when working with Terraform or declarative languages in general. Typically they do not have for-loops or other traditional procedural logic built into the language to repeat a piece of logic or conditional if-statements to configure reasources on demand. However, there are ways there are some ways to deal with this issue and to create multiple respurces without copy and paste. Terraform comes with different looping constructs, each used slightly different. The count and for_each meta arguments enable us to create multiple instances of a resource. 6.4.1 count Count can be used to loop over any resource and module. Every Terraform resource has a meta-parameter count one can use. Count is the simplest, and most limited iteration construct and all it does is to define how many copies to create of a resource. When creating multiple instance with one specification, the problem is that each instance must have a unique name, otherwise Terraform would cause an error. Therefore we need to index the meta-parameter just like doing it in a for-loop to give each resource a unique name. The example below shows how to do this on an AWS IAM user. resource &quot;aws_iam_user&quot; &quot;example&quot; { count = 2 name = &quot;neo.${count.index}&quot; } variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;snake&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.user_names) # returns the number of items in the given array name = var.user_names[count.index] } After using count on a resource it becomes an array of resources rather than one single resource. The same hold when using count on modules. When adding count to a module it turns it into an array of modules. This can round into problems because the way Terraform identifies each resource within the array is by its index. Now, when removing an item from the middle of the array, all items after it shift one index back. This will result in Terraform deleting every resource after that item and then re-creating these resources again from scratch So after running terraform plan with just three names, Terraform’s internal representation will look like this: variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.user_names) # returns the number of items in the given array name = var.user_names[count.index] } Count as conditional Count can also be used as a form of a conditional if statement. This is possible as Terraform supports conditional expressions. If count is set to one 1, one copy of that resource is created; if set to 0, the resource is not created at all. Writing this as a conditional expression could look something like the follow, where var.enable_autoscaling is a boolean variable either set to True or False. resource &quot;example-1&quot; &quot;example&quot; { count = var.enable_autoscaling ? 1 : 0 name = var.user_names[count.index] } 6.4.2 for-each The for_each expression allows to loop over lists, sets, and maps to create multiple copies of a resource just like the count meta. The main difference between them is that count expects a non-negative number, whereas for_each only accepts a list or map of values. Using the same example as above it would look like this: variable &quot;user_names&quot; { description = &quot;Create IAM users with these names&quot; type = list(string) default = [&quot;adam&quot;, &quot;eve&quot;, &quot;snake&quot;, &quot;apple&quot;] } resource &quot;aws_iam_user&quot; &quot;example&quot; { for_each = toset(var.user_names) name = each.value } output &quot;all_users&quot; { value = aws_iam_user.example } Using a map of resource with the for_each meta rather than an array of resources as with count has the benefit to remove items from the middle of the collection safely and without re-creating the resources following the deleted item. Of course, the same can also be done for modules. module &quot;users&quot; { source = &quot;./iam-user&quot; for_each = toset(var.user_names) user_name = each.value } 6.4.3 for Terraform also offers a similar functionality as Python list comprehension in the form of a for expression. This should not be confused with the for-each expression seen above. The basic syntax is shown below to convert the list of names of previous examples in var.names to uppercase: output &quot;upper_names&quot; { value = [for name in var.names : upper(name)] } output &quot;short_upper_names&quot; { value = [for name in var.names : upper(name) if length(name) &lt; 5] } Using for to loop over lists and maps within a string can be used similarly. This allows us to use control statements directly withing strings using a syntax similar to string interpolation. output &quot;for_directive&quot; { value = &quot;%{ for name in var.names }${name}, %{ endfor }&quot; } 6.4.4 Workspaces Terraform workspaces allow us to keep multiple state files for the same project. When we run Terraform for the first time in a project, the generated state file will go into the default workspace. Later, we can create a new workspace with the terraform workspace new command, optionally supplying an existing state file as a parameter. "],["exemplary-deployment.html", "6.5 Exemplary Deployment", " 6.5 Exemplary Deployment Configuring NGINX on Amazon EC2 Instances Using Terraform In this final exemplary Terraform deployment, we aim to establish our cloud infrastructure on AWS as done in the previous subsections. This infrastructure comprises an AWS Virtual Private Cloud (VPC) housing EC2 instances, upon which NGINX will be deployed. The structural arrangement is as follows: We employ two distinct modules: the \"root\" module and the \"vpc\" module. The \"root\" module is responsible for orchestrating all other modules, respectively the \"vpc\" module, alongside integrating all further resources, such as for example ec2_instances. The ultimate goal is to have NGINX efficiently operational on these EC2 instances within the provisioned AWS VPC. root │ main.tf │ variables.tf │ outputs.tf │ providers.tf │ userdata.tpl │ terraform.pem # (to be created in the aws console) │ └── vpc │ │ main.tf │ │ variables.tf │ │ outputs.tf Before executing the provided Terraform script, it’s necessary to first create an AWS key pair (pem file) with the specific name terraform.pem within the AWS Management Console. This key pair will be instrumental for SSH access to the EC2 instances that the script is designed to provision. To accomplish this, log in to your AWS account via the AWS Management Console, navigate to the EC2 service, and then locate the “Key Pairs” section under “Network &amp; Security” in the EC2 dashboard. Click on the “Create Key Pair” button and assign the name terraform to your key pair. Once created, a .pem file will be generated and automatically downloaded to your computer. Ensure you save this file in a secure location, as it will be used for accessing the EC2 instances. With the terraform.pem file prepared, you can proceed to run the Terraform script, ensuring that the file resides in the same directory where you intend to execute the Terraform commands. 6.5.1 root In this Tutorial, the term root designates the highest-level directory within the codebase, wherein crucial components such as primary modules, provider configurations, and terraform.tfvars variables are typically established. 6.5.1.1 main.tf The main.tf defines the main code for deploying an AWS EC2 instance and related resources. First, it collects data about the latest AWS Ubuntu Amazon Machine Image (AMI) with a specific name pattern using the aws_ami data source. This image will be used as the base for EC2 instances. It deploys an EC2 instance using the \"terraform-aws-modules/ec2-instance/aws\" module, specifying various parameters such as the AMI ID obtained from the data source, instance type t2.micro, the security group we create with the vpc module, the SSH key_name we created within the AWS console, and userdata.tpl script which installed ngnix on the instance. On the bottom of the code, the vpc module is deployed using a custom module located in the \"./vpc\" directory, with a specified security group name. As mentioned earlier, EC2 instance is associated with this security group for network access control. As the EC2 instance is dependent on the security group, the vpc module will be created before the ec2_instance, even if the code is specified afterward. # DATA data &quot;aws_ami&quot; &quot;aws_ubuntu&quot; { most_recent = true owners = [&quot;amazon&quot;] filter { name = &quot;name&quot; values = [&quot;ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*&quot;] } } module &quot;ec2_instances&quot; { source = &quot;terraform-aws-modules/ec2-instance/aws&quot; version = &quot;3.5.0&quot; count = 1 name = &quot;ec2-nginx-demo&quot; ami = data.aws_ami.aws_ubuntu.id instance_type = &quot;t2.micro&quot; vpc_security_group_ids = [module.vpc.aws_security_group_id] key_name = var.pem_key_name user_data = file(&quot;userdata.tpl&quot;) tags = { Name = &quot;NginxDemo&quot; } } module &quot;vpc&quot; { source = &quot;./vpc&quot; security_group_name = &quot;demo_sg&quot; } 6.5.1.2 outputs.tf The outputs.tf file defines an output named \"aws_instance_public_dns\" that captures and makes accessible the public DNS of the first EC2 instance created using the \"ec2_instances\" module. output &quot;aws_instance_public_dns&quot; { value = module.ec2_instances[0].public_dns } 6.5.1.3 providers.tf The providers.tf establishes an AWS provider configuration where the \"aws\" provider is defined with the AWS region specified by \"var.region\". This region configuration ensures that subsequent AWS resources and modules defined in the Terraform configuration will be created within the specified AWS region. # Provider provider &quot;aws&quot; { region = var.region } 6.5.1.4 terraform.tfvars The input provided in the terraform.tfvars file sets the value of the variable \"pem_key_name\". This configuration file is used to assign specific values to variables defined in the Terraform configuration. pem_key_name = &quot;terraform&quot; 6.5.1.5 variables.tf The variables.tf defines two variables: \"pem_key_name\" and \"region\". The \"pem_key_name\" variable is intended for specifying the name of an AWS PEM key within the AWS console and is of string data type. Meanwhile, the \"region\" variable is designed for specifying the AWS region and has a default value set to a default of \"eu-central-1\" if not explicitly specified. variable &quot;pem_key_name&quot; { description = &quot;Name of the pem key within AWS console&quot; type = string } variable &quot;region&quot; { description = &quot;AWS Region&quot; type = string default = &quot;eu-central-1&quot; } 6.5.2 vpc The \"vpc\" module is responsible for defining and configuring an Amazon Virtual Private Cloud (VPC) on AWS as part of the infrastructure provisioning process. 6.5.2.1 main.tf The provided Terraform code accomplishes two primary tasks. Firstly, it establishes a default Virtual Private Cloud (VPC) by defining an AWS resource of type aws_default_vpc named “default.” This action results in the creation of a default VPC within the AWS region corresponding to the Terraform configuration’s execution environment. Default VPCs are pre-configured VPCs automatically provided by AWS upon the creation of an AWS account. Secondly, the code proceeds to create an AWS security group resource designated as “aws_security_group” with the name specified by the variable var.security_group_name. This security group is given the description “allow ssh on 22 &amp; http on port 80.” It is associated with the previously created default VPC (aws_default_vpc.default.id). The security group’s inbound rules permit incoming traffic on port 22 (SSH) and port 80 (HTTP) from any source IP address (0.0.0.0/0). Additionally, an egress rule with unrestricted outbound access is configured to allow all traffic (protocol “-1”) from the security group. This security group definition is instrumental in governing the network access policies for associated AWS resources, such as EC2 instances. # Default VPC resource &quot;aws_default_vpc&quot; &quot;default&quot; {} # Security group resource &quot;aws_security_group&quot; &quot;nginx_demo&quot; { name = var.security_group_name description = &quot;allow ssh on 22 &amp; http on port 80&quot; vpc_id = aws_default_vpc.default.id ingress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } ingress { from_port = 80 to_port = 80 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } egress { from_port = 0 to_port = 0 protocol = &quot;-1&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } } 6.5.2.2 variables.tf The variables.tf defines only one variable names \"security_group_name\". This variable serves as a parameter that allows to customize and provide a name to the custom security group when using the Terraform configuration. variable &quot;security_group_name&quot; { type = string description = &quot;Name of the security group&quot; } 6.5.2.3 outputs.tf The provided outputs.tf defines an output named \"aws\\_security\\_group_id\" which captures and makes available the value of the created id attribute of the AWS security group resource with the identifier \"nginx_demo\", we created earlier. output &quot;aws_security_group_id&quot; { value = aws_security_group.nginx_demo.id } 6.5.3 Run the code As outlined in the section Basic usage, you can execute the Terraform code by following a sequence of Terraform CLI commands. Here’s a step-by-step guide to working with the code: Initialize Terraform (if you haven’t already done so) and downloads the necessary providers and modules. terraform init Validate the Configuration to check the syntax and validity of your Terraform configuration files. terraform validate Plan the Infrastructure and create an execution plan, displaying the changes that Terraform will make to your infrastructure. Review the plan to ensure everything is correct. terraform plan Apply the Configuration and confirm the changes by entering “yes” when prompted. terraform apply Testing NGINX Deployment After the terraform apply command successfully completes, you can access the NGINX deployment using the public DNS defined as an output in your Terraform script. Open the URL provided in a web browser to verify the NGINX server’s availability. Destroy Resources (After Testing) to clean up and remove all the resources created by Terraform. bash terraform destroy "],["ml-platform-design.html", "Chapter 7 ML Platform Design", " Chapter 7 ML Platform Design ML platforms can be set up in various ways to apply MLOps practices to the machine learning workflow. (1) SaaS tools provide an integrated development and management experience, with an aim to offer an end-to-end process. (2) Custom-made platforms offer high flexibility and can be tailored to specific needs. However, integrating multiple different services requires significant engineering effort. (3) Many cloud providers offer a mix of SaaS and custom-tailored platforms, providing a relatively well-integrated experience while remaining open enough to integrate other services. Overview This project involves building a custom-tailored ML platform focused on MLOps engineering, as the entire infrastructure will be set up from scratch. An exemplary ML platform will be developed using Airflow and MLflow for management during the machine learning lifecycle and JupyterHub to provide an integrated development environment. Even though there are workflow tools more specifically designed for machine learning pipelines, for example Kubeflow Pipelines, Airflow and MLflow can leverage and an combine there functionalities to provide similar capabilites. Airflow provides the workflow management for the platform whilst MLflow is used for machine learning tracking. MLflow further allow to register each model effortlessly. As an MLOps plattform should also provide an environment to develop machine learning model code, JupyterHub will be deployed to be able to develop code in the cloud and without the need for a local setup. The coding environment will synchronize with Airflow’s DAG repository to seamlessly integrate the defined models within the workflow management. Airflow and MLflow are very flexible with their running environment and their stack would be very suitable for small scale systems, where there is no need for a setup maintaining a Kubernetes cluster. While it would be possible to run anything on a docker/docker-compose setup, this work will scale the mentioned tools to a Kubernetes cluster in the cloud to fully enable the concept of an MLOps plattform. The infrastructure will be maintained using the Infrastructure as Code tool Terraform, and incorporate best Ops practices such as CI/CD and automation. The project will also incorporate the work done by data and machine learning scientists since basic machine learning models will be implemented and run on the platform. Infrastructure The necessary AWS infrastructure is set up using Terraform. This includes creating an AWS EKS cluster and the associated ressources like a virtual private cloud (VPC), subnets, security groups, IAM roles, as well as further AWS ressources needed to deploy custom modules. Networking is handled by AWS Application Load Balancing service or Ingress controller to route traffic to the correct service/pod in the cluster. Once the EKS cluster is set up, Kubernetes can be used to deploy and manage applications on the cluster. Helm, a package manager for Kubernetes, is used to manage the deployment of Airflow and MLflow. The EKS cluster allows for easy scalability and management of the platforms. The code is made public on a Github repository and Github Actions is used for automating the deployment of the infrastructure using CI/CD principles. MLOps Tools Once the infrastructure is set up, machine learning models can be trained on the EKS cluster as Kubernetes pods, using Airflows scheduling processes. Airflow’s ability to scan local directories or Git repositories will be used to import the relevant machine learning code from second Github repository. Similarly, to building Airflow workflows, the machine learning code will also include using the MLFlow API to allow for model tracking and storage. Github Actions is used as a CI/CD pipeline to automatically build, test, and deploy the machine learning model code to this repository similarly as it is used in the repository for the infrastructure. Model serving is done via Sagemaker, which allows for automatic scalability and seamlessly integrates with MLflow. Monitoring and logging is achieved using Prometheus &amp; Grafana to monitor the health and performance of the EKS cluster and its components, such as worker nodes, Kubernetes pods, etc and similarly for monitoring the deployed models as applications. Whereas the deployment of the infrastructure would be taken care of by MLOps-, DevOps-, and Data Engineers, the development of the Airflow workflows including MLFlow would be taken care of by Data Scientist and ML Engineers. "],["platform-deployment.html", "Chapter 8 Platform Deployment", " Chapter 8 Platform Deployment The provided directory structure represents the Terraform project for managing the infrastructure of our ML platform. It follows a modular organization to promote reusability and maintainability of the codebase. The full codebase is also available and can be accessed on github root │ main.tf │ variables.tf │ outputs.tf │ providers.tf │ └── infrastructure │ │ │ └── vpc │ │ │ └── eks │ │ │ └── networking │ │ │ └── rds │ └── modules │ └── airflow │ └── mlflow │ └── jupyterhub By structuring the Terraform project this way it becomes easier to manage, scale, and maintain the infrastructure as the project grows. Each module can be independently developed, tested, and reused across different projects, promoting consistency and reducing duplication of code and effort. Root The root directory of the Terraform project contains the general configuration files related to the overall infrastructure setup. The main.tf Terraform configuration file, where all major resources are defined and organized into modules. The variables.tf containing the definition of input variables used throughout the project, allowing users to customize the infrastructure setup. The outputs.tf defining the output variables that expose relevant information about the deployed infrastructure. The providers.tf that defining and configuring the providers used in the project, for example, AWS, Kubernetes, Helm. Infrastructure The infrastructure directory holds the individual modules responsible for provisioning specific components of the AWS Cloud and EKS setup. vpc defines a module that configures resources related to the Virtual Private Cloud (VPC), such as subnets, route tables, and internet gateways. The eks module is responsible for creating and configuring an Amazon Elastic Kubernetes Service (EKS) cluster, including worker nodes and other related resources like the Cluster Autoscaler, Elastic Block Storage, or Elastic File System. networking contains networking components that provide access to the cluster using ingresses and DNS records, for example the AWS Application Load Balancer or an External DNS. The rds module provides resources to deploy and Amazon Relational Database Service (RDS), such as database instances, subnets, and security groups. This module is needed for the specific tools and components of our ML platform. Modules The modules directory contains Terraform modules that are specific for setting up out ML Platform and provides the components to integrate the MLOps Framework, such as tools for model tracking (MLflow), workflow management (Airflow), or a integrated development environment (JupyterHub). airflow provides the Terraform module to deploy an Apache Airflow instance based on the Helm provider, which enables to orchestrate our ML workflows. The module is highly customized as it sets up necessary connections to other services, sets airflow variables that can be used by Data Scientists, creates an ingress ressource, and enables user management and authentication using Github. The mlflow module sets up MLflow to managing machine learning experiments and models. As MLflow does not natively provide a solution to deploy on Kubernetes, a custom Helm deployment is integrated that configures the necessary deployment, services, and ingress ressources. jupyterhub deploys a JupyterHub environment via Helm that enables multi-user notebook environment, suitable for collaborative data science and machine learning work. The Helm chart is highly customized providing user management and authentication via Github, provisioning ingress resources, and cloning a custom Github repository that provides all our Data Science and Machine Learning code. Prerequisites &amp; Installation The installation and deployment process involves several key steps to ensure a smooth setup of your environment. Before proceeding with the installation, it’s crucial to complete the following essential prerequisites: installing the necessary tools, establishing a GitHub organization along with OAuth apps, and obtaining a DNS name to link with your services. Please adhere to the installation instructions of the repositories’ readme document. "],["root-module.html", "8.1 Root module", " 8.1 Root module The root directory of the Terraform infrastructure consists of the main module, calling other submodules that deploy specific infrastructure setting or tools. This enables to have an overview about the deployment in one place. At first, the necessary cluster infrastructure is deployed such as the vpc and the eks cluster itself. Afterward the custom tools to be run on EKS are deployed, such as airflow, mlflow, and jupyterhub. The following will look a bit more detailed into the call of the module airflow, yet a lot of it also applies for the other modules. The module imports are structure in three parts. At first the general information about the module are given, such as name of the module, or the cluster_name, as well as more specific variables needed for specific Terraform calls in the module, like cluster_endpoint. Terraform does not provide the functionality to activate or deactivate a module by itself. As this is a useful feature, a custom workaround is proposed by setting the count a module as such count = var.deploy_airflow ? 1 : 0. This will set the cound of the module to 0 or 1, depending on the var.deploy_airflow variable. This functionality is proposed for all custom modules. Secondly, as Airflow needs access to and RDS Database, the RDS module is called. Therefore it is needed to pass the relevant information to create the the RDS under the correct settings, like vpc_id, rds_engine, or storage_type. Third, variable values for the Airflow Helm chart are passed to the module. Using Helm makes the deployment of Airflow very easy. Since there are customizations on the deployment, such as a connection to the Airflow DAG repository on Github, it is necessary to specify these information beforehand, and to integrate them into the deployment. locals { cluster_name = &quot;${var.name_prefix}-eks&quot; vpc_name = &quot;${var.name_prefix}-vpc&quot; port_airflow = var.port_airflow port_mlflow = var.port_mlflow mlflow_s3_bucket_name = &quot;${var.name_prefix}-mlflow-bucket&quot; force_destroy_s3_bucket = true storage_type = &quot;gp2&quot; max_allocated_storage = var.max_allocated_storage airflow_github_ssh = var.airflow_github_ssh git_username = var.git_username git_token = var.git_token git_repository_url = var.git_repository_url git_branch = var.git_branch } data &quot;aws_caller_identity&quot; &quot;current&quot; {} # INFRASTRUCTURE module &quot;vpc&quot; { source = &quot;./infrastructure/vpc&quot; cluster_name = local.cluster_name vpc_name = local.vpc_name } module &quot;eks&quot; { source = &quot;./infrastructure/eks&quot; cluster_name = local.cluster_name eks_cluster_version = &quot;1.23&quot; vpc_id = module.vpc.vpc_id private_subnets = module.vpc.private_subnets security_group_id_one = [module.vpc.worker_group_mgmt_one_id] security_group_id_two = [module.vpc.worker_group_mgmt_two_id] depends_on = [ module.vpc ] } # CUSTOM TOOLS module &quot;airflow&quot; { count = var.deploy_airflow ? 1 : 0 source = &quot;./modules/airflow&quot; name = &quot;airflow&quot; cluster_name = local.cluster_name cluster_endpoint = module.eks.cluster_endpoint # RDS vpc_id = module.vpc.vpc_id private_subnets = module.vpc.private_subnets private_subnets_cidr_blocks = module.vpc.private_subnets_cidr_blocks rds_port = local.port_airflow rds_name = &quot;airflow&quot; rds_engine = &quot;postgres&quot; rds_engine_version = &quot;13.3&quot; rds_instance_class = &quot;db.t3.micro&quot; storage_type = local.storage_type max_allocated_storage = local.max_allocated_storage # HELM helm_chart_repository = &quot;https://airflow-helm.github.io/charts&quot; helm_chart_name = &quot;airflow&quot; helm_chart_version = &quot;8.6.1&quot; git_username = local.git_username git_token = local.git_token git_repository_url = local.git_repository_url git_branch = local.git_branch depends_on = [ module.eks ] } module &quot;mlflow&quot; { count = var.deploy_mlflow ? 1 : 0 source = &quot;./modules/mlflow&quot; name = &quot;mlflow&quot; mlflow_s3_bucket_name = local.mlflow_s3_bucket_name s3_force_destroy = local.force_destroy_s3_bucket # RDS vpc_id = module.vpc.vpc_id private_subnets = module.vpc.private_subnets private_subnets_cidr_blocks = module.vpc.private_subnets_cidr_blocks rds_port = local.port_mlflow rds_name = &quot;mlflow&quot; rds_engine = &quot;mysql&quot; rds_engine_version = &quot;8.0.30&quot; rds_instance_class = &quot;db.t3.micro&quot; storage_type = local.storage_type max_allocated_storage = local.max_allocated_storage depends_on = [ module.eks ] } module &quot;jupyterhub&quot; { count = var.deploy_jupyterhub ? 1 : 0 source = &quot;./modules/jupyterhub&quot; name = &quot;jupyterhub&quot; cluster_name = local.cluster_name cluster_endpoint = module.eks.cluster_endpoint # HELM helm_chart_repository = &quot;https://jupyterhub.github.io/helm-chart/&quot; helm_chart_name = &quot;jupyterhub&quot; helm_chart_version = &quot;2.0.0&quot; depends_on = [ module.eks ] } "],["infrastructure-2.html", "8.2 Infrastructure", " 8.2 Infrastructure The subdirectory infrastructure consists of four main modules, vpc, eks, networking, and rds. The former three are responsible to create the cluster itself, as well as the necessary tools to implement the platform functionalities. The rds module is merely an extension linked to the cluster which is needed to store data of tools like Airflow or Mlflow. The rds module is thereby called in the corresponding modules where an AWS RDS is needed, even though the module is placed in the Infrastructure directory. 8.2.1 Virtual Private Cloud The provided code in the vpc module establishes a Virtual Private Cloud (VPC) with associated subnets and security groups. It configures the required networking and security infrastructure to serve as the foundation to deploy an AWS EKS cluster. The VPC is created using the terraform-aws-modules/vpc/aws module version 5.0.0. The VPC is assigned the IPv4 CIDR block \"10.0.0.0/16\" and spans across all three available AWS availability zones within the specified region eu-central-1. It includes both public and private subnets, with private subnets associated with NAT gateways for internet access. DNS hostnames are enabled for the instances launched within the VPC. The VPC subnets are tagged with specific metadata relevant to Kubernetes cluster management. The public subnets are tagged with \"kubernetes.io/cluster/${local.cluster_name}\" set to \"shared\" and \"kubernetes.io/role/elb\" set to 1. The private subnets are tagged with \"kubernetes.io/cluster/${local.cluster_name}\" set to \"shared\" and \"kubernetes.io/role/internal-elb\" set to 1. Additionally, three security groups are defined to manage access to worker nodes. They are intended to provide secure management access to the worker nodes within the EKS cluster. Two of these security groups, \"worker_group_mgmt_one\" and \"worker_group_mgmt_two\", allow SSH access from specific CIDR blocks. The third security group, \"all_worker_mgmt,\" allows SSH access from multiple CIDR blocks, including \"10.0.0.0/8\", \"172.16.0.0/12\", \"192.168.0.0/16\" locals { cluster_name = var.cluster_name } data &quot;aws_availability_zones&quot; &quot;available&quot; {} module &quot;vpc&quot; { source = &quot;terraform-aws-modules/vpc/aws&quot; version = &quot;5.0.0&quot; name = var.vpc_name cidr = &quot;10.0.0.0/16&quot; azs = slice(data.aws_availability_zones.available.names, 0, 3) private_subnets = [&quot;10.0.1.0/24&quot;, &quot;10.0.2.0/24&quot;, &quot;10.0.3.0/24&quot;] public_subnets = [&quot;10.0.4.0/24&quot;, &quot;10.0.5.0/24&quot;, &quot;10.0.6.0/24&quot;] enable_nat_gateway = true single_nat_gateway = true enable_dns_hostnames = true public_subnet_tags = { &quot;kubernetes.io/cluster/${local.cluster_name}&quot; = &quot;shared&quot; &quot;kubernetes.io/role/elb&quot; = 1 } private_subnet_tags = { &quot;kubernetes.io/cluster/${local.cluster_name}&quot; = &quot;shared&quot; &quot;kubernetes.io/role/internal-elb&quot; = 1 } } resource &quot;aws_security_group&quot; &quot;worker_group_mgmt_one&quot; { name_prefix = &quot;worker_group_mgmt_one&quot; vpc_id = module.vpc.vpc_id ingress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [ &quot;10.0.0.0/8&quot;, ] } } resource &quot;aws_security_group&quot; &quot;worker_group_mgmt_two&quot; { name_prefix = &quot;worker_group_mgmt_two&quot; vpc_id = module.vpc.vpc_id ingress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [ &quot;192.168.0.0/16&quot;, ] } } resource &quot;aws_security_group&quot; &quot;all_worker_mgmt&quot; { name_prefix = &quot;all_worker_management&quot; vpc_id = module.vpc.vpc_id ingress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [ &quot;10.0.0.0/8&quot;, &quot;172.16.0.0/12&quot;, &quot;192.168.0.0/16&quot;, ] } } 8.2.2 Elastic Kubernetes Service The provided Terraform code sets up an AWS EKS (Elastic Kubernetes Service) cluster with specific configurations and multiple node groups. The \"eks\" module is used to create the EKS cluster, specifying its name and version. The cluster has public and private access endpoints enabled, and a managed AWS authentication configuration. The creation of the vpc module is a prerequisite for the \"eks\" module, as the latter requires information like the vpc_id, or subnet_ids for a successful creation. The EKS cluster itself is composed of three managed node groups: \"group_t3_small\", \"group_t3_medium\", and \"group_t3_large\". Each node group uses a different instance type (t3.small, t3.medium, and t3.large) and has specific scaling policies. All three node groups have auto-scaling enabled. The node group \"group_t3_medium\" has set the minimum and desired sizes of nodes to 4, which ensures a base amount of nodes and thus resources to manage further deployments. The \"group_t3_large\" is tainted with a NoSchedule. This node group can be used for more resource intensive tasks by specifiyng a pod’s toleration. The eks module also deploys several Kubernetes add-ons, including coredns, kube-proxy, aws-ebs-csi-driver, and vpc-cni. The vpc-cni add-on is configured with specific environment settings, enabling prefix delegation for IP addresses. CoreDNS provides DNS-based service discovery, allowing pods and services to communicate with each other using domain names, and thus enabling seamless communication within the cluster without the need for explicit IP addresses. kube-proxy: is responsible for network proxying on Kubernetes nodes which ensures that network traffic is properly routed to the appropriate pods, services, and endpoints. It allows for an seamless communication between different parts of the cluster. aws-ebs-csi-driver(Container Storage Interface) is an add-on that enables Kubernetes pods to use Amazon Elastic Block Store (EBS) volumes for persistent storage, allowing data to be retained across pod restarts and ensuring data durability for stateful applications. The EBS configuration and deployment are describen in the following subsection Elastic Block Store, but the respective service_account_role_arn is linked to the EKS cluster on creation. vpc-cni (Container Network Interface) is essential for AWS EKS clusters, as it enables networking for pods using AWS VPC (Virtual Private Cloud) networking. It ensures that each pod gets an IP address from the VPC subnet and can communicate securely with other AWS resources within the VPC. locals { cluster_name = var.cluster_name cluster_namespace = &quot;kube-system&quot; ebs_csi_service_account_name = &quot;ebs-csi-controller-sa&quot; ebs_csi_service_account_role_name = &quot;${var.cluster_name}-ebs-csi-controller&quot; autoscaler_service_account_name = &quot;autoscaler-controller-sa&quot; autoscaler_service_account_role_name = &quot;${var.cluster_name}-autoscaler-controller&quot; nodegroup_t3_small_label = &quot;t3_small&quot; nodegroup_t3_medium_label = &quot;t3_medium&quot; nodegroup_t3_large_label = &quot;t3_large&quot; eks_asg_tag_list_nodegroup_t3_small_label = { &quot;k8s.io/cluster-autoscaler/enabled&quot; : true &quot;k8s.io/cluster-autoscaler/${local.cluster_name}&quot; : &quot;owned&quot; &quot;k8s.io/cluster-autoscaler/node-template/label/role&quot; : local.nodegroup_t3_small_label } eks_asg_tag_list_nodegroup_t3_medium_label = { &quot;k8s.io/cluster-autoscaler/enabled&quot; : true &quot;k8s.io/cluster-autoscaler/${local.cluster_name}&quot; : &quot;owned&quot; &quot;k8s.io/cluster-autoscaler/node-template/label/role&quot; : local.nodegroup_t3_medium_label } eks_asg_tag_list_nodegroup_t3_large_label = { &quot;k8s.io/cluster-autoscaler/enabled&quot; : true &quot;k8s.io/cluster-autoscaler/${local.cluster_name}&quot; : &quot;owned&quot; &quot;k8s.io/cluster-autoscaler/node-template/label/role&quot; : local.nodegroup_t3_large_label &quot;k8s.io/cluster-autoscaler/node-template/taint/dedicated&quot; : &quot;${local.nodegroup_t3_large_label}:NoSchedule&quot; } tags = { Owner = &quot;terraform&quot; } } data &quot;aws_caller_identity&quot; &quot;current&quot; {} # # EKS # module &quot;eks&quot; { source = &quot;terraform-aws-modules/eks/aws&quot; version = &quot;19.5.1&quot; cluster_name = local.cluster_name cluster_version = var.eks_cluster_version cluster_enabled_log_types = [&quot;api&quot;, &quot;controllerManager&quot;, &quot;scheduler&quot;] vpc_id = var.vpc_id subnet_ids = var.private_subnets cluster_endpoint_private_access = true cluster_endpoint_public_access = true manage_aws_auth_configmap = true # aws_auth_users = local.cluster_users # add users in later step cluster_addons = { coredns = { most_recent = true }, kube-proxy = { most_recent = true }, aws-ebs-csi-driver = { service_account_role_arn = &quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.ebs_csi_service_account_role_name}&quot; }, vpc-cni = { most_recent = true before_compute = true service_account_role_arn = module.vpc_cni_irsa.iam_role_arn configuration_values = jsonencode({ env = { # Reference docs https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html ENABLE_PREFIX_DELEGATION = &quot;true&quot; WARM_PREFIX_TARGET = &quot;1&quot; } }) } } eks_managed_node_group_defaults = { ami_type = &quot;AL2_x86_64&quot; disk_size = 10 iam_role_attach_cni_policy = true enable_monitoring = true } eks_managed_node_groups = { group_t3_small = { name = &quot;ng0_t3_small&quot; instance_types = [&quot;t3.small&quot;] min_size = 0 max_size = 6 desired_size = 0 capacity_type = &quot;ON_DEMAND&quot; labels = { role = local.nodegroup_t3_small_label } tags = { &quot;k8s.io/cluster-autoscaler/enabled&quot; = &quot;true&quot; &quot;k8s.io/cluster-autoscaler/${local.cluster_name}&quot; = &quot;owned&quot; &quot;k8s.io/cluster-autoscaler/node-template/label/role&quot; = &quot;${local.nodegroup_t3_small_label}&quot; } } group_t3_medium = { name = &quot;ng1_t3_medium&quot; instance_types = [&quot;t3.medium&quot;] min_size = 4 max_size = 6 desired_size = 4 capacity_type = &quot;ON_DEMAND&quot; labels = { role = local.nodegroup_t3_medium_label } tags = { &quot;k8s.io/cluster-autoscaler/enabled&quot; = &quot;true&quot; &quot;k8s.io/cluster-autoscaler/${local.cluster_name}&quot; = &quot;owned&quot; &quot;k8s.io/cluster-autoscaler/node-template/label/role&quot; = &quot;${local.nodegroup_t3_medium_label}&quot; } } group_t3_large = { name = &quot;ng2_t3_large&quot; instance_types = [&quot;t3.large&quot;] min_size = 0 max_size = 3 desired_size = 0 capacity_type = &quot;ON_DEMAND&quot; labels = { role = local.nodegroup_t3_large_label } taints = [ { key = &quot;dedicated&quot; value = local.nodegroup_t3_large_label effect = &quot;NO_SCHEDULE&quot; } ] tags = { &quot;k8s.io/cluster-autoscaler/enabled&quot; = &quot;true&quot; &quot;k8s.io/cluster-autoscaler/${local.cluster_name}&quot; = &quot;owned&quot; &quot;k8s.io/cluster-autoscaler/node-template/label/role&quot; = &quot;${local.nodegroup_t3_large_label}&quot; &quot;k8s.io/cluster-autoscaler/node-template/taint/dedicated&quot; = &quot;${local.nodegroup_t3_large_label}:NoSchedule&quot; } } } tags = local.tags } # Role for Service Account module &quot;vpc_cni_irsa&quot; { source = &quot;terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks&quot; version = &quot;~&gt; 5.0&quot; role_name_prefix = &quot;VPC-CNI-IRSA&quot; attach_vpc_cni_policy = true vpc_cni_enable_ipv4 = true oidc_providers = { main = { provider_arn = module.eks.oidc_provider_arn namespace_service_accounts = [&quot;kube-system:aws-node&quot;] } } } 8.2.2.1 Elastic Block Store The EBS CSI controller (Elastic Block Store Container Storage Interface) is set up by defining an IAM (Identity and Access Management) role using the \"ebs_csi_controller_role\" module. The role allows the EBS CSI controller to assume a specific IAM role with OIDC (OpenID Connect) authentication, granting it the necessary permissions for EBS-related actions in the AWS environment by an IAM policy. The IAM policy associated with the role is created likewise and permits various EC2 actions, such as attaching and detaching volumes, creating and deleting snapshots, and describing instances and volumes. The code also configures the default Kubernetes StorageClass named \"gp2\" and annotates it as not the default storage class for the cluster, managing how storage volumes are provisioned and utilized in the cluster. Ensuring that the \"gp2\" StorageClass does not become the default storage class is needed as we additionally create an EFS Storage (Elastic File System), which is described in the next subsection. # # EBS CSI controller # module &quot;ebs_csi_controller_role&quot; { source = &quot;terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc&quot; version = &quot;5.11.1&quot; create_role = true role_name = local.ebs_csi_service_account_role_name provider_url = replace(module.eks.cluster_oidc_issuer_url, &quot;https://&quot;, &quot;&quot;) role_policy_arns = [aws_iam_policy.ebs_csi_controller_sa.arn] oidc_fully_qualified_subjects = [&quot;system:serviceaccount:${local.cluster_namespace}:${local.ebs_csi_service_account_name}&quot;] } resource &quot;aws_iam_policy&quot; &quot;ebs_csi_controller_sa&quot; { name = local.ebs_csi_service_account_name description = &quot;EKS ebs-csi-controller policy for cluster ${var.cluster_name}&quot; policy = jsonencode({ &quot;Version&quot; : &quot;2012-10-17&quot;, &quot;Statement&quot; : [ { &quot;Action&quot; : [ &quot;ec2:AttachVolume&quot;, &quot;ec2:CreateSnapshot&quot;, &quot;ec2:CreateTags&quot;, &quot;ec2:CreateVolume&quot;, &quot;ec2:DeleteSnapshot&quot;, &quot;ec2:DeleteTags&quot;, &quot;ec2:DeleteVolume&quot;, &quot;ec2:DescribeInstances&quot;, &quot;ec2:DescribeSnapshots&quot;, &quot;ec2:DescribeTags&quot;, &quot;ec2:DescribeVolumes&quot;, &quot;ec2:DetachVolume&quot;, ], &quot;Effect&quot; : &quot;Allow&quot;, &quot;Resource&quot; : &quot;*&quot; } ] }) } resource &quot;kubernetes_annotations&quot; &quot;ebs-no-default-storageclass&quot; { api_version = &quot;storage.k8s.io/v1&quot; kind = &quot;StorageClass&quot; force = &quot;true&quot; metadata { name = &quot;gp2&quot; } annotations = { &quot;storageclass.kubernetes.io/is-default-class&quot; = &quot;false&quot; } } 8.2.2.2 Elastic File System The EFS CSI (Elastic File System Container Storage Interface) driver permits EKS pods to use EFS as a persistent volume for data storage, enabling pods to use EFS as a scalable and shared storage solution.. The driver itself is deployed using a Helm chart through the \"helm_release\" resource. Of course we also need to create an IAM role for the EFS CSI driver, which is done using the \"attach_efs_csi_role\" module, which allows the driver to assume a role with OIDC authentication, and grants the necessary permissions for working with EFS, similar to the EBS setup. For security, the code creates an AWS security group named \"allow_nfs\" that allows inbound NFS traffic on port 2049 from the private subnets of the VPC. This allows the EFS mount targets to communicate with the EFS file system securely. The EFS file system and access points are created manually for each private subnet mapping the \"aws_efs_mount_target\" to the \"aws_efs_file_system\" resource. Finally, the code defines a Kubernetes StorageClass named \"efs\" using the \"kubernetes_storage_class_v1\" resource. The StorageClass specifies the EFS CSI driver as the storage provisioner and the EFS file system created earlier as the backing storage. Additionally, the \"efs\" StorageClass is marked as the default storage class for the cluster using an annotation. This allows dynamic provisioning of EFS-backed persistent volumes for Kubernetes pods on default, simplifying the process of handling storage in the EKS cluster. This is done for example for the Airflow deployment in a later step. # # EFS # resource &quot;helm_release&quot; &quot;aws_efs_csi_driver&quot; { chart = &quot;aws-efs-csi-driver&quot; name = &quot;aws-efs-csi-driver&quot; namespace = &quot;kube-system&quot; repository = &quot;https://kubernetes-sigs.github.io/aws-efs-csi-driver/&quot; set { name = &quot;controller.serviceAccount.create&quot; value = true } set { name = &quot;controller.serviceAccount.annotations.eks\\\\.amazonaws\\\\.com/role-arn&quot; value = module.attach_efs_csi_role.iam_role_arn } set { name = &quot;controller.serviceAccount.name&quot; value = &quot;efs-csi-controller-sa&quot; } } module &quot;attach_efs_csi_role&quot; { source = &quot;terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks&quot; role_name = &quot;efs-csi&quot; attach_efs_csi_policy = true oidc_providers = { ex = { provider_arn = module.eks.oidc_provider_arn namespace_service_accounts = [&quot;kube-system:efs-csi-controller-sa&quot;] } } } resource &quot;aws_security_group&quot; &quot;allow_nfs&quot; { name = &quot;allow nfs for efs&quot; description = &quot;Allow NFS inbound traffic&quot; vpc_id = var.vpc_id ingress { description = &quot;NFS from VPC&quot; from_port = 2049 to_port = 2049 protocol = &quot;tcp&quot; cidr_blocks = var.private_subnets_cidr_blocks } egress { from_port = 0 to_port = 0 protocol = &quot;-1&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] ipv6_cidr_blocks = [&quot;::/0&quot;] } } resource &quot;aws_efs_file_system&quot; &quot;stw_node_efs&quot; { creation_token = &quot;efs-for-stw-node&quot; } resource &quot;aws_efs_mount_target&quot; &quot;stw_node_efs_mt_0&quot; { file_system_id = aws_efs_file_system.stw_node_efs.id subnet_id = var.private_subnets[0] security_groups = [aws_security_group.allow_nfs.id] } resource &quot;aws_efs_mount_target&quot; &quot;stw_node_efs_mt_1&quot; { file_system_id = aws_efs_file_system.stw_node_efs.id subnet_id = var.private_subnets[1] security_groups = [aws_security_group.allow_nfs.id] } resource &quot;aws_efs_mount_target&quot; &quot;stw_node_efs_mt_2&quot; { file_system_id = aws_efs_file_system.stw_node_efs.id subnet_id = var.private_subnets[2] security_groups = [aws_security_group.allow_nfs.id] } resource &quot;kubernetes_storage_class_v1&quot; &quot;efs&quot; { metadata { name = &quot;efs&quot; annotations = { &quot;storageclass.kubernetes.io/is-default-class&quot; = &quot;true&quot; } } storage_provisioner = &quot;efs.csi.aws.com&quot; parameters = { provisioningMode = &quot;efs-ap&quot; # Dynamic provisioning fileSystemId = aws_efs_file_system.stw_node_efs.id # module.efs.id directoryPerms = &quot;777&quot; } mount_options = [ &quot;iam&quot; ] } 8.2.2.3 Cluster Autoscaler The EKS Cluster Autoscaler ensures that the cluster can automatically scale its worker nodes based on the workload demands, ensuring optimal resource utilization and performance. The necessary IAM settings are set up prior to deploying the Autoscaler. First, an IAM policy named \"node_additional\" is created to grant permission to describe EC2 instances and related resources. This enables the Autoscaler to gather information about the current state of the worker nodes and make informed decisions regarding scaling. For each managed node group in the EKS cluster (defined by the \"eks_managed_node_groups\" module output), the IAM policy is attached to its corresponding IAM role. This ensures that all worker nodes have the required permissions to work with the Autoscaler. After setting up the IAM policies, tags are added to provide the necessary information for the EKS Cluster Autoscaler to identify and manage the Auto Scaling Groups effectively and to support cluster autoscaling from zero for each node group. The tags are created for each node group (\"nodegroup_t3_small\", \"nodegroup_t3_medium\" ,and \"nodegroup_t3_large\") and are based on the specified tag lists defined in the \"local.eks_asg_tag_list_*\" variables. The EKS Cluster Autoscaler itself is instantiated using the custom \"eks_autoscaler\" module on the bottom of the code snippet. The module is called to set up the Autoscaler for the EKS cluster and the required input variables are provided accordingly. Its components are described in detailed in the following. # # EKS Cluster autoscaler # resource &quot;aws_iam_policy&quot; &quot;node_additional&quot; { name = &quot;${local.cluster_name}-additional&quot; description = &quot;${local.cluster_name} node additional policy&quot; policy = jsonencode({ Version = &quot;2012-10-17&quot; Statement = [ { Action = [ &quot;ec2:Describe*&quot;, ] Effect = &quot;Allow&quot; Resource = &quot;*&quot; }, ] }) } resource &quot;aws_iam_role_policy_attachment&quot; &quot;additional&quot; { for_each = module.eks.eks_managed_node_groups policy_arn = aws_iam_policy.node_additional.arn role = each.value.iam_role_name } # Tags for the ASG to support cluster-autoscaler scale up from 0 for nodegroup2 resource &quot;aws_autoscaling_group_tag&quot; &quot;nodegroup_t3_small&quot; { for_each = local.eks_asg_tag_list_nodegroup_t3_small_label autoscaling_group_name = element(module.eks.eks_managed_node_groups_autoscaling_group_names, 2) tag { key = each.key value = each.value propagate_at_launch = true } } resource &quot;aws_autoscaling_group_tag&quot; &quot;nodegroup_t3_medium&quot; { for_each = local.eks_asg_tag_list_nodegroup_t3_medium_label autoscaling_group_name = element(module.eks.eks_managed_node_groups_autoscaling_group_names, 1) tag { key = each.key value = each.value propagate_at_launch = true } } resource &quot;aws_autoscaling_group_tag&quot; &quot;nodegroup_t3_large&quot; { for_each = local.eks_asg_tag_list_nodegroup_t3_large_label autoscaling_group_name = element(module.eks.eks_managed_node_groups_autoscaling_group_names, 0) tag { key = each.key value = each.value propagate_at_launch = true } } module &quot;eks_autoscaler&quot; { source = &quot;./autoscaler&quot; cluster_name = local.cluster_name cluster_namespace = local.cluster_namespace aws_region = var.aws_region cluster_oidc_issuer_url = module.eks.cluster_oidc_issuer_url autoscaler_service_account_name = local.autoscaler_service_account_name } The configurationof the Cluster Autoscaler begins with the creation of a Helm release named \"cluster-autoscaler\" using the \"helm_release\" resource. The Helm chart is sourced from the \"kubernetes.github.io/autoscaler\" repository with the chart version \"9.10.7\". The settings inside the Helm release include the AWS region, RBAC (Role-Based Access Control) settings for the service account, cluster auto-discovery settings, and the creation of the service account with the required permissions. The necessary resources for the settings are created accordingly in the following. The service account is created using the \"iam_assumable_role_admin\" module with an assumable IAM role that allows the service account to access the necessary resources for scaling. It is associated with the OIDC (OpenID Connect) provider for the cluster to permit access. An IAM policy named \"cluster_autoscaler\" is created to permit the Cluster Autoscaler to interact with Auto Scaling Groups, EC2 instances, launch configurations, and tags. The policy includes two statements: \"clusterAutoscalerAll\" and \"clusterAutoscalerOwn\". The first statement grants read access to Auto Scaling Group-related resources, while the second statement allows the Cluster Autoscaler to modify the desired capacity of the Auto Scaling Groups and terminate instances. The policy also includes conditions to ensure that the Cluster Autoscaler can only modify resources with specific tags. The conditions check that the Auto Scaling Group has a tag \"k8s.io/cluster-autoscaler/enabled\" set to \"true\" and a tag \"k8s.io/cluster-autoscaler/&lt;cluster_name&gt;\" set to \"owned\". If you remember it, we have set these tags when setting up the managed node groups for the EKS Cluster in the previous step. resource &quot;helm_release&quot; &quot;cluster-autoscaler&quot; { name = &quot;cluster-autoscaler&quot; namespace = var.cluster_namespace repository = &quot;https://kubernetes.github.io/autoscaler&quot; chart = &quot;cluster-autoscaler&quot; version = &quot;9.10.7&quot; create_namespace = false set { name = &quot;awsRegion&quot; value = var.aws_region } set { name = &quot;rbac.serviceAccount.name&quot; value = var.autoscaler_service_account_name } set { name = &quot;rbac.serviceAccount.annotations.eks\\\\.amazonaws\\\\.com/role-arn&quot; value = module.iam_assumable_role_admin.iam_role_arn type = &quot;string&quot; } set { name = &quot;autoDiscovery.clusterName&quot; value = var.cluster_name } set { name = &quot;autoDiscovery.enabled&quot; value = &quot;true&quot; } set { name = &quot;rbac.create&quot; value = &quot;true&quot; } } module &quot;iam_assumable_role_admin&quot; { source = &quot;terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc&quot; version = &quot;~&gt; 4.0&quot; create_role = true role_name = &quot;cluster-autoscaler&quot; provider_url = replace(var.cluster_oidc_issuer_url, &quot;https://&quot;, &quot;&quot;) role_policy_arns = [aws_iam_policy.cluster_autoscaler.arn] oidc_fully_qualified_subjects = [&quot;system:serviceaccount:${var.cluster_namespace}:${var.autoscaler_service_account_name}&quot;] } resource &quot;aws_iam_policy&quot; &quot;cluster_autoscaler&quot; { name_prefix = &quot;cluster-autoscaler&quot; description = &quot;EKS cluster-autoscaler policy for cluster ${var.cluster_name}&quot; policy = data.aws_iam_policy_document.cluster_autoscaler.json } data &quot;aws_iam_policy_document&quot; &quot;cluster_autoscaler&quot; { statement { sid = &quot;clusterAutoscalerAll&quot; effect = &quot;Allow&quot; actions = [ &quot;autoscaling:DescribeAutoScalingGroups&quot;, &quot;autoscaling:DescribeAutoScalingInstances&quot;, &quot;autoscaling:DescribeLaunchConfigurations&quot;, &quot;autoscaling:DescribeTags&quot;, &quot;ec2:DescribeLaunchTemplateVersions&quot;, ] resources = [&quot;*&quot;] } statement { sid = &quot;clusterAutoscalerOwn&quot; effect = &quot;Allow&quot; actions = [ &quot;autoscaling:SetDesiredCapacity&quot;, &quot;autoscaling:TerminateInstanceInAutoScalingGroup&quot;, &quot;autoscaling:UpdateAutoScalingGroup&quot;, ] resources = [&quot;*&quot;] condition { test = &quot;StringEquals&quot; variable = &quot;autoscaling:ResourceTag/k8s.io/cluster-autoscaler/${var.cluster_name}&quot; values = [&quot;owned&quot;] } condition { test = &quot;StringEquals&quot; variable = &quot;autoscaling:ResourceTag/k8s.io/cluster-autoscaler/enabled&quot; values = [&quot;true&quot;] } } } 8.2.3 Networking The networking module of the infrastructure directory integrates an Application Load Balancer (ALB) and External DNS in the cluster. Both play crucial roles in managing and exposing Kubernetes applications within the EKS cluster to the outside world. The ALB serves as an Ingress Controller to route external traffic to Kubernetes services, while External DNS automates the management of DNS records, making it easier to access services using user-friendly domain names. The root module of network just calls both submodules, which are described in detail in the upcoming sections. module &quot;external-dns&quot; { ... } module &quot;application-load-balancer&quot; { ... } 8.2.3.1 AWS Application Load Balancer (ALB) The ALB is a managed load balancer service provided by AWS. In the context of an EKS cluster, the ALB serves as an Ingress Controller and thus is responsible for routing external traffic to the appropriate services and pods running inside your Kubernetes cluster. The ALB acts as the entry point to our applications and enables us to expose multiple services over a single public IP address or domain name, which simplifies access for users and clients. The code starts by defining some local variables, followed by creating an assumable IAM role for the AWS Load Balancer Controller service account by the module aws_load_balancer_controller_controller_role. The service account holds the necessary permissions and associates with the OIDC provider of the EKS cluster as it is the same module call we already used multiple times beforehand. The IAM policy for the role is defined in the \"aws_iam_policy.aws_load_balancer_controller_controller_sa\" resource. Since its policy document is quite extensive, it is loaded from a file named \"AWSLoadBalancerControllerPolicy.json.“. In summary, the AWS IAM document allows the AWS Elastic Load Balancing (ELB) controller, specifically the Elastic Load Balancer V2 (ELBV2) API, to perform various actions related to managing load balancers, target groups, listeners, rules, and tags. The document includes several”Allow” statements that grant permissions for actions like describing and managing load balancers, target groups, listeners, and rules. It also allows the controller to create and delete load balancers, target groups, and listeners, as well as modify their attributes. Additionally, the document permits the addition and removal of tags for ELBV2 resources. After setting up the IAM role, the code proceeds to install the AWS Load Balancer Controller using Helm. The Helm chart is sourced from the \"aws.github.io/eks-charts\" repository, specifying version \"v2.4.2\". The service account configuration is provided to the Helm release’s values, including the name of the service account and annotations to associate it with the IAM role created earlier. The \"eks.amazonaws.com/role-arn\" annotation points to the ARN of the IAM role associated with the service account, allowing the controller to assume that role and operate with the appropriate permissions. locals { aws_load_balancer_controller_service_account_role_name = &quot;aws-load-balancer-controller-role&quot; aws_load_balancer_controller_service_account_name = &quot;aws-load-balancer-controller-sa&quot; } data &quot;aws_caller_identity&quot; &quot;current&quot; {} data &quot;aws_region&quot; &quot;current&quot; {} # module &quot;aws_load_balancer_controller_controller_role&quot; { source = &quot;terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc&quot; version = &quot;5.11.1&quot; create_role = true role_name = local.aws_load_balancer_controller_service_account_role_name provider_url = replace(var.cluster_oidc_issuer_url, &quot;https://&quot;, &quot;&quot;) role_policy_arns = [aws_iam_policy.aws_load_balancer_controller_controller_sa.arn] oidc_fully_qualified_subjects = [&quot;system:serviceaccount:kube-system:${local.aws_load_balancer_controller_service_account_name}&quot;] } resource &quot;aws_iam_policy&quot; &quot;aws_load_balancer_controller_controller_sa&quot; { name = local.aws_load_balancer_controller_service_account_name description = &quot;EKS ebs-csi-controller policy for cluster ${var.cluster_name}&quot; policy = file(&quot;${path.module}/AWSLoadBalancerControllerPolicy.json&quot;) } resource &quot;helm_release&quot; &quot;aws-load-balancer-controller&quot; { name = var.helm_chart_name namespace = var.namespace chart = &quot;aws-load-balancer-controller&quot; create_namespace = false repository = &quot;https://aws.github.io/eks-charts&quot; version = var.helm_chart_version values = [yamlencode({ clusterName = var.cluster_name image = { tag = &quot;v2.4.2&quot; }, serviceAccount = { name = &quot;${local.aws_load_balancer_controller_service_account_name}&quot; annotations = { &quot;eks.amazonaws.com/role-arn&quot; = &quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.aws_load_balancer_controller_service_account_role_name}&quot; } } })] } 8.2.3.2 External DNS External DNS is a Kubernetes add-on that automates the creation and management of DNS records for Kubernetes services. It is particularly useful when services are exposed to the internet through the ALB or any other Ingress Controller. When an Ingress resource is created that defines how external traffic should be routed to services within the EKS cluster, External DNS automatically updates the DNS provider with the corresponding DNS records (in our case this is Route 53 in AWS). Automatically configuring DNS records ensures that the records are always up-to-date, which helps maintain consistency and reliability in the DNS configuration, and users can access the Kubernetes services using user-friendly domain names rather than relying on IP addresses. The code is structured similar to the ALB and defines local variables first, followed by creating a service account to interact with AWS resources. The service account, its role with OIDC and the policy with relevant permissions are created by the external_dns_controller_role module same to as we know it from previous implementations. The policy allows the external DNS controller to operate within the specified AWS Route 53 hosted zone, such as changing resource record sets, and listing hosted zones and resource record sets. Finally, the Helm is used to to deploy the external DNS controller as a Kubernetes resource. The Helm release configuration includes specifying the previously create service account, the IAM role-arn associated with it, the aws.region where the Route 53 hosted zone exists, and a domainFilter which filters to a specific domain provided by us. locals { external_dns_service_account_role_name = &quot;external-dns-role&quot; external_dns_service_account_name = &quot;external-dns-sa&quot; } data &quot;aws_caller_identity&quot; &quot;current&quot; {} data &quot;aws_region&quot; &quot;current&quot; {} # module &quot;external_dns_controller_role&quot; { source = &quot;terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc&quot; version = &quot;5.11.1&quot; create_role = true role_name = local.external_dns_service_account_role_name provider_url = replace(var.cluster_oidc_issuer_url, &quot;https://&quot;, &quot;&quot;) role_policy_arns = [aws_iam_policy.external_dns_controller_sa.arn] oidc_fully_qualified_subjects = [&quot;system:serviceaccount:${var.namespace}:${local.external_dns_service_account_name}&quot;] } resource &quot;aws_iam_policy&quot; &quot;external_dns_controller_sa&quot; { name = local.external_dns_service_account_name description = &quot;EKS ebs-csi-controller policy for cluster ${var.cluster_name}&quot; policy = jsonencode({ &quot;Version&quot; : &quot;2012-10-17&quot;, &quot;Statement&quot; : [ { &quot;Effect&quot; : &quot;Allow&quot;, &quot;Action&quot; : [ &quot;route53:ChangeResourceRecordSets&quot; ], &quot;Resource&quot; : [ &quot;arn:aws:route53:::hostedzone/*&quot; ] }, { &quot;Effect&quot; : &quot;Allow&quot;, &quot;Action&quot; : [ &quot;route53:ListHostedZones&quot;, &quot;route53:ListResourceRecordSets&quot; ], &quot;Resource&quot; : [ &quot;*&quot; ] } ] }) } resource &quot;helm_release&quot; &quot;external_dns&quot; { name = var.name namespace = var.namespace chart = var.helm_chart_name create_namespace = false repository = &quot;https://charts.bitnami.com/bitnami&quot; version = var.helm_chart_version values = [yamlencode({ serviceAccount = { create = true name = &quot;${local.external_dns_service_account_name}&quot; annotations = { &quot;eks.amazonaws.com/role-arn&quot; = &quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.external_dns_service_account_role_name}&quot; } }, aws = { zoneType = &quot;public&quot; region = &quot;${data.aws_region.current.name}&quot; }, policy = &quot;sync&quot; domainFilter = [ &quot;${var.domain_name}&quot; ] provider = &quot;aws&quot; txtOwnerId = &quot;${var.name}&quot; })] } 8.2.4 Relational Database Service The Amazon RDS (Relational Database Service) instance is provisioned by the aws_db_instance resource. It configures the instance with the specified settings, such as allocated_storage, storage_type, engine, db_name, username, and password, etc. All these parameters are provided whenever the module is invoked, e.g. in the Airflow or Mlflow modules.. The skip_final_snapshot set to true states that no final DB snapshot will be created when the instance is deleted. The resource aws_db_subnet_group creates an RDS subnet group with the name \"vpc-subnet-group-${local.rds_name}\". It associates the RDS instance with the private subnets specified in the VPC module, and is used to define the subnets in which the RDS instance can be launched. Similar to the subnet group, the RDS instance uses an own security group. The security group aws_security_group is attached to the RDS instance. It specifies ingress (inbound)) and egress (outbound) rules to control network traffic. In this case, it allows inbound access on the specified port used by the RDS engine (5432 for PostgreSQL) from the CIDR blocks specified in the private_subnets_cidr_blocks, and allows all outbound traffic (0.0.0.0/0) from the RDS instance. The rds module is not necessarily needed to run a kubernetes cluster properly. It is merely an extension of the cluster and is needed to store relevant data of the tools used, such as airflow or mlflow. The module is thus called directly from the own airflow and mlflow modules. locals { rds_name = var.rds_name rds_engine = var.rds_engine rds_engine_version = var.rds_engine_version rds_port = var.rds_port } resource &quot;aws_db_subnet_group&quot; &quot;default&quot; { name = &quot;vpc-subnet-group-${local.rds_name}&quot; subnet_ids = var.private_subnets } resource &quot;aws_db_instance&quot; &quot;rds_instance&quot; { allocated_storage = var.max_allocated_storage storage_type = var.storage_type engine = local.rds_engine engine_version = local.rds_engine_version instance_class = var.rds_instance_class db_name = &quot;${local.rds_name}_db&quot; username = &quot;${local.rds_name}_admin&quot; password = var.rds_password identifier = &quot;${local.rds_name}-${local.rds_engine}&quot; port = local.rds_port vpc_security_group_ids = [aws_security_group.rds_sg.id] db_subnet_group_name = aws_db_subnet_group.default.name skip_final_snapshot = true } resource &quot;aws_security_group&quot; &quot;rds_sg&quot; { name = &quot;${local.rds_name}-${local.rds_engine}-sg&quot; vpc_id = var.vpc_id ingress { description = &quot;Enable postgres access&quot; from_port = local.rds_port to_port = local.rds_port protocol = &quot;tcp&quot; cidr_blocks = var.private_subnets_cidr_blocks } egress { from_port = 0 to_port = 0 protocol = &quot;-1&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } } "],["components.html", "8.3 Components", " 8.3 Components In this configuration, several custom modules are in place, including Airflow, MLflow, JupyterHub, and monitoring. Each module has a distinct role in deploying specific workflow tools. Additionally, these module names correspond to their respective namespaces within the cluster. 8.3.1 User Profiles The code within the \"User-profiles\" module serves the purpose of configuring IAM (Identity and Access Management) policies and roles tailored for an ML (Machine Learning) platform. It involves the creation of AWS IAM user profiles, each linked to specific users and their corresponding access keys. These users are granted access privileges determined by specific policies, which can be designated as either user or developer access levels. All pertinent information is securely stored within AWS Secrets Manager. The list of users is dynamically provided to the module through the var.profiles variable from the root modules. At the outset of the code, data sources are defined to retrieve essential information about the AWS caller’s identity and the current AWS region. These data sources, namely aws_caller_identity and aws_region, serve as repositories for critical details that may be utilized in subsequent configurations. Furthermore, the code introduces an AWS managed policy named “AmazonSageMakerFullAccess” using the aws_iam_policy data source. This policy is characterized by its unique ARN (Amazon Resource Name) and is configured to grant comprehensive access privileges to Amazon SageMaker services. data &quot;aws_caller_identity&quot; &quot;current&quot; {} data &quot;aws_region&quot; &quot;current&quot; {} data &quot;aws_iam_policy&quot; &quot;AmazonSageMakerFullAccess&quot; { arn = &quot;arn:aws:iam::aws:policy/AmazonSageMakerFullAccess&quot; } An IAM policy titled \"mlplatform-developer-access-policy\" is meticulously crafted to cater to platform developers, extending to them full-fledged access to a wide array of AWS services. This comprehensive access includes EKS, EC2 , S3, RDS, and VPC. The policy’s specifications are delineated within a JSON file situated at the designated file path. In a parallel fashion, an IAM policy denoted as \"mlplatform-user-access-policy\" is tailored to accommodate platform users, authorizing them to harness the capabilities of Amazon SageMaker services. This policy is also constructed through the utilization of a JSON file, residing at the specified path, and is thoughtfully designed to provide users with the necessary access privileges. resource &quot;aws_iam_policy&quot; &quot;mlplatform_developer_access_policy&quot; { name = &quot;mlplatform-developer-access-policy&quot; description = &quot;Access for platform developers granting them full EKS, EC2, S3, RDS, VPC access&quot; policy = file(&quot;${path.module}/access_policies/AccessPolicyDeveloper.json&quot;) } resource &quot;aws_iam_policy&quot; &quot;mlplatform_user_access_policy&quot; { name = &quot;mlplatform-user-access-policy&quot; description = &quot;Access for platform users granting them access to Sagemaker&quot; policy = file(&quot;${path.module}/access_policies/AccessPolicyUser.json&quot;) } For each profile outlined within the var.profiles variable, the “aws-profiles” module is instantiated. The primary purpose of this module is to facilitate the creation of IAM users, IAM roles, and AWS Secrets Manager secrets. Furthermore, it establishes the necessary associations by linking the previously defined developer and user access policies with their corresponding roles. module &quot;aws-profiles&quot; { for_each = var.profiles source = &quot;./aws-profiles&quot; profile = each.value access_policy_developer = aws_iam_policy.mlplatform_developer_access_policy.arn access_policy_user = aws_iam_policy.mlplatform_user_access_policy.arn } In the concluding phase of the code, the \"AmazonSageMakerFullAccess\" policy is affixed to IAM roles that are employed by platform users. This process involves iterating through the local.user_user_access_auth_list, which encompasses a collection of IAM role names designated for users. For each user role, the SageMaker access policy is linked or attached. # Add additional policies to ALL users resource &quot;aws_iam_role_policy_attachment&quot; &quot;sagemaker_access_user_role_policy&quot; { for_each = toset(local.user_user_access_auth_list) role = each.value policy_arn = data.aws_iam_policy.AmazonSageMakerFullAccess.arn depends_on = [module.aws-profiles] } 8.3.1.1 AWS Profiles The aws-profiles module, which is invoked within the user-profiles module, streamlines the creation of IAM users, roles, access keys, and Secrets Manager secrets tailored to user profiles within an ML platform. This module takes charge of managing IAM permissions and securely storing secrets, ensuring that platform users and developers can access AWS resources securely. In this segment, local variables are established based on the details provided in the var.profile. It dissects the username into firstName and lastName, extracts the role, and assembles a username for the IAM user. Just as in the preceding code snippet, this section fetches details regarding the AWS caller’s identity and the current AWS region. locals { firstName = split(&quot;.&quot;, var.profile.username)[0] lastName = split(&quot;.&quot;, var.profile.username)[1] role = var.profile.role username = &quot;${local.firstName}-${local.lastName}&quot; } data &quot;aws_caller_identity&quot; &quot;current&quot; {} data &quot;aws_region&quot; &quot;current&quot; {} Next, an IAM user is generated using the previously constructed username. The user is established at the root level (“/”) within IAM. Additionally, an IAM access key is generated for this IAM user, providing the capability for programmatic access to AWS resources. resource &quot;aws_iam_user&quot; &quot;this&quot; { name = local.username path = &quot;/&quot; } resource &quot;aws_iam_access_key&quot; &quot;this&quot; { user = aws_iam_user.this.name } Following this, an IAM role is generated and assigned the name \"mlplatform-access-${local.firstName}-${local.lastName}\". This role is configured with a maximum session duration of 28,800 seconds, equivalent to 8 hours. The assume_role_policy is defined to grant the IAM user the ability to assume this role, and it also authorizes Amazon S3 to assume this role. This is commonly employed to facilitate access to S3 buckets. resource &quot;aws_iam_role&quot; &quot;user_access_role&quot; { name = &quot;mlplatform-access-${local.firstName}-${local.lastName}&quot; max_session_duration = 28800 assume_role_policy = &lt;&lt;EOF { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/${aws_iam_user.this.name}&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;Service&quot;: &quot;s3.amazonaws.com&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; } ] } EOF # # tags = { # # tag-key = &quot;tag-value&quot; # # } } Furthermore, the code dynamically attaches two IAM role policy attachments based on the role specified in var.profile. If the role is designated as “Developer”, the \"access_policy_developer\" is linked to the IAM role. Alternatively, if the role is marked as “User”, the \"access_policy_user\" is connected to the IAM role. This conditional attachment of policies ensures that each IAM role is granted the appropriate level of access based on the user’s role designation. resource &quot;aws_iam_role_policy_attachment&quot; &quot;role_attachement_policy_developer&quot; { count = local.role == &quot;Developer&quot; ? 1 : 0 role = aws_iam_role.user_access_role.name policy_arn = var.access_policy_developer } resource &quot;aws_iam_role_policy_attachment&quot; &quot;role_attachement_policy_user&quot; { count = local.role == &quot;User&quot; ? 1 : 0 role = aws_iam_role.user_access_role.name policy_arn = var.access_policy_user } Concluding the module code, an AWS Secrets Manager secret is generated with a name derived from the username found in var.profile. A secret version is also established and populated with a range of attributes, encompassing access key information, username, email, role, and IAM role ARN. This comprehensive secret ensures secure storage and retrieval of critical user-specific data for authentication and access control. resource &quot;aws_secretsmanager_secret&quot; &quot;this&quot; { name = var.profile.username recovery_window_in_days = 0 } resource &quot;aws_secretsmanager_secret_version&quot; &quot;this&quot; { secret_id = aws_secretsmanager_secret.this.id secret_string = jsonencode( { &quot;ACCESS_KEY_ID&quot; : aws_iam_access_key.this.id, &quot;SECRET_ACCESS_KEY&quot; : aws_iam_access_key.this.secret &quot;username&quot; : var.profile.username &quot;email&quot; : var.profile.email &quot;role&quot; : local.role &quot;firstName&quot; : local.firstName &quot;lastName&quot; : local.lastName &quot;AWS_role&quot; : aws_iam_role.user_access_role.arn }) } 8.3.2 Airflow The Airflow module is responsible for provisioning all components related to the deployment of Airflow. Being a crucial workflow orchestration tool in our ML platform, Airflow is tightly integrated with various other components in the Terraform codebase, which requires it to receive multiple input variables and configurations. This Terraform code serves as the foundation for erecting an ML platform dashboard based on Apache Airflow, encompassing the configuration of a diverse array of components, including IAM roles, data storage, RDS, and Helm releases. The deployment of Airflow itself via this Terraform code hinges on the utilization of a Helm chart. This codebase also seamlessly integrates the Airflow deployment with AWS S3 for efficient data storage and logging, while relying on an AWS RDS instance from the infrastructure section to serve as the bedrock for metadata storage. Furthermore, pertinent Kubernetes secrets are meticulously integrated into the setup to ensure a secure and well-protected deployment. The code initiation begins with the declaration of several local variables, encompassing prefixes, secret names, and variable lists. These variables serve as cornerstones throughout the code, fostering consistency in naming conventions and configurations. Additionally, essential data sources, such as \"aws_caller_identity\" and \"aws_region\", are defined. A dedicated Kubernetes namespace for Airflow is also established, providing a controlled and isolated environment for Airflow resources within the Kubernetes cluster. locals { prefix = &quot;${var.name_prefix}-${var.namespace}&quot; k8s_airflow_db_secret_name = &quot;${local.prefix}-db-auth&quot; git_airflow_repo_secret_name = &quot;${local.prefix}-https-git-secret&quot; git_organization_secret_name = &quot;${local.prefix}-organization-git-secret&quot; s3_data_bucket_secret_name = &quot;${var.namespace}-${var.s3_data_bucket_secret_name}&quot; s3_data_bucket_name = &quot;${local.prefix}-${var.s3_data_bucket_name}&quot; airflow_variable_list_addition = [ { key = &quot;s3_access_name&quot; value = &quot;${local.s3_data_bucket_secret_name}&quot; } ] airflow_variable_list_full = concat(var.airflow_variable_list, local.airflow_variable_list_addition) } data &quot;aws_caller_identity&quot; &quot;current&quot; {} data &quot;aws_region&quot; &quot;current&quot; {} # resource &quot;kubernetes_namespace&quot; &quot;airflow&quot; { metadata { name = var.namespace } } Subsequently, a customized Terraform module, denoted as \"iam-service-account,\" is enlisted to configure IAM roles and policies tailored to service accounts associated with Airflow. These configurations include enabling role access through an OIDC provider and attaching an IAM policy that grants access to S3 buckets related to MLflow. These roles and policies serve as the bedrock for permissions and access control over a multitude of resources. module &quot;iam-service-account&quot; { source = &quot;./iam-service-account&quot; namespace = var.namespace oidc_provider_arn = var.oidc_provider_arn s3_mlflow_bucket_policy_arn = var.s3_mlflow_bucket_policy_arn } Moreover, an additional customized Terraform module, titled \"s3-data-storage,\" is harnessed to configure data storage. This entails the definition of an S3 bucket designed for data storage, complete with associated configurations such as the bucket name, secret names, and policies. This data storage facility empowers Airflow users to securely store a diverse range of data, including training images and more. module &quot;s3-data-storage&quot; { source = &quot;./data-storage&quot; namespace = var.namespace s3_data_bucket_name = local.s3_data_bucket_name s3_data_bucket_secret_name = local.s3_data_bucket_secret_name s3_mlflow_bucket_policy_arn = var.s3_mlflow_bucket_policy_arn s3_force_destroy = true } Before the Airflow deployment can take shape, the code diligently crafts Kubernetes secrets serving various purposes. These secrets play a pivotal role in safeguarding sensitive information and providing secure access and configurations for Airflow. They encompass a spectrum of crucial details, including database credentials, Git authentication credentials, AWS account information, and SageMaker access data. resource &quot;kubernetes_secret&quot; &quot;airflow_db_credentials&quot; { metadata { name = local.k8s_airflow_db_secret_name namespace = helm_release.airflow.namespace } data = { &quot;postgresql-password&quot; = module.rds-airflow.rds_password } } resource &quot;kubernetes_secret&quot; &quot;airflow_https_git_secret&quot; { metadata { name = local.git_airflow_repo_secret_name namespace = helm_release.airflow.namespace } data = { &quot;username&quot; = var.git_username &quot;password&quot; = var.git_token } } resource &quot;kubernetes_secret&quot; &quot;airflow_organization_git_secret&quot; { metadata { name = local.git_organization_secret_name namespace = helm_release.airflow.namespace } data = { &quot;GITHUB_CLIENT_ID&quot; = var.git_client_id &quot;GITHUB_CLIENT_SECRET&quot; = var.git_client_secret } } # secret with account information resource &quot;kubernetes_secret&quot; &quot;aws-account-information&quot; { metadata { name = &quot;${var.namespace}-aws-account-information&quot; namespace = var.namespace } data = { &quot;AWS_REGION&quot; = &quot;${data.aws_region.current.name}&quot; &quot;AWS_ID&quot; = &quot;${data.aws_caller_identity.current.account_id}&quot; } } # secret for sagemaker resource &quot;kubernetes_secret&quot; &quot;sagemaker-access&quot; { metadata { name = &quot;${var.namespace}-sagemaker-access&quot; namespace = var.namespace } data = { &quot;AWS_ROLE_NAME_SAGEMAKER&quot; = var.sagemaker_access_role_name } } Additionally, the code orchestrates the deployment of an RDS database through the utilization of a dedicated module. This RDS database is earmarked to serve as the metadata repository for the Airflow deployment, with a randomly generated password enhancing security. resource &quot;random_password&quot; &quot;rds_password&quot; { length = 16 special = false } module &quot;rds-airflow&quot; { source = &quot;../../infrastructure/rds&quot; vpc_id = var.vpc_id private_subnets = var.private_subnets private_subnets_cidr_blocks = var.private_subnets_cidr_blocks rds_port = var.rds_port rds_name = var.rds_name rds_password = coalesce(var.rds_password, random_password.rds_password.result) rds_engine = var.rds_engine rds_engine_version = var.rds_engine_version rds_instance_class = var.rds_instance_class storage_type = var.rds_storage_type max_allocated_storage = var.rds_max_allocated_storage } In this final phase, Apache Airflow is deployed using Helm, and meticulous configurations are set to ensure its smooth operation. These configurations encompass various aspects of Airflow, encompassing Git authentication, database connections, and Ingress settings for external accessibility. The Helm release configurations can be dissected into several key parts, each serving a specific purpose within the larger infrastructure of the dashboard. Helm Release Configuration: This segment delineates the Helm release resource named “airflow,” responsible for the deployment of Apache Airflow. It encompasses crucial details such as the Helm chart repository, chart name, chart version, and assorted configuration options. Airflow Configuration: In this section, comprehensive configurations for Apache Airflow are provided. This includes environment variables, Airflow-specific settings, and additional parameters required for tailoring the Airflow deployment. Of notable significance is the establishment of GitHub authentication and the definition of the Airflow webserver’s base URL. Service Account Configuration: This segment is dedicated to specifying the configuration for the service account utilized by Airflow. It initiates the creation of a service account named “airflow-sa” and establishes its association with an IAM role, as denoted by the “eks.amazonaws.com/role-arn” annotation. Ingress Configuration: Here, meticulous configuration of the Ingress for Apache Airflow takes place, facilitating external accessibility. This involves specifying annotations and settings for the Ingress controller, including hostname and health check path. Web Configuration: This component defines settings pertinent to the Airflow web component. It encompasses aspects such as readiness and liveness probes, which are instrumental in verifying the responsiveness of the web server. Additionally, provisions are made for configuration overrides through the utilization of a custom Python file, affording flexibility in tailoring the web server’s behavior. # HELM resource &quot;helm_release&quot; &quot;airflow&quot; { name = var.name namespace = var.namespace create_namespace = var.create_namespace repository = &quot;https://airflow-helm.github.io/charts&quot; chart = var.helm_chart_name version = var.helm_chart_version wait = false # deactivate post install hooks otherwise will fail values = [yamlencode({ airflow = { extraEnv = [ { name = &quot;GITHUB_CLIENT_ID&quot; valueFrom = { secretKeyRef = { name = local.git_organization_secret_name key = &quot;GITHUB_CLIENT_ID&quot; } } }, { name = &quot;GITHUB_CLIENT_SECRET&quot; valueFrom = { secretKeyRef = { name = local.git_organization_secret_name key = &quot;GITHUB_CLIENT_SECRET&quot; } } } ], config = { AIRFLOW__WEBSERVER__EXPOSE_CONFIG = false AIRFLOW__WEBSERVER__BASE_URL = &quot;http://${var.domain_name}/${var.domain_suffix}&quot; AIRFLOW__CORE__LOAD_EXAMPLES = false AIRFLOW__CORE__DEFAULT_TIMEZONE = &quot;Europe/Amsterdam&quot; }, users = [] image = { repository = &quot;seblum/airflow&quot; tag = &quot;2.6.3-python3.11-custom-light&quot; pullPolicy = &quot;IfNotPresent&quot; pullSecret = &quot;&quot; uid = 50000 gid = 0 }, executor = &quot;KubernetesExecutor&quot; fernetKey = var.fernet_key webserverSecretKey = &quot;THIS IS UNSAFE!&quot; variables = local.airflow_variable_list_full }, serviceAccount = { create = true name = &quot;airflow-sa&quot; annotations = { &quot;eks.amazonaws.com/role-arn&quot; = &quot;${module.iam-service-account.airflow_service_account_role_arn}&quot; } }, scheduler = { logCleanup = { enabled = false } }, workers = { enabled = false logCleanup = { enables = true } }, flower = { enabled = false }, postgresql = { enabled = false }, redis = { enabled = false }, externalDatabase = { type = &quot;postgres&quot; host = module.rds-airflow.rds_host port = var.rds_port database = &quot;airflow_db&quot; user = &quot;airflow_admin&quot; passwordSecret = local.k8s_airflow_db_secret_name passwordSecretKey = &quot;postgresql-password&quot; }, dags = { path = &quot;/opt/airflow/dags&quot; gitSync = { enabled = true repo = var.git_repository_url branch = var.git_branch revision = &quot;HEAD&quot; # repoSubPath = &quot;workflows&quot; httpSecret = local.git_airflow_repo_secret_name httpSecretUsernameKey = &quot;username&quot; httpSecretPasswordKey = &quot;password&quot; syncWait = 60 syncTimeout = 120 } }, logs = { path = &quot;/opt/airflow/logs&quot; persistence = { enabled = true storageClass : &quot;efs&quot; size : &quot;5Gi&quot; accessMode : &quot;ReadWriteMany&quot; } }, ingress = { enabled = true apiVersion = &quot;networking.k8s.io/v1&quot; web = { annotations = { &quot;external-dns.alpha.kubernetes.io/hostname&quot; = &quot;${var.domain_name}&quot; &quot;alb.ingress.kubernetes.io/scheme&quot; = &quot;internet-facing&quot; &quot;alb.ingress.kubernetes.io/target-type&quot; = &quot;ip&quot; &quot;kubernetes.io/ingress.class&quot; = &quot;alb&quot; &quot;alb.ingress.kubernetes.io/group.name&quot; = &quot;mlplatform&quot; &quot;alb.ingress.kubernetes.io/healthcheck-path&quot; = &quot;/${var.domain_suffix}/health&quot; } path = &quot;/${var.domain_suffix}&quot; host = &quot;${var.domain_name}&quot; precedingPaths = [{ path = &quot;/${var.domain_suffix}*&quot; serviceName = &quot;airflow-web&quot; servicePort = &quot;web&quot; }] } }, web = { readinessProbe = { enabled = true initialDelaySeconds = 45 }, livenessProbe = { enabled = true initialDelaySeconds = 45 }, webserverConfig = { stringOverride = file(&quot;${path.module}/WebServerConfig.py&quot;) } }, })] } In summary, the Terraform code provisions the necessary infrastructure components, IAM roles and policies, data storage, RDS database, Kubernetes secrets, and deploys Apache Airflow using Helm. This setup forms the foundation for the ML platform’s dashboard, enabling workflow orchestration and data management capabilities with Airflow. 8.3.2.1 WebServerConfig In a final step of the Helm Chart, a custom WebServerConfig.py is specified which is set to integrate our Airflow deployment with a Github Authentication provider. The Python script consists of two major parts: a custom AirflowSecurityManager class definition and the actual webserver_config configuration file for Apache Airflow’s web server. The custom CustomSecurityManager class extends the default AirflowSecurityManager to retrieves user information from the GitHub OAuth provider. The webserver_config configuration sets up the configurations for the web server component of Apache Airflow by indicating that OAuth will be used for user authentication. The SECURITY_MANAGER_CLASS is set to the previously defined CustomSecurityManager to customizes how user information is retrieved from the OAuth provider. Finally, the GitHub provider is configured with its required parameters like client_id, client_secret, and API endpoints. ####################################### # Custom AirflowSecurityManager ####################################### from airflow.www.security import AirflowSecurityManager import os class CustomSecurityManager(AirflowSecurityManager): def get_oauth_user_info(self, provider, resp): if provider == &quot;github&quot;: user_data = self.appbuilder.sm.oauth_remotes[provider].get(&quot;user&quot;).json() emails_data = ( self.appbuilder.sm.oauth_remotes[provider].get(&quot;user/emails&quot;).json() ) teams_data = ( self.appbuilder.sm.oauth_remotes[provider].get(&quot;user/teams&quot;).json() ) # unpack the user&#39;s name first_name = &quot;&quot; last_name = &quot;&quot; name = user_data.get(&quot;name&quot;, &quot;&quot;).split(maxsplit=1) if len(name) == 1: first_name = name[0] elif len(name) == 2: first_name = name[0] last_name = name[1] # unpack the user&#39;s email email = &quot;&quot; for email_data in emails_data: if email_data[&quot;primary&quot;]: email = email_data[&quot;email&quot;] break # unpack the user&#39;s teams as role_keys # NOTE: each role key will be &quot;my-github-org/my-team-name&quot; role_keys = [] for team_data in teams_data: team_org = team_data[&quot;organization&quot;][&quot;login&quot;] team_slug = team_data[&quot;slug&quot;] team_ref = team_org + &quot;/&quot; + team_slug role_keys.append(team_ref) return { &quot;username&quot;: &quot;github_&quot; + user_data.get(&quot;login&quot;, &quot;&quot;), &quot;first_name&quot;: first_name, &quot;last_name&quot;: last_name, &quot;email&quot;: email, &quot;role_keys&quot;: role_keys, } else: return {} ####################################### # Actual `webserver_config.py` ####################################### from flask_appbuilder.security.manager import AUTH_OAUTH # only needed for airflow 1.10 # from airflow import configuration as conf # SQLALCHEMY_DATABASE_URI = conf.get(&quot;core&quot;, &quot;SQL_ALCHEMY_CONN&quot;) AUTH_TYPE = AUTH_OAUTH SECURITY_MANAGER_CLASS = CustomSecurityManager # registration configs AUTH_USER_REGISTRATION = True # allow users who are not already in the FAB DB AUTH_USER_REGISTRATION_ROLE = ( &quot;Public&quot; # this role will be given in addition to any AUTH_ROLES_MAPPING ) # the list of providers which the user can choose from OAUTH_PROVIDERS = [ { &quot;name&quot;: &quot;github&quot;, &quot;icon&quot;: &quot;fa-github&quot;, &quot;token_key&quot;: &quot;access_token&quot;, &quot;remote_app&quot;: { &quot;client_id&quot;: os.getenv(&quot;GITHUB_CLIENT_ID&quot;), &quot;client_secret&quot;: os.getenv(&quot;GITHUB_CLIENT_SECRET&quot;), &quot;api_base_url&quot;: &quot;https://api.github.com&quot;, &quot;client_kwargs&quot;: {&quot;scope&quot;: &quot;read:org read:user user:email&quot;}, &quot;access_token_url&quot;: &quot;https://github.com/login/oauth/access_token&quot;, &quot;authorize_url&quot;: &quot;https://github.com/login/oauth/authorize&quot;, }, }, ] # a mapping from the values of `userinfo[&quot;role_keys&quot;]` to a list of FAB roles AUTH_ROLES_MAPPING = { &quot;github-organization/airflow-users-team&quot;: [&quot;User&quot;], &quot;github-organization/airflow-admin-team&quot;: [&quot;Admin&quot;], } # if we should replace ALL the user&#39;s roles each login, or only on registration AUTH_ROLES_SYNC_AT_LOGIN = True # force users to re-auth after 30min of inactivity (to keep roles in sync) PERMANENT_SESSION_LIFETIME = 1800 8.3.3 Mlflow In order to enable model tracking, the deployment of MLflow necessitates specific requirements. These include the establishment of a data store on AWS S3, a metadata store utilizing PostgreSQL (RDS), and the MLflow server itself. The initial two components are crafted using Terraform resources. However, it’s worth noting that MLflow lacks native support for Kubernetes and an official Helm chart. In light of this, despite being a highly effective tool, we need to create a fundamental custom Helm chart for deploying the MLflow server. Furthermore, a custom container image is employed for running MLflow. This process entails the creation of YAML configurations for deployment, service, and configmap, all of which are executed on our Kubernetes cluster. The provided Terraform code assumes responsibility for orchestrating the deployment of a dashboard tailored for an ML platform, relying on the power of MLflow. This intricate process encompasses various configurations and resource deployments. It commences by defining local variables, including a unique S3 bucket name. Subsequently, an S3 bucket with the name \"mlflow\" is generated, designed explicitly for housing MLflow artifacts. locals { s3_bucket_name = &quot;${var.name_prefix}-${var.namespace}-${var.s3_bucket_name}&quot; s3_bucket_path_prefix = &quot;users&quot; } data &quot;aws_caller_identity&quot; &quot;current&quot; {} # create s3 bucket for artifacts resource &quot;aws_s3_bucket&quot; &quot;mlflow&quot; { bucket = local.s3_bucket_name # tags = var.tags force_destroy = var.s3_force_destroy } resource &quot;aws_s3_bucket_server_side_encryption_configuration&quot; &quot;bucket_state_encryption&quot; { bucket = aws_s3_bucket.mlflow.bucket rule { apply_server_side_encryption_by_default { sse_algorithm = &quot;AES256&quot; } } } Following this, an IAM role by the name of \"mlflow_s3_role\" is crafted, bestowing access to the S3 bucket. This role assumes a web identity role using the OIDC provider ARN specified by var.oidc_provider_arn. In a similar vein, an IAM policy named \"mlflow_s3_policy\" is forged, extending specific permissions for S3 access. These permissions encompass actions such as object creation, listing, and deletion within the S3 bucket. The policy is meticulously scoped to the S3 bucket and its associated objects. Subsequently, the \"mlflow_s3_policy\" is attached to the \"mlflow_s3_role\", effectively enabling the IAM role to wield the permissions delineated in the policy. # &quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${var.eks_oidc_provider}&quot; resource &quot;aws_iam_role&quot; &quot;mlflow_s3_role&quot; { name = &quot;${var.namespace}-s3-access-role&quot; assume_role_policy = &lt;&lt;EOF { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Action&quot; : &quot;sts:AssumeRoleWithWebIdentity&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot; : { &quot;Federated&quot; : [ &quot;${var.oidc_provider_arn}&quot; ] } } ] } EOF tags = { tag-key = &quot;tag-value&quot; } } resource &quot;aws_iam_policy&quot; &quot;mlflow_s3_policy&quot; { name = &quot;${var.namespace}-s3-access-policy&quot; path = &quot;/&quot; policy = jsonencode({ &quot;Version&quot; : &quot;2012-10-17&quot;, &quot;Statement&quot; : [ { &quot;Effect&quot; : &quot;Allow&quot;, &quot;Action&quot; : [ &quot;s3:*Object&quot;, &quot;s3:GetObjectVersion&quot;, &quot;s3:*&quot; ], &quot;Resource&quot; : [ &quot;arn:aws:s3:::${local.s3_bucket_name}/*&quot;, &quot;arn:aws:s3:::${local.s3_bucket_name}&quot; ] }, { &quot;Effect&quot; : &quot;Allow&quot;, &quot;Action&quot; : [ &quot;s3:ListBucket&quot;, &quot;s3:ListBucketVersions&quot; ], &quot;Resource&quot; : [ &quot;arn:aws:s3:::${local.s3_bucket_name}/*&quot;, &quot;arn:aws:s3:::${local.s3_bucket_name}&quot; ], &quot;Condition&quot; : { &quot;StringLike&quot; : { &quot;s3:prefix&quot; : [ &quot;${local.s3_bucket_path_prefix}/*&quot; ] } } } ] }) } resource &quot;aws_iam_role_policy_attachment&quot; &quot;mlflow_s3_policy&quot; { role = aws_iam_role.mlflow_s3_role.name policy_arn = aws_iam_policy.mlflow_s3_policy.arn } Much like the Airflow deployment, an RDS database is deployed for use as a metadata store through the specified RDS module. Here, a random password is generated for the RDS database that will be employed by MLflow. resource &quot;random_password&quot; &quot;rds_password&quot; { length = 16 # MLFlow has troubles using special characters special = false } # create rds for s3 module &quot;rds-mlflow&quot; { source = &quot;../../infrastructure/rds&quot; vpc_id = var.vpc_id private_subnets = var.private_subnets private_subnets_cidr_blocks = var.private_subnets_cidr_blocks rds_port = var.rds_port rds_name = var.rds_name rds_password = coalesce(var.rds_password, random_password.rds_password.result) rds_engine = var.rds_engine rds_engine_version = var.rds_engine_version rds_instance_class = var.rds_instance_class storage_type = var.rds_storage_type max_allocated_storage = var.rds_max_allocated_storage } The final phase of this deployment leverages Helm to roll out MLflow. It encompasses crucial aspects such as the Docker image selection for MLflow, configuration settings for external access through Ingress, configuration of the S3 bucket, and provision of essential information regarding the RDS database. This Helm release orchestrates the deployment of MLflow, a pivotal component within the ML platform’s dashboard. Given the absence of a native Helm chart for MLflow, a custom Helm chart has been crafted to facilitate this deployment. The chart brings together various Kubernetes deployments, including the MLflow deployment itself, a service account, a secret, and an ingress configuration. For a detailed look at the chart and its components, you can refer to this implementation. Furthermore, as MLflow doesn’t offer a suitable container image for our specific use case, a custom container image has been defined and can be explored here. resource &quot;helm_release&quot; &quot;mlflow&quot; { name = var.name namespace = var.namespace create_namespace = var.create_namespace chart = &quot;${path.module}/helm/&quot; values = [yamlencode({ deployment = { image = &quot;seblum/mlflow:v2.4.1&quot; namespace = var.namespace name = var.name }, ingress = { host = var.domain_name path = var.domain_suffix }, artifacts = { s3_role_arn = &quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${aws_iam_role.mlflow_s3_role.name}&quot;, s3_key_prefix = local.s3_bucket_path_prefix, s3_bucket = local.s3_bucket_name, }, rds = { host = module.rds-mlflow.rds_host port = var.rds_port, username = module.rds-mlflow.rds_username, password = module.rds-mlflow.rds_password, db_name = module.rds-mlflow.rds_dbname }, })] } NOTE: It’s important to note that the deployment has an open endpoint, which means it lacks sufficient security measures. 8.3.4 Jupyterhub In our setup, JupyterHub plays a crucial role by providing an Integrated Development Environment (IDE). The Terraform code presented here defines a helm_release responsible for deploying JupyterHub onto our EKS cluster. In contrast to other components of our ML platform, JupyterHub doesn’t require additional resources to operate. The Helm configuration used in this deployment is multifaceted, encompassing various settings and customizations. Its primary aim is to establish a JupyterHub instance that seamlessly integrates with a single-user Jupyter notebook server. This configuration encompasses user sessions, GitHub authentication, proxy settings, Ingress for external access, and various other JupyterHub-related configurations. This ensures that JupyterHub is finely tuned to meet the specific requirements of our ML platform, providing users with the ability to run interactive notebooks and access MLflow services effortlessly. Within the Terraform code, a Helm release named \"jupyterhub\" is defined, orchestrating the deployment of JupyterHub into the designated Kubernetes namespace. The Helm chart is sourced from the JupyterHub Helm chart repository at a version specified by var.helm_chart_version. The values block within this configuration contains a YAML-encoded set of parameters for JupyterHub, including numerous settings related to single-user notebooks, Ingress, proxy, culling, and hub configuration. Single-User Notebook Configuration: This segment of the configuration is dedicated to single-user notebook settings. It encompasses parameters like the default URL for notebooks, the Docker image to be employed, and lifecycle hooks. The Docker image is set to “seblum/jupyterhub-server:latest,” and a postStart lifecycle hook is defined to clone a Git repository specified by var.git_repository_url. Additionally, an environment variable MLFLOW_TRACKING_URI is configured to point to the URI of the MLflow service. Ingress Configuration: The Ingress resource is configured to facilitate external access to JupyterHub. This entails the inclusion of annotations to tailor its behavior. Key settings include the specification of the hostname, scheme, healthcheck path, and ingress class. Hosts are configured to ${var.domain_name} and www.${var.domain_name}, facilitating access through the designated domain name. Proxy Configuration: Within the proxy configuration, the service type for the JupyterHub proxy is set as “ClusterIP.” Additionally, the secretToken is configured with a value provided by var.proxy_secret_token. Culling Configuration: Culling is enabled and finely tuned to manage user sessions. Users are subject to culling when their sessions become idle. Hub Configuration: The hub configuration addresses settings pertaining to the JupyterHub’s base URL, GitHub OAuthenticator, and JupyterHub’s authenticator class. Similar to the Airflow deployment, the JupyterHub instance is configured to utilize GitHub OAuthenticator for user authentication. This OAuthenticator is then configured with the supplied GitHub credentials (var.git_client_id and var.git_client_secret), along with the oauth_callback_url parameter, which specifies a specific endpoint under the provided domain name. resource &quot;helm_release&quot; &quot;jupyterhub&quot; { name = var.name namespace = var.name create_namespace = var.create_namespace repository = &quot;https://jupyterhub.github.io/helm-chart/&quot; chart = var.helm_chart_name version = var.helm_chart_version values = [yamlencode({ singleuser = { defaultUrl = &quot;/lab&quot; image = { name = &quot;seblum/jupyterhub-server&quot; tag = &quot;latest&quot; }, lifecycleHooks = { postStart = { exec = { command = [&quot;git&quot;, &quot;clone&quot;, &quot;${var.git_repository_url}&quot;] } } }, extraEnv = { &quot;MLFLOW_TRACKING_URI&quot; = &quot;http://mlflow-service.mlflow.svc.cluster.local&quot; } }, ingress = { enabled : true annotations = { &quot;external-dns.alpha.kubernetes.io/hostname&quot; = &quot;${var.domain_name}&quot; &quot;alb.ingress.kubernetes.io/scheme&quot; = &quot;internet-facing&quot; &quot;alb.ingress.kubernetes.io/target-type&quot; = &quot;ip&quot; &quot;kubernetes.io/ingress.class&quot; = &quot;alb&quot; &quot;alb.ingress.kubernetes.io/group.name&quot; = &quot;mlplatform&quot; } hosts = [&quot;${var.domain_name}&quot;, &quot;www.${var.domain_name}&quot;] }, proxy = { service = { type = &quot;ClusterIP&quot; } secretToken = var.proxy_secret_token } cull = { enabled = true users = true } hub = { baseUrl = &quot;/${var.domain_suffix}&quot; config = { GitHubOAuthenticator = { client_id = var.git_client_id client_secret = var.git_client_secret oauth_callback_url = &quot;http://${var.domain_name}/${var.domain_suffix}/hub/oauth_callback&quot; } JupyterHub = { authenticator_class = &quot;github&quot; } } } })] } 8.3.5 Monitoring Incorporated into the setup is a monitoring system leveraging Prometheus and Grafana. While not a direct component of the ML pipeline, this configuration serves as an instructive illustration of proficient cluster monitoring. It encompasses a fundamental monitoring setup accomplished through the utilization of Prometheus and Grafana. Prometheus and Grafana play distinct yet complementary roles in the domain of monitoring and observability. Prometheus is primarily responsible for collecting, storing, and alerting based on time-series metrics. It scrapes data from various sources, defines alerting rules, and supports service discovery, making it a robust monitoring and alerting tool. It also offers a query language for metric analysis and flexible data retention policies. On the other hand, Grafana specializes in data visualization and interactive dashboard creation. It connects to data sources like Prometheus and transforms metric data into visually engaging charts and graphs. Grafana is instrumental in designing comprehensive monitoring dashboards, visualizing alerts, and facilitating data exploration. Together, Prometheus and Grafana form a powerful monitoring stack, enabling organizations to monitor, analyze, and visualize their systems effectively. Both tools, Prometheus and Grafana, are seamlessly deployed via a Helm chart. The crucial interconnection between Grafana and Prometheus is thoughtfully established within the Grafana Helm chart, ensuring a cohesive and comprehensive monitoring solution. 8.3.5.1 Prometheus The provided Terraform code facilitates the deployment of Prometheus and Prometheus Operator Custom Resource Definitions (CRDs) for the ML platform dashboard. This deployment leverages Helm for efficient management, allowing for customizable configurations and streamlined monitoring and alerting system setup. The process begins with the definition of a Helm release named \"prometheus\" responsible for deploying Prometheus, a comprehensive monitoring and alerting toolkit, to the specified Kubernetes namespace. The Helm chart utilized for this deployment is sourced from the Prometheus community Helm charts repository, adhering to a specified version. Within the \"values\" block, you’ll find a YAML-encoded configuration for Prometheus. This configuration tailors specific aspects of the Prometheus installation, including the option to disable Alertmanager and Prometheus Pushgateway components. It also provides the flexibility to enable or disable persistent volumes for the Prometheus server. resource &quot;helm_release&quot; &quot;prometheus&quot; { chart = &quot;prometheus&quot; name = &quot;prometheus&quot; namespace = var.namespace create_namespace = var.create_namespace repository = &quot;https://prometheus-community.github.io/helm-charts&quot; version = &quot;19.7.2&quot; values = [ yamlencode({ alertmanager = { enabled = false } prometheus-pushgateway = { enabled = false } server = { persistentVolume = { enabled = false } } }) ] } In addition to the Prometheus release, another Helm release named \"prometheus-operator-crds\" is established. This release is focused on deploying the Custom Resource Definitions (CRDs) essential for the Prometheus Operator. Similarly, the Helm chart used for this deployment originates from the Prometheus community Helm charts repository but at a distinct version. The Prometheus Operator CRDs are essential for defining and managing Prometheus instances and associated resources within the Kubernetes cluster, ensuring effective monitoring and alerting for the ML platform dashboard. resource &quot;helm_release&quot; &quot;prometheus-operator-crds&quot; { chart = &quot;prometheus-operator-crds&quot; name = &quot;prometheus-operator-crds&quot; namespace = var.namespace create_namespace = var.create_namespace repository = &quot;https://prometheus-community.github.io/helm-charts&quot; version = &quot;5.1.0&quot; } 8.3.5.2 Grafana The provided Terraform code deploys Grafana, a dashboard and visualization platform, for an ML platform. The Grafana deployment is highly customized, with various settings and configurations. A Helm release named \"grafana\" is defined, deploying Grafana to the specified Kubernetes namespace. It pulls the Grafana chart from the official Helm charts repository at version \"6.57.4.\" The \"values\" block contains a YAML-encoded configuration for Grafana, including various settings related to service type, Ingress, data sources, dashboard providers, and more. Service and Ingress Configuration: The Grafana service is configured to be of type \"ClusterIP\" and an Ingress resource is enabled for external access. Several annotations are added to the Ingress to customize how it interacts with the Kubernetes cluster, including specifying the hostname, scheme, healthcheck path, and other settings. The Ingress is set up to handle requests for the specified domain name and subdomain, allowing external access to Grafana. Data Sources Configuration: The configuration includes data source settings, specifically for Prometheus. It defines a data source named “Prometheus” with details like the type, URL, access mode, and setting it as the default data source. This configuration allows Grafana to retrieve metrics and data from Prometheus for visualization and dashboard creation. Dashboard Providers Configuration: Grafana’s dashboard providers are configured using a YAML block. It defines a default provider with options specifying the path to dashboards. This configuration enables Grafana to load dashboards from the specified path within the Grafana container. Dashboards Configuration: The code defines a set of dashboards and their configurations. Each dashboard is associated with a data source (in this case, Prometheus) and has various settings such as gnetId (unique identifier), revision, and data source. These configurations determine which data is displayed on each dashboard and how it is accessed. Grafana Configuration (grafana.ini): This section provides a set of configurations for Grafana itself, including security settings that allow embedding Grafana in iframes. It specifies the server’s domain and root URL, enabling Grafana to serve from a subpath. Additionally, GitHub authentication is enabled for user sign-up and authentication, using the provided GitHub OAuth client ID and secret. resource &quot;helm_release&quot; &quot;grafana&quot; { chart = &quot;grafana&quot; name = &quot;grafana&quot; namespace = var.namespace create_namespace = var.create_namespace repository = &quot;https://grafana.github.io/helm-charts/&quot; version = &quot;6.57.4&quot; values = [ yamlencode({ service = { enabled = true type = &quot;ClusterIP&quot; } ingress = { enabled = true annotations = { &quot;external-dns.alpha.kubernetes.io/hostname&quot; = &quot;${var.domain_name}&quot;, &quot;alb.ingress.kubernetes.io/scheme&quot; = &quot;internet-facing&quot;, &quot;alb.ingress.kubernetes.io/target-type&quot; = &quot;ip&quot;, &quot;kubernetes.io/ingress.class&quot; = &quot;alb&quot;, &quot;alb.ingress.kubernetes.io/group.name&quot; = &quot;mlplatform&quot;, &quot;alb.ingress.kubernetes.io/healthcheck-path&quot; = &quot;/api/health&quot; } labels = {} path = &quot;${var.domain_suffix}&quot; pathType = &quot;Prefix&quot; hosts = [ &quot;${var.domain_name}&quot;, &quot;www.${var.domain_name}&quot; ] }, datasources = { &quot;datasources.yaml&quot; = { apiVersion = 1 datasources = [ { name = &quot;Prometheus&quot; type = &quot;prometheus&quot; url = &quot;http://prometheus-server.${var.namespace}.svc.cluster.local&quot; access = &quot;proxy&quot; isDefault = true } ] } }, dashboardProviders = { &quot;dashboardproviders.yaml&quot; = { apiVersion = 1 providers = [ { name = &quot;&#39;default&#39;&quot; orgId = 1 folder = &quot;&#39;&#39;&quot; type : &quot;file&quot; disableDeletion : false editable : true options = { path = &quot;/var/lib/grafana/dashboards/default&quot; } } ] } } dashboards = { default = { prometheus-stats = { gnetId = 2 revision = 2 datasource = &quot;Prometheus&quot; } prometheus-stats-2 = { gnetId = 315 datasource = &quot;Prometheus&quot; } k8s-cluster = { gnetId = 6417 datasource = &quot;Prometheus&quot; } } } &quot;grafana.ini&quot; = { security = { allow_embedding = true # enables iframe loading }, server = { domain : &quot;${var.domain_name}&quot; root_url : &quot;%(protocol)s://%(domain)s/grafana/&quot; serve_from_sub_path : true # https://grafana.com/docs/grafana/latest/auth/github/#enable-github-in-grafana }, &quot;auth.github&quot; = { enabled = true allow_sign_up = true scopes = &quot;user:email,read:org&quot; auth_url = &quot;https://github.com/login/oauth/authorize&quot; token_url = &quot;https://github.com/login/oauth/access_token&quot; api_url = &quot;https://api.github.com/user&quot; # team_ids: grafana-user-team # allowed_organizations: client_id = var.git_client_id client_secret = var.git_client_secret } } })] } 8.3.6 Sagemaker The Sagemaker module serves as a vital component of the ML platform, ensuring seamless interaction with SageMaker by providing essential IAM roles and permissions. Additionally, it deploys a custom Helm chart designed to harness the capabilities of a straightforward Streamlit application, effectively presenting all deployed SageMaker endpoints. The Terraform code commences by establishing local variables, meticulously capturing a spectrum of configuration details pivotal for this deployment. These variables encompass crucial information such as Docker image specifics, ECR repository nomenclature, IAM role designations, and more. The judicious utilization of these variables throughout the code enhances both consistency and reusability. locals { docker_mlflow_sagemaker_base_image = var.docker_mlflow_sagemaker_base_image base_image_tag = split(&quot;:&quot;, var.docker_mlflow_sagemaker_base_image)[1] ecr_repository_name = &quot;mlflow-sagemaker-deployment&quot; iam_name_sagemaker_access = &quot;sagemaker-access&quot; sagemaker_dashboard_read_access_user_name = &quot;sagemaker-dashboard-read-access-user&quot; sagemaker_dashboard_read_access_role_name = &quot;sagemaker-dashboard-read-access-role&quot; sagemaker_dashboard_read_access_secret = &quot;sagemaker-dashboard-read-access-secret&quot; } data &quot;aws_caller_identity&quot; &quot;current&quot; {} data &quot;aws_region&quot; &quot;current&quot; {} data &quot;aws_iam_policy&quot; &quot;AmazonSageMakerFullAccess&quot; { arn = &quot;arn:aws:iam::aws:policy/AmazonSageMakerFullAccess&quot; } data &quot;aws_iam_policy&quot; &quot;AmazonSageMakerReadOnlyAccess&quot; { arn = &quot;arn:aws:iam::aws:policy/AmazonSageMakerReadOnly&quot; } The code efficiently leverages the \"terraform-aws-modules/ecr/aws\" module, effectively birthing an Elastic Container Registry (ECR) repository christened as \"mlflow-sagemaker-deployment\". Furthermore, a \"null_resource\" block is meticulously crafted to orchestrate the packaging and dissemination of a Docker image to the ECR repository. This resource leverages a local-exec provisioner, deftly executing a series of Docker commands. These commands encompass the retrieval of the base image, affixing it with the ECR repository URL, securing authentication to the ECR registry, and ultimately, the seamless transmission of the image. This strategic orchestration guarantees the availability of the MLflow base image, purpose-built for SageMaker deployments, within the ECR repository, ready for deployment when needed. # Create Container Registry module &quot;ecr&quot; { source = &quot;terraform-aws-modules/ecr/aws&quot; repository_name = local.ecr_repository_name repository_lifecycle_policy = jsonencode({ rules = [ { rulePriority = 1, description = &quot;Keep last 30 images&quot;, selection = { tagStatus = &quot;tagged&quot;, tagPrefixList = [&quot;v&quot;], countType = &quot;imageCountMoreThan&quot;, countNumber = 30 }, action = { type = &quot;expire&quot; } } ] }) repository_force_delete = true # tags = { # Terraform = &quot;true&quot; # Environment = &quot;dev&quot; # } } # mlflow sagemaker build-and-push-container --build --no-push -c mlflow-sagemaker-deployment # https://mlflow.org/docs/latest/cli.html resource &quot;null_resource&quot; &quot;docker_packaging&quot; { provisioner &quot;local-exec&quot; { command = &lt;&lt;EOF docker pull &quot;${local.docker_mlflow_sagemaker_base_image}&quot; docker tag &quot;${local.docker_mlflow_sagemaker_base_image}&quot; &quot;${module.ecr.repository_url}:${local.base_image_tag}&quot; aws ecr get-login-password --region ${data.aws_region.current.name} | docker login --username AWS --password-stdin ${data.aws_caller_identity.current.account_id}.dkr.ecr.${data.aws_region.current.name}.amazonaws.com docker push &quot;${module.ecr.repository_url}:${local.base_image_tag}&quot; EOF } # triggers = { # &quot;run_at&quot; = timestamp() # } depends_on = [ module.ecr, ] } Moreover, the Terraform module assumes responsibility for the creation of an IAM role christened as \"sagemaker_access_role\". This role plays a pivotal role, enabling SageMaker to assume its authority for requisite access. The trust policy governing this role stipulates SageMaker’s unequivocal authority to assume it. Notably, the code adjoins the \"AmazonSageMakerFullAccess\" IAM policy to the \"sagemaker_access_role\", conferring comprehensive access rights to SageMaker resources. Parallel to the MLflow SageMaker base image residing within ECR, the \"sagemaker_access_role\" becomes an indispensable component, facilitating MLflow’s deployments to AWS SageMaker. # Access role to allow access to Sagemaker resource &quot;aws_iam_role&quot; &quot;sagemaker_access_role&quot; { name = &quot;${local.iam_name_sagemaker_access}-role&quot; max_session_duration = 28800 assume_role_policy = &lt;&lt;EOF { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;Service&quot;: &quot;sagemaker.amazonaws.com&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; } ] } EOF # tags = { # tag-key = &quot;tag-value&quot; # } } resource &quot;aws_iam_role_policy_attachment&quot; &quot;sagemaker_access_role_policy&quot; { role = aws_iam_role.sagemaker_access_role.name policy_arn = data.aws_iam_policy.AmazonSageMakerFullAccess.arn } In the final orchestration, the ML platform’s Sagemaker Dashboard materializes through the medium of Helm. The deployment specifications encompass critical parameters, encompassing the Docker image identity, deployment nomenclature, namespace delineation, Ingress configuration tailored for efficient routing of external traffic, and essential secrets mandatorily required for authentication. Notably, the deployment relies on the Docker image bearing the label \"seblum/streamlit-sagemaker-app:v1.0.0\", which serves as the engine propelling the Streamlit application. For those seeking deeper insights, the inner workings of this Docker image are meticulously documented here. # Helm Deployment resource &quot;helm_release&quot; &quot;sagemaker-dashboard&quot; { name = var.name namespace = var.namespace create_namespace = var.create_namespace chart = &quot;${path.module}/helm/&quot; values = [yamlencode({ deployment = { image = &quot;seblum/streamlit-sagemaker-app:v1.0.0&quot;, name = &quot;sagemaker-streamlit&quot;, namespace = &quot;${var.namespace}&quot; }, ingress = { host = &quot;${var.domain_name}&quot; path = &quot;${var.domain_suffix}&quot; }, secret = { aws_region = &quot;${data.aws_region.current.name}&quot; aws_access_key_id = &quot;${aws_iam_access_key.sagemaker_dashboard_read_access_user_credentials.id}&quot; aws_secret_access_key = &quot;${aws_iam_access_key.sagemaker_dashboard_read_access_user_credentials.secret}&quot; aws_role_name = &quot;${aws_iam_role.sagemaker_dashboard_read_access_role.name}&quot; } })] } In tandem with this deployment, an additional IAM role christened as \"sagemaker_dashboard_read_access_role\" takes center stage, conferring access rights to SageMaker resources. The trust policy associated with this role casts a discerning gaze, specifying the entities deemed eligible to assume its authority. Among the authorized entities are the SageMaker user and the SageMaker dashboard read-access user. To further bolster the role’s capabilities, it is graced with the \"AmazonSageMakerReadOnlyAccess\" IAM policy, gracefully endowing it with read-only access privileges to SageMaker resources. Concomitantly, an IAM user, christened as \"sagemaker_dashboard_read_access_user,\" is ushered into existence, complete with an associated access key. This user is purpose-built for interfacing with SageMaker resources, playing a pivotal role in accessing SageMaker endpoints and seamlessly presenting them within the Streamlit application. # Access role to allow access to Sagemaker resource &quot;aws_iam_role&quot; &quot;sagemaker_dashboard_read_access_role&quot; { name = local.sagemaker_dashboard_read_access_role_name max_session_duration = 28800 assume_role_policy = &lt;&lt;EOF { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/${aws_iam_user.sagemaker_dashboard_read_access_user.name}&quot; }, &quot;Action&quot;: &quot;sts:AssumeRole&quot; } ] } EOF # tags = { # tag-key = &quot;tag-value&quot; # } } resource &quot;aws_iam_role_policy_attachment&quot; &quot;sagemaker_dashboard_read__access_role_policy&quot; { role = aws_iam_role.sagemaker_dashboard_read_access_role.name policy_arn = data.aws_iam_policy.AmazonSageMakerReadOnlyAccess.arn } resource &quot;aws_iam_user&quot; &quot;sagemaker_dashboard_read_access_user&quot; { name = local.sagemaker_dashboard_read_access_user_name path = &quot;/&quot; } resource &quot;aws_iam_access_key&quot; &quot;sagemaker_dashboard_read_access_user_credentials&quot; { user = aws_iam_user.sagemaker_dashboard_read_access_user.name } 8.3.7 Dashboard The Dashboard module leverages Terraform and Helm to deploy a custom Vue.js-based dashboard for an ML platform. This deployment is accomplished through the definition of a \"helm_release\" resource. A custom Helm chart, akin to the mlflow deployment, takes center stage. This Helm chart is expected to reside in a directory nested within the Terraform module, conveniently defined as \"${path.module}/helm/\". The configuration for this deployment encompasses critical parameters pivotal to the successful deployment of the dashboard. Notably, it specifies the utilization of a bespoke Docker image, bearing the tag \"seblum/vuejs-ml-dashboard:latest\", meticulously tailored for this specific deployment. Moreover, the deployment name and namespace exhibit dynamic characteristics, rendering the code adaptable to diverse environments and specific requirements. A main aspect of the code revolves around the configuration of an Ingress resource, designed to efficiently route external traffic to the dashboard. This resource employs the \"var.domain_name\" variable to determine the host value, which can signify the domain or subdomain intricately linked to the dashboard. Furthermore, the \"path\" parameter derives its value from the \"var.domain_suffix\" variable, delineating the path through which users can access the ML platform’s dashboard. In essence, this Terraform code exemplifies an indispensable tool, enabling the consistent and streamlined deployment and management of the ML dashboard within the Kubernetes environment. resource &quot;helm_release&quot; &quot;dashboard&quot; { name = var.name namespace = var.namespace create_namespace = var.create_namespace chart = &quot;${path.module}/helm/&quot; values = [yamlencode({ deployment = { image = &quot;seblum/vuejs-ml-dashboard:latest&quot; name = var.name namespace = var.namespace }, ingress = { host = var.domain_name path = var.domain_suffix } })] } It’s worth noting that the Vue.js Dashboard, while underpinned by a free Vue.js template from Creative Tim, has been thoughtfully customized to cater to the specific requirements of the ML platform. For an in-depth exploration of Vue.js, the reader is encouraged to explore external resources, as it falls outside the scope of this documentation. Nevertheless, the complete Vue.js dashboard application is readily accessible here for those with a keen interest. "],["design-decisions.html", "8.4 Design Decisions", " 8.4 Design Decisions "],["use-case-development.html", "Chapter 9 Use Case Development", " Chapter 9 Use Case Development The ML platform aims to enable the full development cycle of ML algorithms within DevOps and MLOps principles. This includes to enable development within a sophisticated IDE such as VSCode, as well as incorporating best practices like Gitflow, automation by CI/CD, and the containerization of code. Whereas the previous chapter 8 explained the ML platform itself and its deployment, this chapter illustrates the workflow of developing machine learning models on the platform. Thus, a use case has been selected and implemented such that it integrates each of the previous mentioned principles and best practices. Skin Cancer Detection The exemplary use case CNN for skin cancer detection is based on a dataset from Kaggle and has been chosen to implement a use case leveraging a Deep Learning model, as well as its data quality which requires little to no data exploration and preprocessing. Data The dataset from Kaggle utilized in this project is obtained from the ISIC (International Skin Image Collaboration) Archive. It consists of 1800 images depicting benign moles and 1497 images representing malignant moles that have already been classified. The primary objective of this use case is to develop a model capable of visually classifying moles as either benign or malignant. All the images in the dataset have been uniformly resized to a lower resolution of 224x224x3 RGB. Additionally, the dataset has already been cleaned and divided into a train set and a test set. This allows for a straightforward implementation that focuses on the crucial aspects of this tutorial, which involve putting machine learning into production and leveraging workflow management and model tracking tools. The data is also conveniently sorted based on the two types, namely benign and malignant. data │ └── train │ │ │ └── benign │ │ ... │ │ │ └── malignant │ ... │ └── test │ │ │ └── benign The dataset is kept in an AWS S3 Bucket, which was deployed on the pre-existing ML Platform. The deployment of the Bucket was accomplished using an Infrastructure-as-a-Code (IaaC) script, and the dataset was uploaded to the Bucket at the same time. To enable the ML pipeline to interact with AWS for data retrieval and storage, a custom utils script in the code base provides the necessary functionality. "],["integrated-development-environment-1.html", "9.1 Integrated Development Environment", " 9.1 Integrated Development Environment Jupyterhub serves as the integrated server environment within the ML platform, providing an Integrated Development Environment (IDE). However, it deviates from the traditional Jupyter Notebooks and instead utilizes VSCode as the IDE. Upon initialization, Jupyterhub clones the GitHub repository mlops-airflow-DAGs that contains the code for the use case. This is the same repository that is synchronized with Airflow to load the provided DAGs. The purpose of this approach is to offer a user-friendly and efficient development experience to platform users. In addition to synchronizing with Airflow, the use of GitHub provides additional tools to streamline development and incorporate DevOps practices effectively. One of these tools is Github Actions, which enables automation through CI/CD (Continuous Integration/Continuous Deployment) and supports the Git workflow. By configuring Github Actions through code, developers can seamlessly integrate these practices into their development processes. The configuration files for Github Actions are also cloned alongside the repository, ensuring that the integration and development processes remain smooth and efficient. 9.1.1 Github Repository The code for the model pipeline is located in the GitHub repository called mlops-airflow-DAGs. It encompasses the setup of an Airflow DAG and the utilization of MLflow. The code responsible for the pipeline functionality can be located in the src subdirectory of the repository. The Airflow DAG is written in the airflow_DAG.py file situated in the root directory of the repository. Additionally, the repository includes a GitHub Actions workflow file located in the .github/workflows/ subdirectory. The Docker subdirectory contains the Dockerfiles for the various containers used in the ML pipeline. These include a container with the code for data preprocessing and model training, a Dockerfile for serving the model using fastAPI, and a file that runs a streamlit app for sending inferences to the served model. root │ │ Readme.md └── .github/workflows/ │ └── cnn_skin_cancer │ │ airflow_docker_DAG.py │ │ airflow_k8s_test_inference_DAG.py │ │ airflow_k8s_workflow_DAG.py │ │ │ └── Docker │ │ └── python-base-cnn-model │ │ └── Dockerfile │ │ ... │ │ │ └── inference_test_images │ │ └── 1.jpg │ │ │ 10.jpg │ │ ... │ │ │ └── src │ └── preprocessing.py │ │ train.py │ │ ... │ └── model │ └── utils.py │ ... │ "],["training-deployment-pipeline-workflow.html", "9.2 Training &amp; Deployment Pipeline Workflow", " 9.2 Training &amp; Deployment Pipeline Workflow The code and machine learning pipeline have been modularized into distinct steps, including preprocessing, model training, model comparison, and model serving. Airflow serves as the model workflow tool, generating DAGs for managing the pipeline. MLflow is integrated to facilitate model tracking, registry, and serving functionalities. To ensure portability and scalability, the codebase has been containerized using Docker, allowing it to be executed in Docker and/or Kubernetes environments. A detailed explanation of how these components function will be provided in the following section. The src code is installed as a Python package within the Docker container, enabling easy invocation within the Airflow DAG. However, it is important to note that although Model Serving is triggered within the Airflow pipeline, the serving itself is not done on the Airflow and the EKS cluster respectively, but the model is served on an AWS Sagemaker instance. 9.2.1 Airflow Workflow The configuration for the Airflow DAG, encompassing its structure, tasks, and task dependencies, can be located within the airflow_k8s_workflow_DAG.py file. The DAG is constructed utilizing the TaskFlow API. In the context of this use case, the ML pipeline comprises three primary phases: preprocessing, training, and serving. The preprocessing phase encompasses data manipulation and its subsequent storage within the S3 repository. The training phase and its associated code are designed to accommodate various TensorFlow models, facilitating parallel training across multiple models, consequently reducing the deployment time. Given the existence of multiple models, it becomes imperative to serve only the model that exhibits the most favorable metrics based on the present data. As a result, an intermediary step is incorporated to compare the metrics of all the models and select the most optimal one for serving. To execute these pipeline phases, the Airflow Kubernetes Operator is utilized. This operator guarantees that each phase operates within an independent and segregated environment using either Docker or Kubernetes jobs. To enable this process, it is necessary to containerize the code using Docker. Subsequently, the Airflow task invokes the relevant Python code methods and executes them in accordance with the specified requirements. 9.2.2 MLflow integration Mlflow is leveraged in the preprocessing and model training stages to store crucial data parameters, model training parameters, and metrics, while also enabling the saving of trained models in the model registry. In the airflow_k8s_workflow_DAG.py file, Mlflow is invoked to create an experiment, and the experiment ID is passed to each pipeline step to store parameters in separate runs. This ensures a clear distinction between the execution of different models. The train_model pipeline steps serve as a container for the model training procedure. Within the container, the model is trained using specific code. All the relevant information about the model and the model itself are logged using mlflow as well. This workflow ensures the comprehensive tracking of model parameters and metrics, and the saved model can be accessed and compared during the subsequent model comparison step. In fact, during this stage, the best model is transferred to another model stage within the model registry. 9.2.3 Pipeline Workflow The code below defines the cnn_skin_cancer_workflow function as an Airflow DAG using the dag decorator. Each step of the pipeline, including data preprocessing, model training, model comparison, and serving the best model, is represented as a separate task with the @task.kubernetes decorator. Dependencies between these tasks are established by passing the output of one task as an argument to the next task. The cnn_skin_cancer_workflow object serves as a representation of the entire DAG. The code starts by importing various Python modules and libraries, defining several variables and parameters, and setting up an Enum class for distinguishing different models and model parameters. import os from enum import Enum import mlflow import pendulum from airflow.decorators import dag, task from airflow.kubernetes.secret import Secret from airflow.models import Variable from airflow.operators.bash import BashOperator from airflow.providers.docker.operators.docker import DockerOperator from kubernetes.client import models as k8s ################################################################################ # # SET VARIOUS PARAMETERS # EXPERIMENT_NAME = &quot;cnn_skin_cancer&quot; # mlflow experiment name skin_cancer_container_image = &quot;seblum/cnn-skin-cancer-model:latest&quot; # base image for k8s pods MLFLOW_TRACKING_URI = Variable.get(&quot;MLFLOW_TRACKING_URI&quot;) ECR_REPOSITORY_NAME = Variable.get(&quot;ECR_REPOSITORY_NAME&quot;) ECR_SAGEMAKER_IMAGE_TAG = Variable.get(&quot;ECR_SAGEMAKER_IMAGE_TAG&quot;) # secrets to pass on to k8s pod secret_name = &quot;airflow-sagemaker-access&quot; SECRET_AWS_ROLE_NAME_SAGEMAKER = Secret( deploy_type=&quot;env&quot;, deploy_target=&quot;AWS_ROLE_NAME_SAGEMAKER&quot;, secret=secret_name, key=&quot;AWS_ROLE_NAME_SAGEMAKER&quot;, ) secret_name = &quot;airflow-aws-account-information&quot; SECRET_AWS_ID = Secret(deploy_type=&quot;env&quot;, deploy_target=&quot;AWS_ID&quot;, secret=secret_name, key=&quot;AWS_ID&quot;) SECRET_AWS_REGION = Secret(deploy_type=&quot;env&quot;, deploy_target=&quot;AWS_REGION&quot;, secret=secret_name, key=&quot;AWS_REGION&quot;) # secrets to pass on to k8s pod secret_name = &quot;airflow-s3-data-bucket-access-credentials&quot; SECRET_AWS_BUCKET = Secret(deploy_type=&quot;env&quot;, deploy_target=&quot;AWS_BUCKET&quot;, secret=secret_name, key=&quot;AWS_BUCKET&quot;) SECRET_AWS_ACCESS_KEY_ID = Secret( deploy_type=&quot;env&quot;, deploy_target=&quot;AWS_ACCESS_KEY_ID&quot;, secret=secret_name, key=&quot;AWS_ACCESS_KEY_ID&quot;, ) SECRET_AWS_SECRET_ACCESS_KEY = Secret( deploy_type=&quot;env&quot;, deploy_target=&quot;AWS_SECRET_ACCESS_KEY&quot;, secret=secret_name, key=&quot;AWS_SECRET_ACCESS_KEY&quot;, ) SECRET_AWS_ROLE_NAME = Secret( deploy_type=&quot;env&quot;, deploy_target=&quot;AWS_ROLE_NAME&quot;, secret=secret_name, key=&quot;AWS_ROLE_NAME&quot;, ) # node_selector and toleration to schedule model training on specific nodes tolerations = [k8s.V1Toleration(key=&quot;dedicated&quot;, operator=&quot;Equal&quot;, value=&quot;t3_large&quot;, effect=&quot;NoSchedule&quot;)] node_selector = {&quot;role&quot;: &quot;t3_large&quot;} # Enum Class to distiguish models class Model_Class(Enum): &quot;&quot;&quot;This enum includes different models.&quot;&quot;&quot; Basic = &quot;Basic&quot; CrossVal = &quot;CrossVal&quot; ResNet50 = &quot;ResNet50&quot; # Set various model params model_params = { &quot;num_classes&quot;: 2, &quot;input_shape&quot;: (224, 224, 3), &quot;activation&quot;: &quot;relu&quot;, &quot;kernel_initializer_glob&quot;: &quot;glorot_uniform&quot;, &quot;kernel_initializer_norm&quot;: &quot;normal&quot;, &quot;optimizer&quot;: &quot;adam&quot;, &quot;loss&quot;: &quot;binary_crossentropy&quot;, &quot;metrics&quot;: [&quot;accuracy&quot;], &quot;validation_split&quot;: 0.2, &quot;epochs&quot;: 2, &quot;batch_size&quot;: 64, &quot;learning_rate&quot;: 1e-5, &quot;pooling&quot;: &quot;avg&quot;, # needed for resnet50 &quot;verbose&quot;: 2, } Afterward, the MLflow tracking URI is set and a function make_mlflow() defined that creates an MLflow experiment or sets an existing one as active. The experiment ID is stored in mlflow_experiment_id. The function is called and the MLflow experiment is set respectively. mlflow.set_tracking_uri(MLFLOW_TRACKING_URI) def make_mlflow() -&gt; str: &quot;&quot;&quot; Creates an MLflow experiment and sets it as the active experiment. Returns: str: The experiment ID of the created or existing MLflow experiment. Example: # Create an MLflow experiment and set it as active experiment_id = make_mlflow() print(f&quot;Active MLflow experiment ID: {experiment_id}&quot;) &quot;&quot;&quot; try: mlflow_experiment_id = mlflow.create_experiment(EXPERIMENT_NAME) except: pass mlflow_experiment_id = mlflow.set_experiment(EXPERIMENT_NAME).experiment_id return mlflow_experiment_id # When dag is loaded, mlflow experiment is created mlflow_experiment_id = make_mlflow() After setting up all secrets, variables, and parameters, the Apache Airflow DAG itself is defined and named cnn_skin_cancer_workflow with default arguments and metadata. It’s the beginning of the DAG definition. All ML pipeline steps are defined within the DAG. The task labeled as preprocessing_op is in charge of the initial step within the workflow. Its primary role involves fetching data from an S3 bucket, executing preprocessing procedures, and subsequently storing the processed data back into the S3 storage. The mlflow_experiment_id parameter is provided as input to the task, and the outcome, including the paths to the preprocessed data, is captured within the preprocessed_data variable. The resulting path to the preprocessed data is communicated through the utilization of an XCOM dictionary. The model_training_op task is specifically designed for the training of deep learning models using the preprocessed data. It offers flexibility in terms of model selection by means of the model_class parameter, which leverages the Enum type for making the choice, for example by specifying Model_Class.ResNet50.name and indicating the intention to train a ResNet50 model. Similar to the prior task, it utilizes identical model_params and input parameters derived from the preprocessed data. The outcomes of this task, containing training-related information, are stored within the train_data_basic variable. The model_comparison_op task fulfills the role of comparing multiple previously trained models based on their accuracy. Additionally, it updates the stage of the models within MLflow to facilitate easier differentiation in subsequent stages. It accepts as input the outcomes of training for the Basic, ResNet50, and CrossVal models (train_data_basic, train_data_resnet50, and train_data_crossval). The results are captured within the compare_models_dict variable. The task known as deploy_model_to_sagemaker_op manages the deployment of the top model to the SageMaker platform. It determines and selects the model with the highest performance based on the information contained within the compare_models_dict. @dag( dag_id=&quot;cnn_skin_cancer_workflow&quot;, default_args={ &quot;owner&quot;: &quot;seblum&quot;, &quot;depends_on_past&quot;: False, &quot;start_date&quot;: pendulum.datetime(2021, 1, 1, tz=&quot;Europe/Amsterdam&quot;), &quot;tags&quot;: [&quot;Keras CNN to classify skin cancer&quot;], }, schedule_interval=None, max_active_runs=1, ) def cnn_skin_cancer_workflow(): &quot;&quot;&quot; Apache Airflow DAG for running a workflow to train, compare, and deploy skin cancer classification models. &quot;&quot;&quot; @task.kubernetes( image=skin_cancer_container_image, task_id=&quot;preprocessing_op&quot;, namespace=&quot;airflow&quot;, env_vars={&quot;MLFLOW_TRACKING_URI&quot;: MLFLOW_TRACKING_URI}, in_cluster=True, get_logs=True, do_xcom_push=True, startup_timeout_seconds=300, service_account_name=&quot;airflow-sa&quot;, secrets=[ SECRET_AWS_BUCKET, SECRET_AWS_REGION, SECRET_AWS_ACCESS_KEY_ID, SECRET_AWS_SECRET_ACCESS_KEY, SECRET_AWS_ROLE_NAME, ], ) def preprocessing_op(mlflow_experiment_id: str) -&gt; dict: &quot;&quot;&quot; Perform data preprocessing. Args: mlflow_experiment_id (str): The MLflow experiment ID. Returns: dict: A dictionary containing the paths to preprocessed data. &quot;&quot;&quot; import os aws_bucket = os.getenv(&quot;AWS_BUCKET&quot;) from src.preprocessing import data_preprocessing ( X_train_data_path, y_train_data_path, X_test_data_path, y_test_data_path, ) = data_preprocessing(mlflow_experiment_id=mlflow_experiment_id, aws_bucket=aws_bucket) # Create dictionary with S3 paths to return return_dict = { &quot;X_train_data_path&quot;: X_train_data_path, &quot;y_train_data_path&quot;: y_train_data_path, &quot;X_test_data_path&quot;: X_test_data_path, &quot;y_test_data_path&quot;: y_test_data_path, } return return_dict @task.kubernetes( image=skin_cancer_container_image, task_id=&quot;model_training_op&quot;, namespace=&quot;airflow&quot;, env_vars={&quot;MLFLOW_TRACKING_URI&quot;: MLFLOW_TRACKING_URI}, in_cluster=True, get_logs=True, do_xcom_push=True, startup_timeout_seconds=300, node_selector=node_selector, tolerations=tolerations, service_account_name=&quot;airflow-sa&quot;, secrets=[ SECRET_AWS_BUCKET, SECRET_AWS_REGION, SECRET_AWS_ACCESS_KEY_ID, SECRET_AWS_SECRET_ACCESS_KEY, SECRET_AWS_ROLE_NAME, ], ) def model_training_op(mlflow_experiment_id: str, model_class: str, model_params: dict, input: dict) -&gt; dict: &quot;&quot;&quot; Train a model. Args: mlflow_experiment_id (str): The MLflow experiment ID. model_class (str): The class of the model to train. model_params (dict): A dictionary containing the model parameters. input (dict): A dictionary containing the input data. Returns: dict: A dictionary containing the results of the model training. &quot;&quot;&quot; import os from src.train import train_model aws_bucket = os.getenv(&quot;AWS_BUCKET&quot;) run_id, model_name, model_version, model_stage = train_model( mlflow_experiment_id=mlflow_experiment_id, model_class=model_class, model_params=model_params, aws_bucket=aws_bucket, import_dict=input, ) return_dict = { &quot;run_id&quot;: run_id, &quot;model_name&quot;: model_name, &quot;model_version&quot;: model_version, &quot;model_stage&quot;: model_stage, } return return_dict @task.kubernetes( image=skin_cancer_container_image, task_id=&quot;compare_models_op&quot;, namespace=&quot;airflow&quot;, env_vars={&quot;MLFLOW_TRACKING_URI&quot;: MLFLOW_TRACKING_URI}, in_cluster=True, get_logs=True, do_xcom_push=True, startup_timeout_seconds=300, service_account_name=&quot;airflow-sa&quot;, # Don&#39;t need Access Secrets as SA is given ) def compare_models_op(train_data_basic: dict, train_data_resnet50: dict, train_data_crossval: dict) -&gt; dict: &quot;&quot;&quot; Compare trained models. Args: train_data_basic (dict): A dictionary containing the results of training the basic model. train_data_resnet50 (dict): A dictionary containing the results of training the ResNet50 model. train_data_crossval (dict): A dictionary containing the results of training the CrossVal model. Returns: dict: A dictionary containing the results of the model comparison. &quot;&quot;&quot; compare_dict = { train_data_basic[&quot;model_name&quot;]: train_data_basic[&quot;run_id&quot;], train_data_resnet50[&quot;model_name&quot;]: train_data_resnet50[&quot;run_id&quot;], train_data_crossval[&quot;model_name&quot;]: train_data_crossval[&quot;run_id&quot;], } print(compare_dict) from src.compare_models import compare_models serving_model_name, serving_model_uri, serving_model_version = compare_models(input_dict=compare_dict) return_dict = { &quot;serving_model_name&quot;: serving_model_name, &quot;serving_model_uri&quot;: serving_model_uri, &quot;serving_model_version&quot;: serving_model_version, } return return_dict @task.kubernetes( image=skin_cancer_container_image, task_id=&quot;deploy_model_to_sagemaker_op&quot;, namespace=&quot;airflow&quot;, env_vars={ &quot;MLFLOW_TRACKING_URI&quot;: MLFLOW_TRACKING_URI, &quot;ECR_REPOSITORY_NAME&quot;: ECR_REPOSITORY_NAME, &quot;ECR_SAGEMAKER_IMAGE_TAG&quot;: ECR_SAGEMAKER_IMAGE_TAG, }, in_cluster=True, get_logs=True, startup_timeout_seconds=300, service_account_name=&quot;airflow-sa&quot;, secrets=[ SECRET_AWS_ROLE_NAME_SAGEMAKER, SECRET_AWS_REGION, SECRET_AWS_ID, ], ) def deploy_model_to_sagemaker_op(serving_model_dict: dict): &quot;&quot;&quot; Deploys a machine learning model to Amazon SageMaker using the specified parameters. Args: serving_model_dict (dict): A dictionary containing information about the model to deploy. It should contain the following keys: - &quot;serving_model_name&quot; (str): The name of the MLflow model to be deployed. - &quot;serving_model_uri&quot; (str): The URI or path to the MLflow model in artifact storage. - &quot;serving_model_version&quot; (str): The version of the MLflow model to deploy. Example: serving_model_info = { &quot;serving_model_name&quot;: &quot;my_mlflow_model&quot;, &quot;serving_model_uri&quot;: &quot;s3://my-bucket/mlflow/models/my_model&quot;, &quot;serving_model_version&quot;: &quot;1&quot;, } deploy_model_to_sagemaker_op(serving_model_info) &quot;&quot;&quot; mlflow_model_name, mlflow_model_uri, mlflow_model_version = ( serving_model_dict[&quot;serving_model_name&quot;], serving_model_dict[&quot;serving_model_uri&quot;], serving_model_dict[&quot;serving_model_version&quot;], ) print(f&quot;mlflow_model_name: {mlflow_model_name}&quot;) print(f&quot;mlflow_model_uri: {mlflow_model_uri}&quot;) print(f&quot;mlflow_model_version: {mlflow_model_version}&quot;) from src.deploy_model_to_sagemaker import deploy_model_to_sagemaker result = deploy_model_to_sagemaker( mlflow_model_name=mlflow_model_name, mlflow_model_uri=mlflow_model_uri, mlflow_model_version=mlflow_model_version, mlflow_experiment_name=&quot;cnn_skin_cancer&quot;, sagemaker_endpoint_name=&quot;test-cnn-skin-cancer&quot;, sagemaker_instance_type=&quot;ml.t2.large&quot;, ) print(f&quot;Script run successfully: {result}&quot;) Finally, all the previously declared tasks are linked to form the Airflow Workflow DAG. The cnn_skin_cancer_workflow function is called at the end of the code respectively. preprocessed_data = preprocessing_op( mlflow_experiment_id=mlflow_experiment_id, ) train_data_basic = model_training_op( mlflow_experiment_id=mlflow_experiment_id, model_class=Model_Class.Basic.name, model_params=model_params, input=preprocessed_data, ) train_data_resnet50 = model_training_op( mlflow_experiment_id=mlflow_experiment_id, model_class=Model_Class.ResNet50.name, model_params=model_params, input=preprocessed_data, ) train_data_crossval = model_training_op( mlflow_experiment_id=mlflow_experiment_id, model_class=Model_Class.CrossVal.name, model_params=model_params, input=preprocessed_data, ) compare_models_dict = compare_models_op(train_data_basic, train_data_resnet50, train_data_crossval) deploy_model_to_sagemaker_op(compare_models_dict) cnn_skin_cancer_workflow() "],["pipeline-workflow-steps.html", "9.3 Pipeline Workflow Steps", " 9.3 Pipeline Workflow Steps As mentioned previously, the machine learning pipeline for this particular use case comprises three primary stages: preprocessing, training, and serving. Furthermore, only the model that achieves the highest accuracy is chosen for deployment, which introduces an additional step for model comparison. Each of these steps will be further explained in the upcoming sections. 9.3.1 Data Preprocessing The data processing stage involves three primary processes. First, the raw data is loaded from an S3 Bucket. Second, the data is preprocessed and converted into the required format. Finally, the preprocessed data is stored in a way that allows it to be utilized by subsequent models. The data processing functionality is implemented within the given data_preprocessing function. The utils module, imported at the beginning, provides the functionality to access, load, and store data from S3. The data is normalized and transformed into a NumPy array to make it compatible with TensorFlow Keras models. The function returns the names and paths of the preprocessed and uploaded data, making it convenient for selecting them for future model training. Moreover, the data preprocessing stage establishes a connection with MLflow to record the sizes of the datasets. At the beginning of each pipeline step are the import statements for various Python modules and libraries used in the code. They will not be mentioned within the description, but you can look them up in the provided code. The code starts with the function definition data_preprocessing. The function takes several input parameters and returns a tuple with paths to the preprocessed data stored as NumPy arrays. The function is equipped with a @timeit decorator, which serves to measure the execution time of the preprocessing operation. @timeit def data_preprocessing( mlflow_experiment_id: str, aws_bucket: str, path_preprocessed: str = &quot;preprocessed&quot;, ) -&gt; Tuple[str, str, str, str]: &quot;&quot;&quot;Preprocesses data for further use within model training. Raw data is read from given S3 Bucket, normalized, and stored ad a NumPy Array within S3 again. Output directory is on &quot;/preprocessed&quot;. The shape of the data set is logged to MLflow. Args: mlflow_experiment_id (str): Experiment ID of the MLflow run to log data aws_bucket (str): S3 Bucket to read raw data from and write preprocessed data path_preprocessed (str, optional): Subdirectory to store the preprocessed data on the provided S3 Bucket. Defaults to &quot;preprocessed&quot;. Returns: Tuple[str, str, str, str]: Four strings denoting the path of the preprocessed data stored as NumPy Arrays: X_train_data_path, y_train_data_path, X_test_data_path, y_test_data_path &quot;&quot;&quot; It continues by retrieving the MLflow tracking URI from an environment variable and setting it as the tracking URI for MLflow. An AWS session is set up based on AWS Access Key and AWS Secret Access Key, which is similarly provided using environment variables. The AWSSession Object is a custom class to handle the interaction with S3 buckets. Afterward. the paths within an S3 bucket for different data folders are defined, including training and testing data for benign and malignant cases. mlflow_tracking_uri = os.getenv(&quot;MLFLOW_TRACKING_URI&quot;) mlflow.set_tracking_uri(mlflow_tracking_uri) # Instantiate aws session based on AWS Access Key # AWS Access Key is fetched within AWS Session by os.getenv aws_session = AWSSession() aws_session.set_sessions() # Set paths within s3 path_raw_data = f&quot;s3://{aws_bucket}/data/&quot; folder_benign_train = f&quot;{path_raw_data}train/benign&quot; folder_malignant_train = f&quot;{path_raw_data}train/malignant&quot; folder_benign_test = f&quot;{path_raw_data}test/benign&quot; folder_malignant_test = f&quot;{path_raw_data}test/malignant&quot; The code proceeds by defining three hidden functions that play a crucial role in the preprocessing of the data: Image Loading and Conversion: A function named _load_and_convert_images designed to handle the loading and conversion of images retrieved from an S3 bucket folder into a NumPy array. It utilizes the aws_session to access and process these images. Creating Labels: The following function, _create_label, serves the purpose of generating a label array suitable for a given dataset. This function is responsible for preparing labels essential for classification tasks. The inner function, create_label, undertakes the task of constructing a label array that corresponds to a specific dataset, thus facilitating classification operations. Merging Data: Lastly, the function _merge_data is introduced to combine two datasets into a unified dataset. This inner function, _merge_data_, plays the role of merging two distinct datasets into a single, consolidated dataset. It effectively combines data originating from diverse sources, allowing for a comprehensive dataset. @timeit def _load_and_convert_images(folder_path: str) -&gt; np.array: &quot;&quot;&quot; Loads and converts images from an S3 bucket folder into a NumPy array. Args: folder_path (str): The path to the S3 bucket folder. Returns: np.array: The NumPy array containing the converted images. Raises: None &quot;&quot;&quot; ims = [ aws_session.read_image_from_s3(s3_bucket=aws_bucket, imname=filename) for filename in tqdm(aws_session.list_files_in_bucket(folder_path)) ] return np.array(ims, dtype=&quot;uint8&quot;) def _create_label(x_dataset: np.array) -&gt; np.array: &quot;&quot;&quot; Creates label array for the given dataset. Args: x_dataset (np.array): The dataset for which labels are to be created. Returns: np.array: The label array. Raises: None &quot;&quot;&quot; return np.zeros(x_dataset.shape[0]) def _merge_data(set_one: np.array, set_two: np.array) -&gt; np.array: &quot;&quot;&quot; Merges two datasets into a single dataset. Args: set_one (np.array): The first dataset. set_two (np.array): The second dataset. Returns: np.array: The merged dataset. Raises: None &quot;&quot;&quot; return np.concatenate((set_one, set_two), axis=0) The code continues further with the actual data preprocessing steps, including calling the previously stated hidden functions and loading images, creating labels, merging data, and performing data normalization and augmentation. An MLflow run is created for this step to log parameters such as the training and testing size of the data. # Start a MLflow run to log the size of the data timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;) with mlflow.start_run(experiment_id=mlflow_experiment_id, run_name=f&quot;{timestamp}_Preprocessing&quot;) as run: print(&quot;\\n&gt; Loading images from S3...&quot;) # Load in training pictures X_benign = _load_and_convert_images(folder_benign_train) X_malignant = _load_and_convert_images(folder_malignant_train) # Load in testing pictures X_benign_test = _load_and_convert_images(folder_benign_test) X_malignant_test = _load_and_convert_images(folder_malignant_test) # Log train-test size in MLflow print(&quot;\\n&gt; Log data parameters&quot;) mlflow.log_param(&quot;train_size_benign&quot;, X_benign.shape[0]) mlflow.log_param(&quot;train_size_malignant&quot;, X_malignant.shape[0]) mlflow.log_param(&quot;test_size_benign&quot;, X_benign_test.shape[0]) mlflow.log_param(&quot;test_size_malignant&quot;, X_malignant_test.shape[0]) print(&quot;\\n&gt; Preprocessing...&quot;) # Create labels y_benign = _create_label(X_benign) y_malignant = _create_label(X_malignant) y_benign_test = _create_label(X_benign_test) y_malignant_test = _create_label(X_malignant_test) # Merge data y_train = _merge_data(y_benign, y_malignant) y_test = _merge_data(y_benign_test, y_malignant_test) X_train = _merge_data(X_benign, X_malignant) X_test = _merge_data(X_benign_test, X_malignant_test) # Shuffle data X_train, y_train = shuffle(X_train, y_train) X_test, y_test = shuffle(X_test, y_test) y_train = to_categorical(y_train, num_classes=2) y_test = to_categorical(y_test, num_classes=2) # With data augmentation to prevent overfitting X_train = X_train / 255.0 X_test = X_test / 255.0 Once the data has been normalized and converted into a NumPy array, it is then transferred to an S3 bucket with the assistance of the aws_session. The function returns the names and paths of the preprocessed data that has been uploaded. This information is made available for use in subsequent functions. print(&quot;\\n&gt; Upload numpy arrays to S3...&quot;) aws_session.upload_npy_to_s3( data=X_train, s3_bucket=aws_bucket, file_key=f&quot;{path_preprocessed}/X_train.pkl&quot;, ) aws_session.upload_npy_to_s3( data=y_train, s3_bucket=aws_bucket, file_key=f&quot;{path_preprocessed}/y_train.pkl&quot;, ) aws_session.upload_npy_to_s3( data=X_test, s3_bucket=aws_bucket, file_key=f&quot;{path_preprocessed}/X_test.pkl&quot;, ) aws_session.upload_npy_to_s3( data=y_test, s3_bucket=aws_bucket, file_key=f&quot;{path_preprocessed}/y_test.pkl&quot;, ) X_train_data_path = f&quot;{path_preprocessed}/X_train.pkl&quot; y_train_data_path = f&quot;{path_preprocessed}/y_train.pkl&quot; X_test_data_path = f&quot;{path_preprocessed}/X_test.pkl&quot; y_test_data_path = f&quot;{path_preprocessed}/y_test.pkl&quot; return X_train_data_path, y_train_data_path, X_test_data_path, y_test_data_path 9.3.2 Model Training The training step is designed to accommodate different models based on the selected model. The custom model.utils package, imported at the beginning, enables the selection and retrieval of models. The chosen model can be specified by passing its name to the get_model function, which then returns the corresponding model. These models are implemented using TensorFlow Keras and their code is stored in the /model directory. The model is trained using the model_params parameters provided to the training function, which include all the necessary hyperparameters. The training and evaluation are conducted using the preprocessed data from the previous step, which is downloaded from S3 at the beginning. Depending on the selected model, a KFold cross-validation is performed to improve the model’s fit. MLflow is utilized to track the model’s progress. By invoking mlflow.start_run(), a new MLflow run is initiated. The model_params are logged using mlflow.log_params, and MLflow autolog is enabled for Keras models through mlflow.keras.autolog(). After successful training, the models are stored in the model registry. The trained model is logged using mlflow.keras.register_model, with the specified model_name as the destination. The Function Definition train_model takes several input parameters, including mlflow_experiment_id, model_class, model_params, aws_bucket, and an optional import_dict for importing data. It returns a tuple containing information about the run and model. The code of the functions starts by setting the tracking URI for MLflow. def train_model( mlflow_experiment_id: str, model_class: Enum, model_params: dict, aws_bucket: str, import_dict: dict = {}, ) -&gt; Tuple[str, str, int, str]: &quot;&quot;&quot; Trains a machine learning model and logs the results to MLflow. Args: mlflow_experiment_id (str): The ID of the MLflow experiment to log the results. model_class (Enum): The class of the model to train. model_params (dict): A dictionary containing the parameters for the model. aws_bucket (str): The AWS S3 bucket name for data storage. import_dict (dict, optional): A dictionary containing paths for importing data. Defaults to {}. Returns: Tuple[str, str, int, str]: A tuple containing the run ID, model name, model version, and current stage. Raises: None &quot;&quot;&quot; mlflow_tracking_uri = os.getenv(&quot;MLFLOW_TRACKING_URI&quot;) mlflow.set_tracking_uri(mlflow_tracking_uri) Afterward, the data required for training and testing the model is loaded from AWS S3 buckets. It fetches file paths from the import_dict dictionary, and instantiates an AWSSession. It uses the AWSSession class to download NumPy arrays from the specified S3 bucket. print(&quot;\\n&gt; Loading data...&quot;) X_train_data_path = import_dict.get(&quot;X_train_data_path&quot;) y_train_data_path = import_dict.get(&quot;y_train_data_path&quot;) X_test_data_path = import_dict.get(&quot;X_test_data_path&quot;) y_test_data_path = import_dict.get(&quot;y_test_data_path&quot;) # Instantiate aws session based on AWS Access Key # AWS Access Key is fetched within AWS Session by os.getenv aws_session = AWSSession() aws_session.set_sessions() # Read NumPy Arrays from S3 X_train = aws_session.download_npy_from_s3(s3_bucket=aws_bucket, file_key=X_train_data_path) y_train = aws_session.download_npy_from_s3(s3_bucket=aws_bucket, file_key=y_train_data_path) X_test = aws_session.download_npy_from_s3(s3_bucket=aws_bucket, file_key=X_test_data_path) y_test = aws_session.download_npy_from_s3(s3_bucket=aws_bucket, file_key=y_test_data_path) The training of the machine learning model is contingent upon the chosen model class. If the model_class is set to Model_Class.CrossVal, the training process involves k-fold cross-validation using the BasicNet as the model. Conversely, if the model_class is anything other than Model_Class.CrossVal, the model undergoes training without cross-validation, following the specifications associated with the provided class and parameters. The selection of the model is based on the model_class Enum, which could be either Model_Class.CrossVal or Model_Class.ResNet50. During the training process, the mlflow.autolog functionality is employed, allowing for the automatic logging of all essential run parameters. This feature can be enabled or disabled as needed to facilitate model training and parameter logging. print(&quot;\\n&gt; Training model...&quot;) print(model_class) timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;) with mlflow.start_run(experiment_id=mlflow_experiment_id, run_name=f&quot;{timestamp}-{model_class}&quot;) as run: mlflow.log_params(model_params) learning_rate_reduction = ReduceLROnPlateau(monitor=&quot;accuracy&quot;, patience=5, verbose=1, factor=0.5, min_lr=1e-7) # If CrossVal is selected, train BasicNet as Cross-Validated Model if model_class == Model_Class.CrossVal.value: kfold = KFold(n_splits=3, shuffle=True, random_state=11) cvscores = [] for train, test in kfold.split(X_train, y_train): model = get_model(Model_Class.Basic.value, model_params) # Train Model model.fit( X_train[train], y_train[train], epochs=model_params.get(&quot;epochs&quot;), batch_size=model_params.get(&quot;batch_size&quot;), verbose=model_params.get(&quot;verbose&quot;), ) scores = model.evaluate(X_train[test], y_train[test], verbose=0) print(&quot;%s: %.2f%%&quot; % (model.metrics_names[1], scores[1] * 100)) cvscores.append(scores[1] * 100) K.clear_session() # TODO: not very safe, create if-else on other Enums else: model = get_model(model_class, model_params) mlflow.keras.autolog() # Train Model model.fit( X_train, y_train, validation_split=model_params.get(&quot;validation_split&quot;), epochs=model_params.get(&quot;epochs&quot;), batch_size=model_params.get(&quot;batch_size&quot;), verbose=model_params.get(&quot;verbose&quot;), callbacks=[learning_rate_reduction], ) mlflow.keras.autolog(disable=True) run_id = run.info.run_id model_uri = f&quot;runs:/{run_id}/{model_class}&quot; After model training, the trained model is tested on a separate test dataset and log the prediction accuracy as a metric in MLflow. The model is then registered with MLflow and necessary metadata about the registered model are stored in the variable mv. # Testing model on test data to evaluate print(&quot;\\n&gt; Testing model...&quot;) y_pred = model.predict(X_test) prediction_accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)) mlflow.log_metric(&quot;prediction_accuracy&quot;, prediction_accuracy) print(f&quot;Prediction Accuracy: {prediction_accuracy}&quot;) print(&quot;\\n&gt; Register model...&quot;) mv = mlflow.register_model(model_uri, model_class) Finally, the function returns a tuple containing the run ID and crucial information about the model, such as its name, version, and stage. return run_id, mv.name, mv.version, mv.current_stage 9.3.3 Model Comparison The compare_models function is responsible for model comparison within MLflow. It evaluates MLflow models by considering a specified metric and promotes the best-performing model to the \"Staging\" stage within the MLflow Registry. This function accepts two arguments: input_dict, which is a dictionary containing model names and run IDs, and an optional metric parameter (defaulting to \"prediction_accuracy\") used for conducting the comparison. def compare_models(input_dict: dict, metric: str = &quot;prediction_accuracy&quot;) -&gt; Tuple[str, str, int]: &quot;&quot;&quot; Compares a given set of MLflow models based on their logged metric. The model with the best metric will be transferred to a &quot;Staging&quot; stage within the MLflow Registry. Args: input_dict (dict): A dictionary containing the names and run IDs of the MLflow models to compare. metric (str, optional): The metric to compare the models. Defaults to &quot;prediction_accuracy&quot;. Returns: Tuple[str, str, int]: A tuple containing the name of the best performing model, its MLflow URI, and the version of the model. Raises: None &quot;&quot;&quot; mlflow_tracking_uri = os.getenv(&quot;MLFLOW_TRACKING_URI&quot;) mlflow.set_tracking_uri(mlflow_tracking_uri) Once the MLflow tracking URI is configured to align with the “MLFLOW_TRACKING_URI” environment variable, an MLflow client is established. This client is integral to a process known as the Model Comparison Loop. In this loop, models listed in the input_dict dictionary are individually processed. The loop retrieves their respective metrics and aggregates this data within the all_results dictionary, facilitating direct model comparisons. client = mlflow.MlflowClient(tracking_uri=mlflow_tracking_uri) all_results = {} for key, value in input_dict.items(): # extract params/metrics data for run `test_run_id` in a single dict model_results_data_dict = client.get_run(value).data.to_dictionary() # get params and metrics for this run (test_run_id) model_results_accuracy = model_results_data_dict[&quot;metrics&quot;][metric] all_results[key] = model_results_accuracy # Get model with maximum accuracy serving_model_name = max(all_results, key=all_results.get) serving_model_version = client.get_latest_versions(name=serving_model_name, stages=[&quot;None&quot;])[0].version print(f&quot;acc_dict: {all_results}&quot;) print(f&quot;acc_dict_model: {serving_model_name}&quot;) print(f&quot;latest_model_version: {serving_model_version}&quot;) Subsequently, the model with the highest accuracy, as determined from the gathered metrics, is singled out for special attention. This exceptional model is then moved to the \"Staging\" stage within the MLflow Registry, indicating its preparedness for subsequent evaluation and deployment. As a result, a tuple is returned, containing the name of the top-performing model, its corresponding MLflow URI, and the model’s version information. # Transition model to stage &quot;Staging&quot; model_stage = &quot;Staging&quot; client.transition_model_version_stage(name=serving_model_name, version=serving_model_version, stage=model_stage) serving_model_uri = f&quot;models:/{serving_model_name}/{model_stage}&quot; return serving_model_name, serving_model_uri, serving_model_version 9.3.4 Model Deployment &amp; Serving This code serves a specific purpose: to simplify the deployment of MLflow models to AWS SageMaker. It achieves this by meticulously configuring and overseeing the deployment process through a combination of environment variables and utility functions. By making use of MLflow’s SageMaker integration, this code automates the deployment of a pre-trained TensorFlow model from the MLflow registry to a SageMaker instance. The primary responsibility for deploying an MLflow model to an AWS SageMaker endpoint lies with the deploy_model_to_sagemaker function. To kickstart the deployment process, various AWS and MLflow-related environment variables, essential for the deployment, are initially fetched. def deploy_model_to_sagemaker( mlflow_model_name: str, mlflow_model_uri: str, mlflow_experiment_name: str, mlflow_model_version: int, sagemaker_endpoint_name: str, sagemaker_instance_type: str, ) -&gt; bool: &quot;&quot;&quot; Deploy a machine learning model to AWS SageMaker from MLflow. This function deploys an MLflow model to an AWS SageMaker endpoint using the specified configuration. Args: mlflow_model_name (str): The name of the MLflow model to deploy. mlflow_model_uri (str): The URI of the MLflow model from the registry. mlflow_experiment_name (str): The name of the MLflow experiment containing the model. mlflow_model_version (int): The version of the MLflow model to deploy. sagemaker_endpoint_name (str): The desired name for the SageMaker endpoint. sagemaker_instance_type (str): The SageMaker instance type for deployment. Returns: bool: True if the task was completed successfully, otherwise False. &quot;&quot;&quot; # Retrieve AWS and MLflow environment variables AWS_ID = os.getenv(&quot;AWS_ID&quot;) AWS_REGION = os.getenv(&quot;AWS_REGION&quot;) AWS_ACCESS_ROLE_NAME_SAGEMAKER = os.getenv(&quot;AWS_ROLE_NAME_SAGEMAKER&quot;) ECR_REPOSITORY_NAME = os.getenv(&quot;ECR_REPOSITORY_NAME&quot;) ECR_SAGEMAKER_IMAGE_TAG = os.getenv(&quot;ECR_SAGEMAKER_IMAGE_TAG&quot;) MLFLOW_TRACKING_URI = os.getenv(&quot;MLFLOW_TRACKING_URI&quot;) # Set the MLflow tracking URI mlflow.set_tracking_uri(MLFLOW_TRACKING_URI) Within this main deployment function, there are three utility functions that have been defined: _build_image_url: This function constructs the ECR image URL required for SageMaker. _build_execution_role_arn: It is responsible for generating the SageMaker execution role ARN. _get_mlflow_parameters: This utility function retrieves crucial MLflow model parameters, including the model URI and source code. def _build_image_url( aws_id: str, aws_region: str, ecr_repository_name: str, ecr_sagemaker_image_tag: str, ) -&gt; str: &quot;&quot;&quot; Build the ECR image URL for SageMaker. Args: aws_id (str): AWS account ID. aws_region (str): AWS region. ecr_repository_name (str): Name of the ECR repository. ecr_sagemaker_image_tag (str): Tag for the ECR image. Returns: str: The ECR image URL for SageMaker. &quot;&quot;&quot; image_url = f&quot;{aws_id}.dkr.ecr.{aws_region}.amazonaws.com/{ecr_repository_name}:{ecr_sagemaker_image_tag}&quot; return image_url def _build_execution_role_arn(aws_id: str, access_role_name: str) -&gt; str: &quot;&quot;&quot; Build the SageMaker execution role ARN. Args: aws_id (str): AWS account ID. access_role_name (str): SageMaker execution role name. Returns: str: The SageMaker execution role ARN. &quot;&quot;&quot; execution_role_arn = f&quot;arn:aws:iam::{aws_id}:role/{access_role_name}&quot; return execution_role_arn def _get_mlflow_parameters(experiment_name: str, model_name: str, model_version: int) -&gt; (str, str, str): &quot;&quot;&quot; Retrieve MLflow model parameters. Args: experiment_name (str): Name of the MLflow experiment. model_name (str): Name of the MLflow model. model_version (int): Version of the MLflow model. Returns: Tuple[str, str]: The model URI and source. &quot;&quot;&quot; client = mlflow.MlflowClient() model_version_details = client.get_model_version( name=model_name, version=model_version, ) # This is for local # experiment_id = dict(mlflow.get_experiment_by_name(experiment_name))[&quot;experiment_id&quot;] # run_id = model_version_details.run_id # model_uri = f&quot;mlruns/{experiment_id}/{run_id}/artifacts/{model_name}&quot; model_source = model_version_details.source model_source_adapted = f&quot;{model_source.removesuffix(model_name)}model&quot; return model_source, model_source_adapted Following their definition, these utility functions are invoked sequentially. Initially, the ECR image URL for SageMaker and the SageMaker execution role ARN are constructed using _build_image_url and _build_execution_role_arn. Subsequently, _get_mlflow_parameters is called to retrieve pertinent MLflow model parameters, encompassing the model source and any adaptations made to the source code. image_url = _build_image_url( aws_id=AWS_ID, aws_region=AWS_REGION, ecr_repository_name=ECR_REPOSITORY_NAME, ecr_sagemaker_image_tag=ECR_SAGEMAKER_IMAGE_TAG, ) execution_role_arn = _build_execution_role_arn(aws_id=AWS_ID, access_role_name=AWS_ACCESS_ROLE_NAME_SAGEMAKER) model_source, model_source_adapted = _get_mlflow_parameters( experiment_name=mlflow_experiment_name, model_name=mlflow_model_name, model_version=mlflow_model_version, ) print(f&quot;model_source: {model_source}&quot;) print(f&quot;mlflow_model_uri: {mlflow_model_uri}&quot;) print(f&quot;model_source_adapted: {model_source_adapted}&quot;) Each of these preceding steps is considered a prerequisite to the actual deployment of the model to SageMaker. The MLflow model deployment to an AWS SageMaker endpoint is executed via MLflow’s SageMaker integration. This is accomplished by invoking mlflow.sagemaker._deploy with all the previously accumulated information. Given that the setup of the SageMaker instance and environment can be time-consuming, a suitable timeout is established. Ultimately, the function returns True upon successful completion of the deployment task; otherwise, it returns False. mlflow.sagemaker._deploy( mode=&quot;create&quot;, app_name=sagemaker_endpoint_name, model_uri=model_source_adapted, image_url=image_url, execution_role_arn=execution_role_arn, instance_type=sagemaker_instance_type, instance_count=1, region_name=AWS_REGION, timeout_seconds=2400, ) return True "],["model-inferencing.html", "9.4 Model Inferencing", " 9.4 Model Inferencing For testing model inference, there is a separate Airflow DAG tailored specifically for conducting inference tests on a CNN SageMaker deployment. This DAG is responsible for various tasks, including verifying the status of the SageMaker endpoint, handling multiple sample images for inference, and retrieving predictions from the endpoint. The configuration of this DAG includes essential metadata and execution parameters. 9.4.1 Pipeline Workflow Similar to the cnn_skin_cancer_workflow DAG, this code commences by importing the necessary modules and libraries required for the Airflow DAG. Furthermore, it configures specific parameters, including the skin_cancer_container_image, which represents the container image used for Kubernetes pods, and the SECRET_AWS_REGION, a secret housing AWS region information, later passed to the container as an environment variable. import pendulum from airflow.decorators import dag, task from airflow.kubernetes.secret import Secret from airflow.models import Variable # SET PARAMETERS skin_cancer_container_image = &quot;seblum/cnn-skin-cancer-model:latest&quot; # base image for k8s pods SECRET_AWS_REGION = Secret( deploy_type=&quot;env&quot;, deploy_target=&quot;AWS_REGION&quot;, secret=&quot;airflow-aws-account-information&quot;, key=&quot;AWS_REGION&quot; ) The Airflow DAG itself, named cnn_skin_cancer_sagemaker_inference_test, is then defined, complete with its metadata, scheduling details, and associated tasks. Within this DAG definition, there exists an inference task known as inference_call_op, which is established using the @task.kubernetes decorator. This task is responsible for conducting inference on a SageMaker endpoint, processing multiple images. It is configured with the previously defined secret and container image. @dag( dag_id=&quot;cnn_skin_cancer_sagemaker_test_inference&quot;, default_args={ &quot;owner&quot;: &quot;seblum&quot;, &quot;depends_on_past&quot;: False, &quot;start_date&quot;: pendulum.datetime(2021, 1, 1, tz=&quot;Europe/Amsterdam&quot;), &quot;tags&quot;: [&quot;Inference test on CNN sagemaker deployment&quot;], }, schedule_interval=None, max_active_runs=1, ) def cnn_skin_cancer_sagemaker_inference_test(): &quot;&quot;&quot; Apache Airflow DAG for testing inference on a CNN SageMaker deployment. &quot;&quot;&quot; @task.kubernetes( image=skin_cancer_container_image, task_id=&quot;inference_call_op&quot;, namespace=&quot;airflow&quot;, in_cluster=True, get_logs=True, startup_timeout_seconds=300, service_account_name=&quot;airflow-sa&quot;, secrets=[ SECRET_AWS_REGION, ], ) def inference_call_op(): &quot;&quot;&quot; Perform inference on a SageMaker endpoint with multiple images. &quot;&quot;&quot; import json from src.inference_to_sagemaker import ( endpoint_status, get_image_directory, preprocess_image, query_endpoint, read_imagefile, ) Inside the inference_call_op task, a sequence of actions takes place, encompassing SageMaker endpoint status verification, image data preparation, image preprocessing, and the actual inference process. Since the Airflow workflow comprises only a single step, the function is called directly following its definition. Subsequently, the DAG is executed by invoking the cnn_skin_cancer_sagemaker_inference_test() function. sagemaker_endpoint_name = &quot;test-cnn-skin-cancer&quot; image_directoy = get_image_directory() print(f&quot;Image directory: {image_directoy}&quot;) filenames = [&quot;1.jpg&quot;, &quot;10.jpg&quot;, &quot;1003.jpg&quot;, &quot;1005.jpg&quot;, &quot;1007.jpg&quot;] for file in filenames: filepath = f&quot;{image_directoy}/{file}&quot; print(f&quot;[+] New Inference&quot;) print(f&quot;[+] FilePath is {filepath}&quot;) # Check endpoint status print(&quot;[+] Endpoint Status&quot;) print(f&quot;Application status is {endpoint_status(sagemaker_endpoint_name)}&quot;) image = read_imagefile(filepath) print(&quot;[+] Preprocess Data&quot;) np_image = preprocess_image(image) # Add instances fiels so np_image can be inferenced by MLflow model payload = json.dumps({&quot;instances&quot;: np_image.tolist()}) print(&quot;[+] Prediction&quot;) predictions = query_endpoint(app_name=sagemaker_endpoint_name, data=payload) print(f&quot;Received response for {file}: {predictions}&quot;) inference_call_op() cnn_skin_cancer_sagemaker_inference_test() 9.4.2 Inference Workflow Code Collectively, these functions provide comprehensive support for testing and interacting with an Amazon SageMaker endpoint. Their functionality encompasses tasks such as image data preparation and processing, endpoint status verification, and querying the endpoint to obtain predictions or responses. 9.4.2.0.1 get_image_directory Function This function is responsible for retrieving the absolute file path for the ‘inference_test_images’ directory, relative to the current script’s location. def get_image_directory() -&gt; str: &quot;&quot;&quot; Get the file path for the &#39;inference_test_images&#39; directory relative to the current script&#39;s location. Returns: str: The absolute file path to the &#39;inference_test_images&#39; directory. &quot;&quot;&quot; path = f&quot;{Path(__file__).parent.parent}/inference_test_images&quot; return path 9.4.2.0.2 read_imagefile Function The read_imagefile function is designed to read an image file, which can be either from a file path or binary data, and return it as a PIL JpegImageFile object. def read_imagefile(data: str) -&gt; JpegImageFile: &quot;&quot;&quot; Reads an image file and returns it as a PIL JpegImageFile object. Args: data (str): The file path or binary data representing the image. Returns: PIL.JpegImagePlugin.JpegImageFile: A PIL JpegImageFile object representing the image. Example: # Read an image file from a file path image_path = &quot;example.jpg&quot; image = read_imagefile(image_path) # Read an image file from binary data with open(&quot;example.jpg&quot;, &quot;rb&quot;) as file: binary_data = file.read() image = read_imagefile(binary_data) &quot;&quot;&quot; image = Image.open(data) return image 9.4.2.0.3 preprocess_image Function The preprocess_image function plays a crucial role in preprocessing a JPEG image for deep learning models. It performs several operations, including converting the image to a NumPy array, scaling its values to fall within the 0 to 1 range, and reshaping it to match the expected input shape for the model. def preprocess_image(image: JpegImageFile) -&gt; np.array: &quot;&quot;&quot; Preprocesses a JPEG image for deep learning models. Args: image (PIL.JpegImagePlugin.JpegImageFile): A PIL image object in JPEG format. Returns: np.ndarray: A NumPy array representing the preprocessed image. The image is converted to a NumPy array with data type &#39;uint8&#39;, scaled to values between 0 and 1, and reshaped to (1, 224, 224, 3). Example: # Load an image using PIL image = Image.open(&quot;example.jpg&quot;) # Preprocess the image preprocessed_image = preprocess_image(image) &quot;&quot;&quot; np_image = np.array(image, dtype=&quot;uint8&quot;) np_image = np_image / 255.0 np_image = np_image.reshape(1, 224, 224, 3) return np_image 9.4.2.0.4 endpoint_status Function The endpoint_status function is responsible for checking the status of an Amazon SageMaker endpoint. It takes the app_name as input, which presumably represents the name or identifier of the endpoint. def endpoint_status(app_name: str) -&gt; str: &quot;&quot;&quot; Checks the status of an Amazon SageMaker endpoint. Args: app_name (str): The name of the SageMaker endpoint to check. Returns: str: The current status of the SageMaker endpoint. Example: # Check the status of a SageMaker endpoint endpoint_name = &quot;my-endpoint&quot; status = endpoint_status(endpoint_name) print(f&quot;Endpoint status: {status}&quot;) &quot;&quot;&quot; AWS_REGION = os.getenv(&quot;AWS_REGION&quot;) sage_client = boto3.client(&quot;sagemaker&quot;, region_name=AWS_REGION) endpoint_description = sage_client.describe_endpoint(EndpointName=app_name) endpoint_status = endpoint_description[&quot;EndpointStatus&quot;] return endpoint_status 9.4.2.0.5 query_endpoint Function The query_endpoint function is responsible for querying an Amazon SageMaker endpoint using input data provided in JSON format. It then retrieves predictions or responses from the endpoint based on the provided input. def query_endpoint(app_name: str, data: str) -&gt; json: &quot;&quot;&quot; Queries an Amazon SageMaker endpoint with input data and retrieves predictions. Args: app_name (str): The name of the SageMaker endpoint to query. data (str): Input data in JSON format to send to the endpoint. Returns: dict: The prediction or response obtained from the SageMaker endpoint. Example: # Query a SageMaker endpoint with JSON data endpoint_name = &quot;my-endpoint&quot; input_data = &#39;{&quot;feature1&quot;: 0.5, &quot;feature2&quot;: 1.2}&#39; prediction = query_endpoint(endpoint_name, input_data) print(f&quot;Endpoint prediction: {prediction}&quot;) &quot;&quot;&quot; AWS_REGION = os.getenv(&quot;AWS_REGION&quot;) client = boto3.session.Session().client(&quot;sagemaker-runtime&quot;, AWS_REGION) response = client.invoke_endpoint( EndpointName=app_name, Body=data, ContentType=&quot;application/json&quot;, ) prediction = response[&quot;Body&quot;].read().decode(&quot;ascii&quot;) prediction = json.loads(prediction) return prediction "],["glossary.html", "Glossary", " Glossary AI Artificial Intelligence - The simulation of human intelligence processes by machines, often used to perform tasks that typically require human intelligence. AWS Amazon Web Services - A comprehensive cloud computing platform provided by Amazon, offering a wide range of cloud services for storage, computation, and more. Azure Microsoft Azure - Microsoft’s cloud computing platform, providing services for building, deploying, and managing applications and services through Microsoft-managed data centers. CI/CD Continuous Integration/Continuous Deployment - Practices that involve automatically integrating code changes into a shared repository and deploying them to production or testing environments. DevOps Development and Operations - A set of practices and cultural philosophies that aim to automate and integrate the processes of software development and IT operations. DVC Data Version Control - A system for managing and versioning machine learning data, ensuring consistent and reproducible data sets for machine learning projects. EC2 Elastic Compute Cloud - A web service that offers resizable compute capacity in the cloud, allowing users to run virtual machines (EC2 instances) for various computing tasks. EKS Elastic Kubernetes Service - A managed container orchestration service that simplifies the deployment, management, and scaling of containerized applications using Kubernetes. ETL Extract, Transform, Load - A process in data management that involves extracting data from various sources, transforming it into a usable format, and loading it into a destination system. GCP Google Cloud Platform - Google’s suite of cloud computing services, providing a range of infrastructure and platform services for building and running applications. Git LFS Git Large File Storage - An extension to Git that allows handling large files more efficiently, often used in software development to manage binary assets. IAM Identity and Access Management - A service that allows organizations to manage access to their AWS resources securely by defining and controlling user permissions and policies. IDE Integrated Development Environment - A software application that provides comprehensive tools and features for software development, including code editing, debugging, and version control. JSON JavaScript Object Notation - A lightweight data interchange format that is human-readable and easy for machines to parse, commonly used for data exchange between systems. MLOps Machine Learning Operations - The combination of machine learning and DevOps practices, focusing on automating and managing the end-to-end machine learning lifecycle. ML Machine Learning - A subset of artificial intelligence that involves algorithms and models enabling computers to learn patterns from data and make predictions or decisions. RDS Relational Database Service - A managed database service that streamlines the setup, operation, and scaling of relational databases like MySQL, PostgreSQL, and others. S3 Simple Storage Service - An object storage service designed for storing and retrieving data, known for its scalability, durability, and security features. VPC Virtual Private Cloud - A virtual network service that enables users to create isolated and customizable network environments within their cloud infrastructure. VM Virtual Machine - An emulation of a computer system that allows multiple operating systems to run on a single physical machine, providing isolation and flexibility for various tasks. "],["contributing.html", "Contributing", " Contributing Thank you for reading my book about MLOps Engineering. Please note that this book is an ongoing project, much like the evolution of software. It will undergo continuous refinement and thus improved over time and more chapters will be added. I want to emphasize that all the text and code for this book are openly accessible on GitHub. If you come across any errors or find something lacking within the book’s content you are welcome to propose corrections, report issues, or make suggestions by opening an issue. Your contributions and insights are highly welcomed. "],["acknowledgements.html", "Acknowledgements", " Acknowledgements I would like to express my gratitude to the everyone contributing to this project and everyone who has provided invaluable insights and support throughout the development of this bookdown project: I am grateful to everyone who have contributed their time and expertise in reviewing drafts and providing valuable feedback. Your input has undoubtedly shaped the final outcome of this bookdown, and I am deeply appreciative of your efforts. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
