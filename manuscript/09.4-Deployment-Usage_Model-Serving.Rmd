
## Model Serving & Inferencing

The process of serving and making inferences utilizes Docker containers and runs them within Kubernetes pods.

The concept involves running a Docker container that serves the pre-trained TensorFlow model using FastAPI. This containerized model is responsible for providing predictions and responses to incoming requests. Additionally, a Streamlit app is used to interact with the served model, enabling users to make inferences by sending input data to the model and receiving the corresponding predictions.


### Model Serving

Model Serving is done via Seldon. Explanation is TBD

<!---
### Model Serving

The model serving application is built on FastAPI, which includes various endpoints catering to our use case. The primary endpoint, `predict`, allows for multiple predictions to be made, while additional maintenance endpoints such as `info` or `health` provide relevant information about the app itself.

To initiate the prediction process, the model is first retrieved from the MLflow registry. The specific model to be fetched and the location of the MLflow server are specified through environment variables. Once the model is loaded, it is used to generate predictions based on the provided input data. The API call returns the predictions in the form of a Python list.

#### Importing Dependencies {.unlisted .unnumbered}

\footnotesize
```python
# Imports necessary packages
import io
import os
from io import BytesIO

import mlflow
import mlflow.keras
import numpy as np
import pandas as pd
from fastapi import FastAPI, File, HTTPException, UploadFile
from fastapi.encoders import jsonable_encoder
from fastapi.responses import JSONResponse
from PIL import Image
from tensorflow import keras
```
\normalsize

#### Creating the FastAPI Instance {.unlisted .unnumbered}
An instance of the FastAPI application is created as well as its name and version defined.

\footnotesize
```python
# Create FastAPI instance
app = FastAPI()

model_name = "Skin Cancer Detection"
version = "v1.0.0"
```
\normalsize

#### Defining API Endpoints {.unlisted .unnumbered}

This section defines three API endpoints: `/info`, `/health`, and `/predict`. The `/info` endpoint returns information about the model, including its name and version. The `/health` endpoint returns the health status of the service. 

\footnotesize
```python
@app.get("/info")
async def model_info():
    """
    Endpoint to retrieve information about the model.

    Returns:
        - Dictionary containing the model name and version
    """
    return {"name": model_name, "version": version}

@app.get("/health")
async def service_health():
    """
    Endpoint to check the health status of the service.

    Returns:
        - Dictionary indicating the health status of the service
    """
    return {"ok"}
```
\normalsize

The `/predict` endpoint is used for making predictions and accepts an uploaded file. Within the predict endpoint, two helper functions for image preprocessing are defined. `_read_imagefile` reads the image file from the provided data and returns it as a PIL Image object. `_preprocess_image` performs normalization and reshaping on the image data to prepare it for the model.

\footnotesize
```python
@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """
    Endpoint to make predictions on skin cancer images.

    Parameters:
        - file: Uploaded image file (JPG format)

    Returns:
        - Prediction results as a JSON object
    """
    # Get environment variables
    MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI")
    MLFLOW_MODEL_NAME = os.getenv("MLFLOW_MODEL_NAME")
    MLFLOW_MODEL_VERSION = os.getenv("MLFLOW_MODEL_VERSION")

    def _read_imagefile(data) -> Image.Image:
        """
        Read image file from bytes data.

        Parameters:
            - data: Bytes data of the image file

        Returns:
            - PIL Image object
        """
        image = Image.open(BytesIO(data))
        return image

    def _preprocess_image(image) -> np.array:
        """
        Preprocess the input image for model prediction.

        Parameters:
            - image: PIL Image object

        Returns:
            - Processed numpy array image
        """
        np_image = np.array(image, dtype="uint8")
        np_image = np_image / 255.0
        np_image = np_image.reshape(1, 224, 224, 3)
        return np_image
```
\normalsize

The following code snippoet contains the actual logic for the `/predict` endpoint. It checks if the uploaded file has a `.jpg` extension. If it does, it reads the image, loads the MLflow model, preprocesses the image, performs the prediction using the model, and returns the predictions as a JSON response. If the file format is not `.jpg`, it raises a `HTTPException` with a status code of 400 and an error message indicating the invalid file format.

\footnotesize
```python
    if file.filename.endswith(".jpg"):
        print("[+] Read File")
        image = _read_imagefile(await file.read())

        print("[+] Initialize MLflow")
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

        print("[+] Load Model")
        model = mlflow.keras.load_model(f"models:/{MLFLOW_MODEL_NAME}/{MLFLOW_MODEL_VERSION}")

        print("[+] Preprocess Data")
        np_image = _preprocess_image(image)

        print("[+] Initiate Prediction")
        preds = model.predict(np_image)

        print("[+] Return Model Prediction")
        return {"prediction": preds.tolist()}
    else:
        # Raise a HTTP 400 Exception, indicating Bad Request
        raise HTTPException(status_code=400, detail="Invalid file format. Only JPG Files accepted.")
```
--->
