## Cluster Infrastructure

The cluster infrastructure consists of three modules, `vpc`, `eks`, and `rds`. The former two are responsible to create the cluster itself, whereas the `rds` module is merely an extension of the cluster and is needed to store relevant data of the tools used, such as airflow or mlflow.

### Infrastructure.VPC

The `vpc` module is based on the official vpc-aws module which is also specified as the source `"terraform-aws-modules/vpc/aws"`.

The vpc is set up in multiple availability zones to enable go reachability. TODO: true?
The vpc creates three private and three public subnets. TODO: What does this say?

Additionally to the VPC itself, the `vpc` module specifies three `aws_security_group`. TODO: where are they used?

```bash
locals {
  # private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  # public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]
  cluster_name = var.cluster_name
}

data "aws_availability_zones" "available" {}

module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "3.19.0"

  name = var.vpc_name

  cidr = "10.0.0.0/16"
  azs  = slice(data.aws_availability_zones.available.names, 0, 3)

  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.4.0/24", "10.0.5.0/24", "10.0.6.0/24"]

  enable_nat_gateway   = true
  single_nat_gateway   = true
  enable_dns_hostnames = true

  public_subnet_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
    "kubernetes.io/role/elb"                      = 1
  }

  private_subnet_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"             = 1
  }
}


resource "aws_security_group" "worker_group_mgmt_one" {
  name_prefix = "worker_group_mgmt_one"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port = 22
    to_port   = 22
    protocol  = "tcp"

    cidr_blocks = [
      "10.0.0.0/8",
    ]
  }
}

resource "aws_security_group" "worker_group_mgmt_two" {
  name_prefix = "worker_group_mgmt_two"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port = 22
    to_port   = 22
    protocol  = "tcp"

    cidr_blocks = [
      "192.168.0.0/16",
    ]
  }
}

resource "aws_security_group" "all_worker_mgmt" {
  name_prefix = "all_worker_management"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port = 22
    to_port   = 22
    protocol  = "tcp"

    cidr_blocks = [
      "10.0.0.0/8",
      "172.16.0.0/12",
      "192.168.0.0/16",
    ]
  }
}
```

### Infrastructure.EKS

The `eks` module is also based on the official `"terraform-aws-modules/eks/aws"` module like the `vpc`. It needs information about the VPC where the EKS cluster needs to be set up, like `vpc_id`, or `subnet_ids`. This means it is depended on the creation of the VPC first.

Additional `cluster_addons` are installed on the cluster such as the `aws-ebs-csi-driver` and the `TODO cluster autoscaler`. The `aws-ebs-csi-driver` driver is needed so the EKS cluster can scale volumes and attach them to running instances as needed.

Finally, the eks module deployes two `eks_managed_node_groups` which are the worker nodes of the cluster. Each worker node consists of a different size. The cluster autoscaler can allocate the needed node accordingly to a running pod.

Besides the eks-modules, and `ebs_csi_controller_role` and a corresponding `aws_iam_policy` are created. They are needed to give the `aws-ebs-csi-driver` the right permissions to do its work. The policy document lists its permissions accordingly.


```bash
locals {
  cluster_name                         = var.cluster_name
  cluster_namespace                    = "kube-system"
  ebs_csi_service_account_name         = "ebs-csi-controller-sa"
  ebs_csi_service_account_role_name    = "${var.cluster_name}-ebs-csi-controller"
  autoscaler_service_account_name      = "autoscaler-controller-sa"
  autoscaler_service_account_role_name = "${var.cluster_name}-autoscaler-controller"
}

data "aws_caller_identity" "current" {}

# TODO: node group in each availability zone
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "19.5.1"

  cluster_name    = local.cluster_name
  cluster_version = "1.24"

  vpc_id                         = var.vpc_id
  subnet_ids                     = var.private_subnets
  cluster_endpoint_public_access = true

  cluster_addons = {
    aws-ebs-csi-driver = {
      service_account_role_arn = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.ebs_csi_service_account_role_name}"
    }
  }

  eks_managed_node_group_defaults = {
    ami_type = "AL2_x86_64"

  }

  eks_managed_node_groups = {
    one = {
      name = "node-group-small"

      instance_types = ["t3.small"]

      min_size     = 1
      max_size     = 3
      desired_size = 2
    }

    two = {
      name = "node-group-medium"

      instance_types = ["t3.medium"]

      min_size     = 1
      max_size     = 2
      desired_size = 1
    }
  }
}


# EBS CSI controller
#
# https://stackoverflow.com/questions/74648632/how-do-i-use-the-aws-ebs-csi-driver-addon-when-using-the-aws-eks-terraform-modul
module "ebs_csi_controller_role" {
  source                        = "terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc"
  version                       = "5.11.1"
  create_role                   = true
  role_name                     = local.ebs_csi_service_account_role_name
  provider_url                  = replace(module.eks.cluster_oidc_issuer_url, "https://", "")
  role_policy_arns              = [aws_iam_policy.ebs_csi_controller_sa.arn]
  oidc_fully_qualified_subjects = ["system:serviceaccount:${local.cluster_namespace}:${local.ebs_csi_service_account_name}"]
}

resource "aws_iam_policy" "ebs_csi_controller_sa" {
  name        = local.ebs_csi_service_account_name
  description = "EKS ebs-csi-controller policy for cluster ${var.cluster_name}"

  policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Action" : [
          "ec2:AttachVolume",
          "ec2:CreateSnapshot",
          "ec2:CreateTags",
          "ec2:CreateVolume",
          "ec2:DeleteSnapshot",
          "ec2:DeleteTags",
          "ec2:DeleteVolume",
          "ec2:DescribeInstances",
          "ec2:DescribeSnapshots",
          "ec2:DescribeTags",
          "ec2:DescribeVolumes",
          "ec2:DetachVolume",
        ],
        "Effect" : "Allow",
        "Resource" : "*"
      }
  ] })
}

```

### Infrastructure.RDS

The `rds` module is not necessarily needed to run a kubernetes cluster properly. It is merely an extension of the cluster and is needed to store relevant data of the tools used, such as airflow or mlflow. The module is thus called directly from the own airflow and mlflow modules.

The RDS itself is created by an `aws_db_instance` resource. It is needs to resources `aws_db_subnet_group` and `aws_security_group`, which themselves depend on a VPC to be created beforehand.
All other values for the creation of an RDS are passed through using Terraform variables when the `rds`-module is called. This includes for example the `db_name`, `rds_password`, `rds_engine`, and the `allocated_storage` of the database.

```bash
locals {
  rds_name           = var.rds_name
  rds_engine         = var.rds_engine
  rds_engine_version = var.rds_engine_version
  rds_port           = var.rds_port
}

resource "aws_db_subnet_group" "default" {
  name       = "vpc-subnet-group-${local.rds_name}"
  subnet_ids = var.private_subnets
}

resource "aws_db_instance" "rds_instance" {
  allocated_storage      = var.max_allocated_storage
  storage_type           = var.storage_type
  engine                 = local.rds_engine
  engine_version         = local.rds_engine_version
  instance_class         = var.rds_instance_class
  db_name                = "${local.rds_name}_db"
  username               = "${local.rds_name}_admin" # push to main
  password               = var.rds_password
  identifier             = "${local.rds_name}-${local.rds_engine}"
  port                   = local.rds_port
  vpc_security_group_ids = [aws_security_group.rds_sg.id]
  db_subnet_group_name   = aws_db_subnet_group.default.name
  skip_final_snapshot    = true
}

resource "aws_security_group" "rds_sg" {
  name   = "${local.rds_name}-${local.rds_engine}-sg"
  vpc_id = var.vpc_id

  ingress {
    description = "Enable postgres access"
    from_port   = local.rds_port
    to_port     = local.rds_port
    protocol    = "tcp"
    cidr_blocks = var.private_subnets_cidr_blocks
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

```
