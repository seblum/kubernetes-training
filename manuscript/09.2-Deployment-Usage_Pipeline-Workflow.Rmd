
## Training & Deployment Pipeline Workflow

The code and machine learning pipeline have been modularized into distinct steps, including preprocessing, model training, model comparison, and model serving. Airflow serves as the model workflow tool, generating DAGs for managing the pipeline. MLflow is integrated to facilitate model tracking, registry, and serving functionalities. To ensure portability and scalability, the codebase has been containerized using Docker, allowing it to be executed in Docker and/or Kubernetes environments.

The `src` code is installed as a Python package within the Docker container, enabling easy invocation within the Airflow DAG. However, it is important to note that although Model Serving is triggered within the Airflow pipeline, it consists of a separate Python code and is not integrated into the `src` package. Likewise, model inferencing has its own distinct description and functionality. The code bases for both model serving and model inferencing can be found in the app/ directory, alongside their respective Dockerfiles. A detailed explanation of how these components function will be provided in the following section.

### Airflow Workflow

The specification of the Airflow DAG, which includes the DAG structure, tasks, and their dependencies, can be found in the `airflow_DAG.py` file. The DAG is built using the TaskFlow API.

An ML pipeline of this use case consists of three main steps: preprocessing, training, and serving. The preprocessing step involves data processing and storing it in the S3 storage. The training step and code are designed to accommodate different TensorFlow models, allowing for parallel training on multiple models, thereby reducing the time to deployment. Since there are multiple models, it is essential to serve only the model with the best metrics based on the current data. Hence, an intermediate step is incorporated to compare the metrics of all the models and select the best one for serving.

To execute the pipeline steps, the Airflow Docker Operator is employed, which ensures that each step runs in a separate and isolated environment using Docker or Kubernetes jobs. Dockerizing the code is a prerequisite for this process. The Airflow task then invokes the relevant methods of the Python code and executes them accordingly.

Once the model is in the serving phase, a Streamlit app is deployed for applying inference on new data.

![](images/09-Deployment-Usage/use-case-pipeline-graph.png){ width=100% }

The code below defines the `ml_pipeline_dag` function as an Airflow DAG using the `dag` decorator. Each step of the pipeline, including data preprocessing, model training, model comparison, and serving the best model, is represented as a separate task with the `@task` decorator. Dependencies between these tasks are established by passing the output of one task as an argument to the next task. The `ml_pipeline` object serves as a representation of the entire DAG.

#### Importing Dependencies {.unlisted .unnumbered}
At first the necessary dependencies for the code are imported, including libraries for MLflow, Airflow, and other utilities.

\footnotesize
```python 

```
\normalsize

#### Setting MLflow Tracking URI and Experiment {.unlisted .unnumbered}
Secondly, the necessary variables and constants for the whole Airflow DAG are defined and set. Either hard coded as string, or read from environment variables. Also, the MLflow tracking URI is set and a MLflow experiment retrieved, or created if none exists already.

\footnotesize
```python 

```
\normalsize

#### Setting Default Arguments and Environment Data {.unlisted .unnumbered}

\footnotesize
```python 

```
\normalsize

#### Defining the Airflow DAG {.unlisted .unnumbered}
After all parameters have been set, the actual Airflow DAG for the CNN skin cancer workflow is defined. As each single task of the ML pipeline of the Airflow DAG is executed as a container run, the container image that is pulled from DockerHub needs to be specified.

\footnotesize
```python 

```
\normalsize

#### Defining Preprocessing Task {.unlisted .unnumbered}
As a first step, the preprocessing task is defined, which performs data preprocessing.

\footnotesize
```python 

```
\normalsize

#### Defining Model Training Task {.unlisted .unnumbered}
Similarly, the model training task is defined, which trains a machine learning model.

\footnotesize
```python 

```
\normalsize

#### Defining Model Comparison Task {.unlisted .unnumbered}
The following code snippet defines the model comparison task, which compares trained models.

\footnotesize
```python

```
\normalsize

#### Defining Pipeline {.unlisted .unnumbered}
After the tasks have been specified, they are connected together to define the workflow pipeline, specifying inputs and outputs.

\footnotesize
```python 

```
\normalsize

Finally, the Airflow DAG function is called in a last step.

\footnotesize
```python
# Call the airflow DAG

```
\normalsize

### MLflow

Mlflow is leveraged in the preprocessing and model training stages to store crucial data parameters, model training parameters, and metrics, while also enabling the saving of trained models in the model registry. In the `airflow_DAG.py` file, Mlflow is invoked to create an experiment, and the experiment ID is passed to each pipeline step to store parameters in separate runs. This ensures a clear distinction between the execution of different models.

The `train_model` pipeline steps serve as a container for the model training procedure. Within the container, the model is trained using specific code. All the relevant information about the model and the model itself are logged using mlflow as well. This workflow ensures the comprehensive tracking of model parameters and metrics, and the saved model can be accessed and compared during the subsequent model comparison step. In fact, during this stage, the best model is transferred to another model stage within the model registry.
