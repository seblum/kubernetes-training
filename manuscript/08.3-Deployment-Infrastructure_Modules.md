## Modules

Within the setup, there are multiple custom modules, namely airflow, mlflow, jupyterhub, and monitoring. Each module is responsible for deploying a specific workflow tool.

These module names also align with their corresponding namespaces within the cluster.

### Airflow

The main tool used for workflow management is Airflow.

To set up the infrastructure, several AWS resources are required. The rds-airflow module in the infrastructure directory is responsible for creating an RDS database that will be attached to the Airflow server. Additionally, S3 buckets are needed for logging and data storage, and IAM entries are created to grant access to the storage and database services.

Deployment of Airflow itself is done using the helm_release resource, which allows deploying a Helm chart onto the cluster within a Terraform setup. The image used for running Airflow is a customized version that includes the necessary dependencies.

Git synchronization is enabled for the Airflow DAG through the airflow_dag Git repository.

Two Kubernetes secrets are utilized:

airflow_db_credentials: Contains the credentials for accessing the Airflow database.
airflow_https_git_secret: Stores the secret for accessing the HTTPS Git repository.
To establish a connection and enable IRSA (IAM Roles for Service Accounts) access to the cluster, a role is created using Terraform.

Logging to a remote location on S3 storage is enabled through the Helm chart configuration.

A custom setup includes an S3 bucket specifically for storing data. The access credentials for this bucket are stored in a Kubernetes secret, and a variable with the secret name is provided to users. Additionally, variables with the internal MLflow URL are provided to allow easy access during the pipeline.

While user management is possible, it is currently not supported in a sophisticated manner within this tutorial.

### Mlflow

To enable model tracking, MLflow is deployed with specific requirements. These include a data store on AWS S3, a metadata store using PostgreSQL (RDS), and the MLflow server. The first two components are created using Terraform resources.

However, MLflow does not have native support for Kubernetes and does not offer an official Helm chart. Despite being an excellent tool, we need to set up a basic custom Helm chart to deploy the MLflow server in this case. We also use a custom container image for running MLflow. This process involves creating YAML configurations for deployment, service, and configmap, which will be executed on our Kubernetes cluster.

It's important to note that the deployment has an open endpoint, which means it lacks sufficient security measures.

### Jupyterhub

JupyterHub is utilized in the setup to provide an IDE (Integrated Development Environment). However, it is highly customized and does not use Jupyter as the environment. Instead, it utilizes code-server with a VS Code IDE running on top of it. The customization is implemented within the Jupyter Helm chart. Additionally, JupyterHub establishes a connection to the mlops-airflow-dag Git repository and synchronizes the code directly upon startup.

The module also encompasses the setup of Elastic Block Store (EBS) for data storage.

While user management is feasible, it is currently not supported in a sophisticated manner within this tutorial.

### Monitoring

Monitoring using Prometheus and Grafana is included in the setup. Although it is not directly part of the ML pipeline, it serves as a good example of how to monitor the cluster effectively. A basic monitoring setup is achieved using Prometheus and Grafana.

Prometheus is employed for [specific purpose], while Grafana is used for [specific purpose]. Both Prometheus and Grafana are deployed using a Helm chart. The connection between Grafana and Prometheus is established within the Grafana Helm chart.

#### Prometheus
#### Grafana
