

## Codebase

The code and machine learning pipeline have been modularized into distinct steps, including preprocessing, model training, model comparison, and model serving. Airflow serves as the model workflow tool, generating DAGs for managing the pipeline. MLflow is integrated to facilitate model tracking, registry, and serving functionalities. To ensure portability and scalability, the codebase has been containerized using Docker, allowing it to be executed in Docker and/or Kubernetes environments.

The `src` code is installed as a Python package within the Docker container, enabling easy invocation within the Airflow DAG. However, it is important to note that although Model Serving is triggered within the Airflow pipeline, it consists of a separate Python code and is not integrated into the `src` package. Likewise, model inferencing has its own distinct description and functionality. The code bases for both model serving and model inferencing can be found in the app/ directory, alongside their respective Dockerfiles. A detailed explanation of how these components function will be provided in the following section.

### Airflow pipeline

The specification of the Airflow DAG, which includes the DAG structure, tasks, and their dependencies, can be found in the `airflow_DAG.py` file. The DAG is built using the TaskFlow API.

An ML pipeline of this use case consists of three main steps: preprocessing, training, and serving. The preprocessing step involves data processing and storing it in the S3 storage. The training step and code are designed to accommodate different TensorFlow models, allowing for parallel training on multiple models, thereby reducing the time to deployment. Since there are multiple models, it is essential to serve only the model with the best metrics based on the current data. Hence, an intermediate step is incorporated to compare the metrics of all the models and select the best one for serving.

To execute the pipeline steps, the Airflow Docker Operator is employed, which ensures that each step runs in a separate and isolated environment using Docker or Kubernetes jobs. Dockerizing the code is a prerequisite for this process. The Airflow task then invokes the relevant methods of the Python code and executes them accordingly.

Once the model is in the serving phase, a Streamlit app is deployed for applying inference on new data.

![DAG pipeline](images/09-Deployment-Usage/DAG-pipeline.png) 

The code below defines the `ml_pipeline_dag` function as an Airflow DAG using the `dag` decorator. Each step of the pipeline, including data preprocessing, model training, model comparison, and serving the best model, is represented as a separate task with the `@task` decorator. Dependencies between these tasks are established by passing the output of one task as an argument to the next task. The `ml_pipeline` object serves as a representation of the entire DAG.

```python

```

### MLflow

Mlflow is leveraged in the preprocessing and model training stages to store crucial data parameters, model training parameters, and metrics, while also enabling the saving of trained models in the model registry. In the `airflow_DAG.py` file, Mlflow is invoked to create an experiment, and the experiment ID is passed to each pipeline step to store parameters in separate runs. This ensures a clear distinction between the execution of different models.

The `train_model` pipeline steps serve as a container for the model training procedure. Within the container, the model is trained using specific code. All the relevant information about the model and the model itself are logged using mlflow as well. This workflow ensures the comprehensive tracking of model parameters and metrics, and the saved model can be accessed and compared during the subsequent model comparison step. In fact, during this stage, the best model is transferred to another model stage within the model registry.

```python

```