

## Codebase

The code and machine learning pipeline have been modularized into distinct steps, including preprocessing, model training, model comparison, and model serving. Airflow serves as the model workflow tool, generating DAGs for managing the pipeline. MLflow is integrated to facilitate model tracking, registry, and serving functionalities. To ensure portability and scalability, the codebase has been containerized using Docker, allowing it to be executed in Docker and/or Kubernetes environments.

The `src` code is installed as a Python package within the Docker container, enabling easy invocation within the Airflow DAG. However, it is important to note that although Model Serving is triggered within the Airflow pipeline, it consists of a separate Python code and is not integrated into the `src` package. Likewise, model inferencing has its own distinct description and functionality. The code bases for both model serving and model inferencing can be found in the app/ directory, alongside their respective Dockerfiles. A detailed explanation of how these components function will be provided in the following section.

### Airflow pipeline

The specification of the Airflow DAG, which includes the DAG structure, tasks, and their dependencies, can be found in the `airflow_DAG.py` file. The DAG is built using the TaskFlow API.

An ML pipeline of this use case consists of three main steps: preprocessing, training, and serving. The preprocessing step involves data processing and storing it in the S3 storage. The training step and code are designed to accommodate different TensorFlow models, allowing for parallel training on multiple models, thereby reducing the time to deployment. Since there are multiple models, it is essential to serve only the model with the best metrics based on the current data. Hence, an intermediate step is incorporated to compare the metrics of all the models and select the best one for serving.

To execute the pipeline steps, the Airflow Docker Operator is employed, which ensures that each step runs in a separate and isolated environment using Docker or Kubernetes jobs. Dockerizing the code is a prerequisite for this process. The Airflow task then invokes the relevant methods of the Python code and executes them accordingly.

Once the model is in the serving phase, a Streamlit app is deployed for applying inference on new data.

![DAG pipeline](images/09-Deployment-Usage/DAG-pipeline.png) 

The code below defines the `ml_pipeline_dag` function as an Airflow DAG using the `dag` decorator. Each step of the pipeline, including data preprocessing, model training, model comparison, and serving the best model, is represented as a separate task with the `@task` decorator. Dependencies between these tasks are established by passing the output of one task as an argument to the next task. The `ml_pipeline` object serves as a representation of the entire DAG.

```python
import os
import mlflow
import pendulum
from airflow.decorators import dag, task
from enum import Enum


# SET MLFLOW

MLFLOW_TRACKING_URI_local = "http://127.0.0.1:5008/"
MLFLOW_TRACKING_URI = "http://host.docker.internal:5008"
EXPERIMENT_NAME = "cnn_skin_cancer"
AWS_BUCKET = os.getenv("AWS_BUCKET")
AWS_REGION = os.getenv("AWS_REGION")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_ROLE_NAME = os.getenv("AWS_ROLE_NAME")

mlflow.set_tracking_uri(MLFLOW_TRACKING_URI_local)

try:
    # Creating an experiment
    mlflow_experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)
except:
    pass
# Setting the environment with the created experiment
mlflow_experiment_id = mlflow.set_experiment(EXPERIMENT_NAME).experiment_id


class Model_Class(Enum):
    """This enum includes different models."""

    Basic = "Basic"
    CrossVal = "CrossVal"
    ResNet50 = "ResNet50"


# SET AIRFLOW

# Set various params and args
dag_default_args = {
    "owner": "seblum",
    "depends_on_past": False,
    "start_date": pendulum.datetime(2021, 1, 1, tz="UTC"),
    # "provide_context": True,
    "tags": ["Keras CNN to classify skin cancer"],
}

## PREPROCESSING

kwargs_data_preprocessing = {
    "MLFLOW_TRACKING_URI": MLFLOW_TRACKING_URI,
    "MLFLOW_EXPERIMENT_ID": mlflow_experiment_id,
    "AWS_ACCESS_KEY_ID": AWS_ACCESS_KEY_ID,
    "AWS_SECRET_ACCESS_KEY": AWS_SECRET_ACCESS_KEY,
    "AWS_BUCKET": AWS_BUCKET,
    "AWS_REGION": AWS_REGION,
    "AWS_ROLE_NAME" : AWS_ROLE_NAME
}

model_params = {
    "num_classes": 2,
    "input_shape": (224, 224, 3),
    "activation": "relu",
    "kernel_initializer_glob": "glorot_uniform",
    "kernel_initializer_norm": "normal",
    "optimizer": "adam",
    "loss": "binary_crossentropy",
    "metrics": ["accuracy"],
    "validation_split": 0.2,
    "epochs": 2,
    "batch_size": 64,
    "learning_rate": 1e-5,
    "pooling": "avg",  # needed for resnet50
    "verbose": 2,
}

@dag(
    "cnn_skin_cancer_workflow",
    default_args=dag_default_args,
    schedule_interval=None,
    max_active_runs=1,
)
def cnn_skin_cancer_workflow():
    @task.docker(
        image="seblum/cnn-model:python-base",
        multiple_outputs=True,
        environment=kwargs_data_preprocessing,
        working_dir="/app",
        force_pull=True,
        # tmp_dir="/app"
        # TODO: set all params
    )
    def preprocessing_op(mlflow_experiment_id, AWS_BUCKET):
        # TODO: remove Bucket

        from src.preprocessing import data_preprocessing

        (
            X_train_data_path,
            y_train_data_path,
            X_test_data_path,
            y_test_data_path,
        ) = data_preprocessing(mlflow_experiment_id=mlflow_experiment_id, aws_bucket=AWS_BUCKET)

        # Create dictionary with S3 paths to return
        return_dict = {
            "X_train_data_path": X_train_data_path,
            "y_train_data_path": y_train_data_path,
            "X_test_data_path": X_test_data_path,
            "y_test_data_path": y_test_data_path,
        }
        return return_dict

    @task.docker(
        image="seblum/cnn-model:python-base",
        multiple_outputs=True,
        environment=kwargs_data_preprocessing,
        working_dir="/app",
        force_pull=True,
    )
    def model_training_op(mlflow_experiment_id, model_class, model_params, AWS_BUCKET, input):
        import os
        import sys

        sys.path.append(os.path.abspath("/app/"))

        from train import train_model

        run_id, model_name, model_version, model_stage = train_model(
            mlflow_experiment_id=mlflow_experiment_id,
            model_class=model_class,
            model_params=model_params,
            aws_bucket=AWS_BUCKET,
            import_dict=input,
        )

        # Create dictionary with S3 paths to return
        return_dict = {
            "run_id": run_id,
            "model_name": model_name,
            "model_version": model_version,
            "model_stage": model_stage,
        }
        return return_dict

    @task.docker(
        image="seblum/cnn-model:python-base",
        multiple_outputs=True,
        environment=kwargs_data_preprocessing,
        force_pull=True,
    )
    def compare_models_op(train_data_basic, train_data_resnet50):
        import os
        import sys

        sys.path.append(os.path.abspath("/app/"))

        compare_dict = {
            train_data_basic["model_name"]: train_data_basic["run_id"],
            train_data_resnet50["model_name"]: train_data_resnet50["run_id"],
        }

        print(compare_dict)
        from compare_models import compare_models

        serving_model_name, serving_model_uri, serving_model_version = compare_models(input_dict=compare_dict)
        return_dict = {
            "serving_model_name": serving_model_name,
            "serving_model_uri": serving_model_uri,
            "serving_model_version": serving_model_version,
        }
        return return_dict

    @task.docker(
        image="seblum/model-serving:fastapi-serve",
        multiple_outputs=True,
        environment=kwargs_data_preprocessing,
        force_pull=True,
    )
    def serve_fastapi_app_op(compare_models_dict):

        import os
        
        os.environ["MLFLOW_MODEL_NAME"] = compare_models_dict["serving_model_name"]
        os.environ["MLFLOW_MODEL_VERSION"] = compare_models_dict["serving_model_version"]
        
        return_dict = {
            "FASTAPI_SERVING_IP": "http://host.docker.internal:5008",
            "FASTAPI_SERVING_PORT": 80
        }
        return return_dict


    @task.docker(
        image="seblum/model-serving:streamlit-inference",
        multiple_outputs=True,
        environment=kwargs_data_preprocessing,
    )
    def serve_streamlit_app_op(serve_fastapi_app_dict):

        import os
        os.environ["FASTAPI_SERVING_IP"] = serve_fastapi_app_dict["FASTAPI_SERVING_IP"]
        os.environ["FASTAPI_SERVING_PORT"] = serve_fastapi_app_dict["FASTAPI_SERVING_PORT"]

        print("Service running")

    # CREATE PIPELINE

    data = preprocessing_op(
        mlflow_experiment_id=mlflow_experiment_id,
        AWS_BUCKET=AWS_BUCKET,
    )
    train_data_basic = model_training_op(
        mlflow_experiment_id=mlflow_experiment_id,
        model_class=Model_Class.Basic.name,
        model_params=model_params,
        AWS_BUCKET=AWS_BUCKET,
        input=data,
    )
    train_data_resnet50 = model_training_op(
        mlflow_experiment_id=mlflow_experiment_id,
        model_class=Model_Class.ResNet50.name,
        model_params=model_params,
        AWS_BUCKET=AWS_BUCKET,
        input=data,
    )
    compare_models_dict = compare_models_op(train_data_basic, train_data_resnet50)
    serve_fastapi_app_dict = serve_fastapi_app_op(compare_models_dict)
    serve_streamlit_app_op(serve_fastapi_app_dict)

cnn_skin_cancer_workflow()

```

### MLflow

Mlflow is leveraged in the preprocessing and model training stages to store crucial data parameters, model training parameters, and metrics, while also enabling the saving of trained models in the model registry. In the `airflow_DAG.py` file, Mlflow is invoked to create an experiment, and the experiment ID is passed to each pipeline step to store parameters in separate runs. This ensures a clear distinction between the execution of different models.

The `train_model` pipeline steps serve as a container for the model training procedure. Within the container, the model is trained using specific code. All the relevant information about the model and the model itself are logged using mlflow as well. This workflow ensures the comprehensive tracking of model parameters and metrics, and the saved model can be accessed and compared during the subsequent model comparison step. In fact, during this stage, the best model is transferred to another model stage within the model registry.
