### Elastic Kubernetes Service

The provided Terraform code sets up an AWS EKS (Elastic Kubernetes Service) cluster with specific configurations and multiple node groups. The `"eks"` module is used to create the EKS cluster, specifying its name and version. The cluster has public and private access endpoints enabled, and a managed AWS authentication configuration.  The creation of the `vpc` module is a prerequisite for the `"eks"` module, as the latter requires information like the `vpc_id`, or `subnet_ids` for a successful creation.

The EKS cluster itself is composed of three managed node groups: `"group_t3_small"`, `"group_t3_medium"`, and `"group_t3_large"`. Each node group uses a different instance type (`t3.small`, `t3.medium`, and `t3.large`) and has specific scaling policies. All three node groups have auto-scaling enabled. The node group `"group_t3_medium"` has set the minimum and desired sizes of nodes to `4`, which ensures a base amount of nodes and thus resources to manage further deployments. The `"group_t3_large"` is tainted with a `NoSchedule`. This node group can be used for more resource intensive tasks by specifiyng a pod's toleration.

The `eks` module also deploys several Kubernetes add-ons, including `coredns`, `kube-proxy`, `aws-ebs-csi-driver`, and `vpc-cni`. The vpc-cni add-on is configured with specific environment settings, enabling prefix delegation for IP addresses. 

- `CoreDNS` provides DNS-based service discovery, allowing pods and services to communicate with each other using domain names, and thus enabling seamless communication within the cluster without the need for explicit IP addresses. 
- `kube-proxy`: is responsible for network proxying on Kubernetes nodes which ensures that network traffic is properly routed to the appropriate pods, services, and endpoints. It allows for an seamless communication between different parts of the cluster. 
- `aws-ebs-csi-driver`(Container Storage Interface) is an add-on that enables Kubernetes pods to use Amazon Elastic Block Store (EBS) volumes for persistent storage, allowing data to be retained across pod restarts and ensuring data durability for stateful applications. The EBS configuration and deployment are describen in the following subsection [Elastic Block Store](#elastic-block-store), but the respective `service_account_role_arn` is linked to the EKS cluster on creation.
- `vpc-cni` (Container Network Interface) is essential for AWS EKS clusters, as it enables networking for pods using AWS VPC (Virtual Private Cloud) networking. It ensures that each pod gets an IP address from the VPC subnet and can communicate securely with other AWS resources within the VPC.

\footnotesize
```javascript
locals {
  cluster_name                         = var.cluster_name
  cluster_namespace                    = "kube-system"
  ebs_csi_service_account_name         = "ebs-csi-controller-sa"
  ebs_csi_service_account_role_name    = "${var.cluster_name}-ebs-csi-controller"
  autoscaler_service_account_name      = "autoscaler-controller-sa"
  autoscaler_service_account_role_name = "${var.cluster_name}-autoscaler-controller"

  nodegroup_t3_small_label    = "t3_small"
  nodegroup_t3_medium_label   = "t3_medium"
  nodegroup_t3_large_label = "t3_large"
  eks_asg_tag_list_nodegroup_t3_small_label = {
    "k8s.io/cluster-autoscaler/enabled" : true
    "k8s.io/cluster-autoscaler/${local.cluster_name}" : "owned"
    "k8s.io/cluster-autoscaler/node-template/label/role" : local.nodegroup_t3_small_label
  }

  eks_asg_tag_list_nodegroup_t3_medium_label = {
    "k8s.io/cluster-autoscaler/enabled" : true
    "k8s.io/cluster-autoscaler/${local.cluster_name}" : "owned"
    "k8s.io/cluster-autoscaler/node-template/label/role" : local.nodegroup_t3_medium_label
  }

  eks_asg_tag_list_nodegroup_t3_large_label = {
    "k8s.io/cluster-autoscaler/enabled" : true
    "k8s.io/cluster-autoscaler/${local.cluster_name}" : "owned"
    "k8s.io/cluster-autoscaler/node-template/label/role" : local.nodegroup_t3_large_label
    "k8s.io/cluster-autoscaler/node-template/taint/dedicated" : "${local.nodegroup_t3_large_label}:NoSchedule"
  }

  tags = {
    Owner = "terraform"
  }
}

data "aws_caller_identity" "current" {}

#
# EKS
#
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "19.5.1"

  cluster_name              = local.cluster_name
  cluster_version           = var.eks_cluster_version
  cluster_enabled_log_types = ["api", "controllerManager", "scheduler"]

  vpc_id     = var.vpc_id
  subnet_ids = var.private_subnets

  cluster_endpoint_private_access = true
  cluster_endpoint_public_access  = true
  manage_aws_auth_configmap       = true

  # aws_auth_users            = local.cluster_users # add users in later step

  cluster_addons = {
    coredns = {
      most_recent = true
    },
    kube-proxy = {
      most_recent = true
    },
    aws-ebs-csi-driver = {
      service_account_role_arn = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.ebs_csi_service_account_role_name}"
    },
    vpc-cni = {
      most_recent              = true
      before_compute           = true
      service_account_role_arn = module.vpc_cni_irsa.iam_role_arn
      configuration_values = jsonencode({
        env = {
          # Reference docs https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html
          ENABLE_PREFIX_DELEGATION = "true"
          WARM_PREFIX_TARGET       = "1"
        }
      })
    }

  }

  eks_managed_node_group_defaults = {
    ami_type                   = "AL2_x86_64"
    disk_size                  = 10
    iam_role_attach_cni_policy = true
    enable_monitoring          = true
  }

  eks_managed_node_groups = {
    group_t3_small = {
      name = "ng0_t3_small"

      instance_types = ["t3.small"]

      min_size      = 0
      max_size      = 6
      desired_size  = 0
      capacity_type = "ON_DEMAND"
      labels = {
        role = local.nodegroup_t3_small_label
      }
      tags = {
        "k8s.io/cluster-autoscaler/enabled"                  = "true"
        "k8s.io/cluster-autoscaler/${local.cluster_name}"    = "owned"
        "k8s.io/cluster-autoscaler/node-template/label/role" = "${local.nodegroup_t3_small_label}"
      }
    }
    group_t3_medium = {
      name = "ng1_t3_medium"

      instance_types = ["t3.medium"]

      min_size      = 4
      max_size      = 6
      desired_size  = 4
      capacity_type = "ON_DEMAND"
      labels = {
        role = local.nodegroup_t3_medium_label
      }
      tags = {
        "k8s.io/cluster-autoscaler/enabled"                       = "true"
        "k8s.io/cluster-autoscaler/${local.cluster_name}"         = "owned"
        "k8s.io/cluster-autoscaler/node-template/label/role"      = "${local.nodegroup_t3_medium_label}"
      }
    }
    group_t3_large = {
      name = "ng2_t3_large"

      instance_types = ["t3.large"]

      min_size      = 0
      max_size      = 3
      desired_size  = 0
      capacity_type = "ON_DEMAND"
      labels = {
        role = local.nodegroup_t3_large_label
      }
      taints = [
        {
          key    = "dedicated"
          value  = local.nodegroup_t3_large_label
          effect = "NO_SCHEDULE"
        }
      ]
      tags = {
        "k8s.io/cluster-autoscaler/enabled"                       = "true"
        "k8s.io/cluster-autoscaler/${local.cluster_name}"         = "owned"
        "k8s.io/cluster-autoscaler/node-template/label/role"      = "${local.nodegroup_t3_large_label}"
        "k8s.io/cluster-autoscaler/node-template/taint/dedicated" = "${local.nodegroup_t3_large_label}:NoSchedule"
      }
    }
  }
  tags = local.tags
}

#  Role for Service Account
module "vpc_cni_irsa" {
  source  = "terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks"
  version = "~> 5.0"

  role_name_prefix      = "VPC-CNI-IRSA"
  attach_vpc_cni_policy = true
  vpc_cni_enable_ipv4   = true

  oidc_providers = {
    main = {
      provider_arn               = module.eks.oidc_provider_arn
      namespace_service_accounts = ["kube-system:aws-node"]
    }
  }
}
```
\normalsize

#### Elastic Block Store

The EBS CSI controller (Elastic Block Store Container Storage Interface) is set up by defining an IAM (Identity and Access Management) role using the `"ebs_csi_controller_role"` module. The role allows the EBS CSI controller to assume a specific IAM role with OIDC (OpenID Connect) authentication, granting it the necessary permissions for EBS-related actions in the AWS environment by an IAM policy. The IAM policy associated with the role is created likewise and permits various EC2 actions, such as attaching and detaching volumes, creating and deleting snapshots, and describing instances and volumes. 

The code also configures the default Kubernetes StorageClass named `"gp2"` and annotates it as not the default storage class for the cluster, managing how storage volumes are provisioned and utilized in the cluster. Ensuring that the `"gp2"` StorageClass does not become the default storage class is needed as we additionally create an EFS Storage (Elastic File System), which is described in the [next subsection](#elastic-file-system).

\footnotesize
```javascript
#
# EBS CSI controller
#
module "ebs_csi_controller_role" {
  source                        = "terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc"
  version                       = "5.11.1"
  create_role                   = true
  role_name                     = local.ebs_csi_service_account_role_name
  provider_url                  = replace(module.eks.cluster_oidc_issuer_url, "https://", "")
  role_policy_arns              = [aws_iam_policy.ebs_csi_controller_sa.arn]
  oidc_fully_qualified_subjects = ["system:serviceaccount:${local.cluster_namespace}:${local.ebs_csi_service_account_name}"]
}

resource "aws_iam_policy" "ebs_csi_controller_sa" {
  name        = local.ebs_csi_service_account_name
  description = "EKS ebs-csi-controller policy for cluster ${var.cluster_name}"

  policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Action" : [
          "ec2:AttachVolume",
          "ec2:CreateSnapshot",
          "ec2:CreateTags",
          "ec2:CreateVolume",
          "ec2:DeleteSnapshot",
          "ec2:DeleteTags",
          "ec2:DeleteVolume",
          "ec2:DescribeInstances",
          "ec2:DescribeSnapshots",
          "ec2:DescribeTags",
          "ec2:DescribeVolumes",
          "ec2:DetachVolume",
        ],
        "Effect" : "Allow",
        "Resource" : "*"
      }
  ] })
}

resource "kubernetes_annotations" "ebs-no-default-storageclass" {
  api_version = "storage.k8s.io/v1"
  kind        = "StorageClass"
  force       = "true"

  metadata {
    name = "gp2"
  }
  annotations = {
    "storageclass.kubernetes.io/is-default-class" = "false"
  }
}
```
\normalsize

#### Elastic File System

The EFS CSI (Elastic File System Container Storage Interface) driver permits EKS pods to use EFS as a persistent volume for data storage, enabling pods to use EFS as a scalable and shared storage solution.. The driver itself is deployed using a Helm chart through the `"helm_release"` resource. Of course we also need to create an IAM role for the EFS CSI driver, which is done using the `"attach_efs_csi_role"` module, which allows the driver to assume a role with OIDC authentication, and grants the necessary permissions for working with EFS, similar to the EBS setup.

For security, the code creates an AWS security group named `"allow_nfs"` that allows inbound NFS traffic on port 2049 from the private subnets of the VPC. This allows the EFS mount targets to communicate with the EFS file system securely. The EFS file system and access points are created manually for each private subnet mapping the `"aws_efs_mount_target"` to the `"aws_efs_file_system"` resource.

Finally, the code defines a Kubernetes StorageClass named `"efs"` using the `"kubernetes_storage_class_v1"` resource. The StorageClass specifies the EFS CSI driver as the storage provisioner and the EFS file system created earlier as the backing storage. Additionally, the `"efs"` StorageClass is marked as the default storage class for the cluster using an annotation. This allows dynamic provisioning of EFS-backed persistent volumes for Kubernetes pods on default, simplifying the process of handling storage in the EKS cluster. This is done for example for the Airflow deployment in a later step.

\footnotesize
```javascript
#
# EFS
#
resource "helm_release" "aws_efs_csi_driver" {
  chart      = "aws-efs-csi-driver"
  name       = "aws-efs-csi-driver"
  namespace  = "kube-system"
  repository = "https://kubernetes-sigs.github.io/aws-efs-csi-driver/"
  set {
    name  = "controller.serviceAccount.create"
    value = true
  }
  set {
    name  = "controller.serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"
    value = module.attach_efs_csi_role.iam_role_arn
  }
  set {
    name  = "controller.serviceAccount.name"
    value = "efs-csi-controller-sa"
  }
}

module "attach_efs_csi_role" {
  source = "terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks"
  role_name             = "efs-csi"
  attach_efs_csi_policy = true
  oidc_providers = {
    ex = {
      provider_arn               = module.eks.oidc_provider_arn
      namespace_service_accounts = ["kube-system:efs-csi-controller-sa"]
    }
  }
}

resource "aws_security_group" "allow_nfs" {
  name        = "allow nfs for efs"
  description = "Allow NFS inbound traffic"
  vpc_id      = var.vpc_id

  ingress {
    description = "NFS from VPC"
    from_port   = 2049
    to_port     = 2049
    protocol    = "tcp"
    cidr_blocks = var.private_subnets_cidr_blocks
  }
  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }
}

resource "aws_efs_file_system" "stw_node_efs" {
  creation_token = "efs-for-stw-node"
}

resource "aws_efs_mount_target" "stw_node_efs_mt_0" {
  file_system_id  = aws_efs_file_system.stw_node_efs.id
  subnet_id       = var.private_subnets[0]
  security_groups = [aws_security_group.allow_nfs.id]
}

resource "aws_efs_mount_target" "stw_node_efs_mt_1" {
  file_system_id  = aws_efs_file_system.stw_node_efs.id
  subnet_id       = var.private_subnets[1]
  security_groups = [aws_security_group.allow_nfs.id]
}

resource "aws_efs_mount_target" "stw_node_efs_mt_2" {
  file_system_id  = aws_efs_file_system.stw_node_efs.id
  subnet_id       = var.private_subnets[2]
  security_groups = [aws_security_group.allow_nfs.id]
}

resource "kubernetes_storage_class_v1" "efs" {
  metadata {
    name = "efs"
    annotations = {
      "storageclass.kubernetes.io/is-default-class" = "true"
    }
  }

  storage_provisioner = "efs.csi.aws.com"
  parameters = {
    provisioningMode = "efs-ap"                            # Dynamic provisioning
    fileSystemId     = aws_efs_file_system.stw_node_efs.id # module.efs.id
    directoryPerms   = "777"
  }

  mount_options = [
    "iam"
  ]
}
```
\normalsize

#### Cluster Autoscaler

The EKS Cluster Autoscaler ensures that the cluster can automatically scale its worker nodes based on the workload demands, ensuring optimal resource utilization and performance.

The necessary IAM settings are set up prior to deploying the Autoscaler. First, an IAM policy named `"node_additional"` is created to grant permission to describe EC2 instances and related resources. This enables the Autoscaler to gather information about the current state of the worker nodes and make informed decisions regarding scaling. For each managed node group in the EKS cluster (defined by the `"eks_managed_node_groups"` module output), the IAM policy is attached to its corresponding IAM role. This ensures that all worker nodes have the required permissions to work with the Autoscaler. After setting up the IAM policies, tags are added to provide the necessary information for the EKS Cluster Autoscaler to identify and manage the Auto Scaling Groups effectively and to support cluster autoscaling from zero for each node group. The tags are created for each node group (`"nodegroup_t3_small"`, `"nodegroup_t3_medium"` ,and `"nodegroup_t3_large"`) and are based on the specified tag lists defined in the `"local.eks_asg_tag_list_*"` variables. 

The EKS Cluster Autoscaler itself is instantiated using the custom `"eks_autoscaler"` module on the bottom of the code snippet. The module is called to set up the Autoscaler for the EKS cluster and the required input variables are provided accordingly. Its components are described in detailed in the following.

\footnotesize
```javascript
#
# EKS Cluster autoscaler
#
resource "aws_iam_policy" "node_additional" {
  name        = "${local.cluster_name}-additional"
  description = "${local.cluster_name} node additional policy"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = [
          "ec2:Describe*",
        ]
        Effect   = "Allow"
        Resource = "*"
      },
    ]
  })
}

resource "aws_iam_role_policy_attachment" "additional" {
  for_each = module.eks.eks_managed_node_groups

  policy_arn = aws_iam_policy.node_additional.arn
  role       = each.value.iam_role_name
}

# Tags for the ASG to support cluster-autoscaler scale up from 0 for nodegroup2
resource "aws_autoscaling_group_tag" "nodegroup_t3_small" {
  for_each               = local.eks_asg_tag_list_nodegroup_t3_small_label
  autoscaling_group_name = element(module.eks.eks_managed_node_groups_autoscaling_group_names, 2)
  tag {
    key                 = each.key
    value               = each.value
    propagate_at_launch = true
  }
}

resource "aws_autoscaling_group_tag" "nodegroup_t3_medium" {
  for_each               = local.eks_asg_tag_list_nodegroup_t3_medium_label
  autoscaling_group_name = element(module.eks.eks_managed_node_groups_autoscaling_group_names, 1)
  tag {
    key                 = each.key
    value               = each.value
    propagate_at_launch = true
  }
}

resource "aws_autoscaling_group_tag" "nodegroup_t3_large" {
  for_each               = local.eks_asg_tag_list_nodegroup_t3_large_label
  autoscaling_group_name = element(module.eks.eks_managed_node_groups_autoscaling_group_names, 0)
  tag {
    key                 = each.key
    value               = each.value
    propagate_at_launch = true
  }
}

module "eks_autoscaler" {
  source                          = "./autoscaler"
  cluster_name                    = local.cluster_name
  cluster_namespace               = local.cluster_namespace
  aws_region                      = var.aws_region
  cluster_oidc_issuer_url         = module.eks.cluster_oidc_issuer_url
  autoscaler_service_account_name = local.autoscaler_service_account_name
}
```
\normalsize

The configurationof the Cluster Autoscaler  begins with the creation of a Helm release named `"cluster-autoscaler"` using the `"helm_release"` resource. The Helm chart is sourced from the `"kubernetes.github.io/autoscaler"` repository with the chart version `"9.10.7"`. The settings inside the Helm release include the AWS region, RBAC (Role-Based Access Control) settings for the service account, cluster auto-discovery settings, and the creation of the service account with the required permissions.

The necessary resources for the settings are created accordingly in the following. The service account is created using the `"iam_assumable_role_admin"` module with an assumable IAM role that allows the service account to access the necessary resources for scaling. It is associated with the OIDC (OpenID Connect) provider for the cluster to permit access.

An IAM policy named `"cluster_autoscaler"` is created to permit the Cluster Autoscaler to interact with Auto Scaling Groups, EC2 instances, launch configurations, and tags. The policy includes two statements: `"clusterAutoscalerAll"` and `"clusterAutoscalerOwn"`. The first statement grants read access to Auto Scaling Group-related resources, while the second statement allows the Cluster Autoscaler to modify the desired capacity of the Auto Scaling Groups and terminate instances. The policy also includes conditions to ensure that the Cluster Autoscaler can only modify resources with specific tags. The conditions check that the Auto Scaling Group has a tag `"k8s.io/cluster-autoscaler/enabled"` set to `"true"` and a tag `"k8s.io/cluster-autoscaler/<cluster_name>"` set to `"owned"`. If you remember it, we have set these tags when setting up the managed node groups for the EKS Cluster in the previous step.

\footnotesize
```javascript
resource "helm_release" "cluster-autoscaler" {
  name             = "cluster-autoscaler"
  namespace        = var.cluster_namespace
  repository       = "https://kubernetes.github.io/autoscaler"
  chart            = "cluster-autoscaler"
  version          = "9.10.7"
  create_namespace = false

  set {
    name  = "awsRegion"
    value = var.aws_region
  }
  set {
    name  = "rbac.serviceAccount.name"
    value = var.autoscaler_service_account_name
  }
  set {
    name  = "rbac.serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"
    value = module.iam_assumable_role_admin.iam_role_arn
    type  = "string"
  }
  set {
    name  = "autoDiscovery.clusterName"
    value = var.cluster_name
  }
  set {
    name  = "autoDiscovery.enabled"
    value = "true"
  }
  set {
    name  = "rbac.create"
    value = "true"
  }
}

module "iam_assumable_role_admin" {
  source                        = "terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc"
  version                       = "~> 4.0"
  create_role                   = true
  role_name                     = "cluster-autoscaler"
  provider_url                  = replace(var.cluster_oidc_issuer_url, "https://", "")
  role_policy_arns              = [aws_iam_policy.cluster_autoscaler.arn]
  oidc_fully_qualified_subjects = ["system:serviceaccount:${var.cluster_namespace}:${var.autoscaler_service_account_name}"]
}

resource "aws_iam_policy" "cluster_autoscaler" {
  name_prefix = "cluster-autoscaler"
  description = "EKS cluster-autoscaler policy for cluster ${var.cluster_name}"
  policy      = data.aws_iam_policy_document.cluster_autoscaler.json
}

data "aws_iam_policy_document" "cluster_autoscaler" {
  statement {
    sid    = "clusterAutoscalerAll"
    effect = "Allow"

    actions = [
      "autoscaling:DescribeAutoScalingGroups",
      "autoscaling:DescribeAutoScalingInstances",
      "autoscaling:DescribeLaunchConfigurations",
      "autoscaling:DescribeTags",
      "ec2:DescribeLaunchTemplateVersions",
    ]

    resources = ["*"]
  }

  statement {
    sid    = "clusterAutoscalerOwn"
    effect = "Allow"

    actions = [
      "autoscaling:SetDesiredCapacity",
      "autoscaling:TerminateInstanceInAutoScalingGroup",
      "autoscaling:UpdateAutoScalingGroup",
    ]

    resources = ["*"]

    condition {
      test     = "StringEquals"
      variable = "autoscaling:ResourceTag/k8s.io/cluster-autoscaler/${var.cluster_name}"
      values   = ["owned"]
    }
    condition {
      test     = "StringEquals"
      variable = "autoscaling:ResourceTag/k8s.io/cluster-autoscaler/enabled"
      values   = ["true"]
    }
  }
}
```
\normalsize
