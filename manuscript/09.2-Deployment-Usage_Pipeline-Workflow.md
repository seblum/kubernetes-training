

## Pipeline Workflow

The code and machine learning pipeline have been modularized into distinct steps, including preprocessing, model training, model comparison, and model serving. Airflow serves as the model workflow tool, generating DAGs for managing the pipeline. MLflow is integrated to facilitate model tracking, registry, and serving functionalities. To ensure portability and scalability, the codebase has been containerized using Docker, allowing it to be executed in Docker and/or Kubernetes environments.

The `src` code is installed as a Python package within the Docker container, enabling easy invocation within the Airflow DAG. However, it is important to note that although Model Serving is triggered within the Airflow pipeline, it consists of a separate Python code and is not integrated into the `src` package. Likewise, model inferencing has its own distinct description and functionality. The code bases for both model serving and model inferencing can be found in the app/ directory, alongside their respective Dockerfiles. A detailed explanation of how these components function will be provided in the following section.

### Airflow Workflow

The specification of the Airflow DAG, which includes the DAG structure, tasks, and their dependencies, can be found in the `airflow_DAG.py` file. The DAG is built using the TaskFlow API.

An ML pipeline of this use case consists of three main steps: preprocessing, training, and serving. The preprocessing step involves data processing and storing it in the S3 storage. The training step and code are designed to accommodate different TensorFlow models, allowing for parallel training on multiple models, thereby reducing the time to deployment. Since there are multiple models, it is essential to serve only the model with the best metrics based on the current data. Hence, an intermediate step is incorporated to compare the metrics of all the models and select the best one for serving.

To execute the pipeline steps, the Airflow Docker Operator is employed, which ensures that each step runs in a separate and isolated environment using Docker or Kubernetes jobs. Dockerizing the code is a prerequisite for this process. The Airflow task then invokes the relevant methods of the Python code and executes them accordingly.

Once the model is in the serving phase, a Streamlit app is deployed for applying inference on new data.

![DAG pipeline](images/09-Deployment-Usage/DAG-pipeline.png) 

The code below defines the `ml_pipeline_dag` function as an Airflow DAG using the `dag` decorator. Each step of the pipeline, including data preprocessing, model training, model comparison, and serving the best model, is represented as a separate task with the `@task` decorator. Dependencies between these tasks are established by passing the output of one task as an argument to the next task. The `ml_pipeline` object serves as a representation of the entire DAG.

#### Importing Dependencies {.unlisted .unnumbered}
At first the necessary dependencies for the code are imported, including libraries for MLflow, Airflow, and other utilities.

```python
# Imports necessary packages
import os
from enum import Enum
import mlflow
import pendulum
from airflow.decorators import dag, task

```

#### Setting MLflow Tracking URI and Experiment {.unlisted .unnumbered}
Secondly, the necessary variables and constants for the whole Airflow DAG are defined and set. Either hard coded as string, or read from environment variables. Also, the MLflow tracking URI is set and a MLflow experiment retrieved, or created if none exists already.

```python
# Define variables and constants
MLFLOW_TRACKING_URI_local = "http://127.0.0.1:5008/"
MLFLOW_TRACKING_URI = "http://host.docker.internal:5008"
EXPERIMENT_NAME = "cnn_skin_cancer"
AWS_BUCKET = os.getenv("AWS_BUCKET")
AWS_REGION = os.getenv("AWS_REGION")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_ROLE_NAME = os.getenv("AWS_ROLE_NAME")

# Set MLflow tracking URI
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI_local)

try:
    # Creating an experiment
    mlflow_experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)
except:
    pass
# Setting the environment with the created experiment
mlflow_experiment_id = mlflow.set_experiment(EXPERIMENT_NAME).experiment_id

```

#### Setting Default Arguments and Environment Data {.unlisted .unnumbered}

```python 
# Set various model params and airflow or environment args
dag_default_args = {
    "owner": "seblum",
    "depends_on_past": False,
    "start_date": pendulum.datetime(2021, 1, 1, tz="UTC"),
    "tags": ["Keras CNN to classify skin cancer"],
}

kwargs_env_data = {
    "MLFLOW_TRACKING_URI": MLFLOW_TRACKING_URI,
    "MLFLOW_EXPERIMENT_ID": mlflow_experiment_id,
    "AWS_ACCESS_KEY_ID": AWS_ACCESS_KEY_ID,
    "AWS_SECRET_ACCESS_KEY": AWS_SECRET_ACCESS_KEY,
    "AWS_BUCKET": AWS_BUCKET,
    "AWS_REGION": AWS_REGION,
    "AWS_ROLE_NAME": AWS_ROLE_NAME,
}

model_params = {
    "num_classes": 2,
    "input_shape": (224, 224, 3),
    "activation": "relu",
    "kernel_initializer_glob": "glorot_uniform",
    "kernel_initializer_norm": "normal",
    "optimizer": "adam",
    "loss": "binary_crossentropy",
    "metrics": ["accuracy"],
    "validation_split": 0.2,
    "epochs": 2,
    "batch_size": 64,
    "learning_rate": 1e-5,
    "pooling": "avg",  # needed for resnet50
    "verbose": 2,
}

```

#### Defining the Airflow DAG {.unlisted .unnumbered}
After all parameters have been set, the actual Airflow DAG for the CNN skin cancer workflow is defined. As each single task of the ML pipeline of the Airflow DAG is executed as a container run, the container image that is pulled from DockerHub needs to be specified.

```python
skin_cancer_container_image = "seblum/cnn-skin-cancer:latest"

@dag(
    "cnn_skin_cancer_docker_workflow",
    default_args=dag_default_args,
    schedule_interval=None,
    max_active_runs=1,
)
def cnn_skin_cancer_workflow():

```

#### Defining Preprocessing Task {.unlisted .unnumbered}
As a first step, the preprocessing task is defined, which performs data preprocessing.

```python
    @task.docker(
        image=skin_cancer_container_image,
        multiple_outputs=True,
        # Add the previously defined variables and constants as environment variables to the container
        environment=kwargs_env_data,
        working_dir="/app",
        force_pull=True,
        network_mode="bridge",
    )
    def preprocessing_op(mlflow_experiment_id):
        """
        Perform data preprocessing.

        Args:
            mlflow_experiment_id (str): The MLflow experiment ID.

        Returns:
            dict: A dictionary containing the paths to preprocessed data.
        """
        import os

        from src.preprocessing import data_preprocessing

        aws_bucket = os.getenv("AWS_BUCKET")

        (
            X_train_data_path,
            y_train_data_path,
            X_test_data_path,
            y_test_data_path,
        ) = data_preprocessing(mlflow_experiment_id=mlflow_experiment_id, aws_bucket=aws_bucket)

        # Create dictionary with S3 paths to return
        return_dict = {
            "X_train_data_path": X_train_data_path,
            "y_train_data_path": y_train_data_path,
            "X_test_data_path": X_test_data_path,
            "y_test_data_path": y_test_data_path,
        }
        return return_dict

```

#### Defining Model Training Task {.unlisted .unnumbered}
Similarly, the model training task is defined, which trains a machine learning model.

```python
    @task.docker(
        image=skin_cancer_container_image,
        multiple_outputs=True,
        environment=kwargs_env_data,
        working_dir="/app",
        force_pull=True,
        network_mode="bridge",
    )
    def model_training_op(mlflow_experiment_id, model_class, model_params, input):
        """
        Train a model.

        Args:
            mlflow_experiment_id (str): The MLflow experiment ID.
            model_class (str): The class of the model to train.
            model_params (dict): A dictionary containing the model parameters.
            input (dict): A dictionary containing the input data.

        Returns:
            dict: A dictionary containing the results of the model training.
        """
        import os

        from src.train import train_model

        aws_bucket = os.getenv("AWS_BUCKET")
        run_id, model_name, model_version, model_stage = train_model(
            mlflow_experiment_id=mlflow_experiment_id,
            model_class=model_class,
            model_params=model_params,
            aws_bucket=aws_bucket,
            import_dict=input,
        )

        return_dict = {
            "run_id": run_id,
            "model_name": model_name,
            "model_version": model_version,
            "model_stage": model_stage,
        }
        return return_dict

```

#### Defining Model Comparison Task {.unlisted .unnumbered}
The following code snippet defines the model comparison task, which compares trained models.

```python
    @task.docker(
        image=skin_cancer_container_image,
        multiple_outputs=True,
        environment=kwargs_env_data,
        force_pull=True,
        network_mode="bridge",
    )
    def compare_models_op(train_data_basic, train_data_resnet50, train_data_crossval):
        """
        Compare trained models.

        Args:
            train_data_basic (dict): A dictionary containing the results of training the basic model.
            train_data_resnet50 (dict): A dictionary containing the results of training the ResNet50 model.
            train_data_crossval (dict): A dictionary containing the results of training the CrossVal model.

        Returns:
            dict: A dictionary containing the results of the model comparison.
        """
        compare_dict = {
            train_data_basic["model_name"]: train_data_basic["run_id"],
            train_data_resnet50["model_name"]: train_data_resnet50["run_id"],
            train_data_crossval["model_name"]: train_data_crossval["run_id"],
        }

        print(compare_dict)
        from src.compare_models import compare_models

        serving_model_name, serving_model_uri, serving_model_version = compare_models(input_dict=compare_dict)
        return_dict = {
            "serving_model_name": serving_model_name,
            "serving_model_uri": serving_model_uri,
            "serving_model_version": serving_model_version,
        }
        return return_dict

```

#### Defining Pipeline {.unlisted .unnumbered}
After the tasks have been specified, they are connected together to define the workflow pipeline, specifying inputs and outputs.

```python 
    # CREATE PIPELINE

    preprocessed_data = preprocessing_op(
        mlflow_experiment_id=mlflow_experiment_id,
    )
    train_data_basic = model_training_op(
        mlflow_experiment_id=mlflow_experiment_id,
        model_class=Model_Class.Basic.name,
        model_params=model_params,
        input=preprocessed_data,
    )
    train_data_resnet50 = model_training_op(
        mlflow_experiment_id=mlflow_experiment_id,
        model_class=Model_Class.ResNet50.name,
        model_params=model_params,
        input=preprocessed_data,
    )
    train_data_crossval = model_training_op(
        mlflow_experiment_id=mlflow_experiment_id,
        model_class=Model_Class.CrossVal.name,
        model_params=model_params,
        input=preprocessed_data,
    )
    
    # Similarly, the operations for compare_models, serve_fastapi_app, and serve_streamlit_app
    # would be added to the pipeline as well.

```

Finally, the Airflow DAG function is called in a last step.

```python
# Call the airflow DAG
cnn_skin_cancer_workflow()

```

### MLflow

Mlflow is leveraged in the preprocessing and model training stages to store crucial data parameters, model training parameters, and metrics, while also enabling the saving of trained models in the model registry. In the `airflow_DAG.py` file, Mlflow is invoked to create an experiment, and the experiment ID is passed to each pipeline step to store parameters in separate runs. This ensures a clear distinction between the execution of different models.

The `train_model` pipeline steps serve as a container for the model training procedure. Within the container, the model is trained using specific code. All the relevant information about the model and the model itself are logged using mlflow as well. This workflow ensures the comprehensive tracking of model parameters and metrics, and the saved model can be accessed and compared during the subsequent model comparison step. In fact, during this stage, the best model is transferred to another model stage within the model registry.
