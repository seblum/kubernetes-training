## Modules

Within the setup, there are multiple custom modules, namely airflow, mlflow, jupyterhub, and monitoring. Each module is responsible for deploying a specific workflow tool.

These module names also align with their corresponding namespaces within the cluster.

### Airflow

The `Airflow` module is responsible for provisioning all components related to the deployment of Airflow. Being a crucial workflow orchestration tool in our ML platform, Airflow is tightly integrated with various other components in the Terraform codebase, which requires it to receive multiple input variables and configurations. 
Airflow itself is deployed in the Terraform code via a Helm chart. The provided Terraform code also integrates the Airflow deployment with AWS S3 for efficient data storage and logging. It also utilizes an AWS RDS instance from the infrastructure section to serve as the metadata storage. Additionally, relevant Kubernetes secrets are incorporated into the setup to ensure a secure deployment.

The code starts by declaring several local variables that store the names of Kubernetes secrets and S3 buckets for data storage and logging. Next, it creates a Kubernetes namespace for Airflow to isolate the deployment.

```javascript
locals {
  k8s_airflow_db_secret_name   = "${var.name_prefix}-${var.namespace}-db-auth"
  git_airflow_repo_secret_name = "${var.name_prefix}-${var.namespace}-https-git-secret"
  git_organization_secret_name = "${var.name_prefix}-${var.namespace}-organization-git-secret"
  s3_data_bucket_secret_name   = "${var.name_prefix}-${var.namespace}-${var.s3_data_bucket_secret_name}"
  s3_data_bucket_name          = "${var.name_prefix}-${var.namespace}-${var.s3_data_bucket_name}"
  s3_log_bucket_name           = "${var.name_prefix}-${var.namespace}-log-storage"
}

data "aws_caller_identity" "current" {}
data "aws_region" "current" {} # 

resource "kubernetes_namespace" "airflow" {
  metadata {

    name = var.namespace
  }
}

#
# Log Storage
#
module "s3-remote-logging" {
  source             = "./remote_logging"
  s3_log_bucket_name = local.s3_log_bucket_name
  namespace          = var.namespace
  s3_force_destroy   = var.s3_force_destroy
  oidc_provider_arn  = var.oidc_provider_arn
}

#
# Data Storage
#
module "s3-data-storage" {
  source                     = "./data_storage"
  s3_data_bucket_name        = local.s3_data_bucket_name
  namespace                  = var.namespace
  s3_force_destroy           = var.s3_force_destroy
  s3_data_bucket_secret_name = local.s3_data_bucket_secret_name
}
```
Afterward, two custom modules, `"s3-remote-logging"` and `"s3-data-storage"` set up S3 buckets for remote logging and data storage. Both modules handle creating the S3 buckets and necessary IAM roles for accessing them. The terraform code of both modules is not depicted here, it is visible on [github](https://github.com/seblum/mlops-airflow-on-eks) though.
The main difference between the modules are in the in the assume role policies that are needed for the different use cases of storing and reading data, or logging to S3. While the `"s3_log_bucket_role"` allows a Federated entity, specified by an OIDC provider ARN, to assume the role using `"sts:AssumeRoleWithWebIdentity"`, the `"s3_data_bucket_role"` allows both a specific IAM user (constructed from the user's ARN) and the Amazon S3 service itself to assume the role using `"sts:AssumeRole"`.

**s3-data-storage role policy**

```javascript
# s3-data-storage role policy
resource "aws_iam_role" "s3_data_bucket_role" {
  name                 = "${var.namespace}-s3-data-bucket-role"
  max_session_duration = 28800

  assume_role_policy = <<EOF
  {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
              "AWS": "arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/${aws_iam_user.s3_data_bucket_user.name}"
            },
            "Action": "sts:AssumeRole"
        },
        {
            "Effect": "Allow",
            "Principal": {
              "Service": "s3.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
  }
  EOF
}
```

**s3-remote-logging role policy**

```javascript
# s3-remote-logging role policy
resource "aws_iam_role" "s3_log_bucket_role" {
  name                 = "${var.namespace}-s3-log-bucket-access-role"
  max_session_duration = 28800

  assume_role_policy = <<EOF
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Action" : "sts:AssumeRoleWithWebIdentity",
        "Effect": "Allow",
        "Principal" : {
          "Federated" : [
            "${var.oidc_provider_arn}" 
          ]
        }
      }
    ]
  }
  EOF
}
```

After the S3 buckets are set up, the code proceeds to create Kubernetes secrets to store various credentials required for Airflow's operation. These include credentials for PostgreSQL database, GitHub authentication secrets for accessing private repositories, and secrets for accessing GitHub organizations which are required to authenticate users. The `"rds-airflow"` module is used to create the RDS instance for Airflow, which will serve as the external database for the deployment.

The Apache Airflow deployment is defined using a `helm_release` of the *Airflow Community Helm Chart* and is highly customized to cater to our specific needs. The release includes various configurations for Airflow, such as custom environment variables, extra environment variables sourced from the GitHub organization secret, and the `KubernetesExecutor` Airflow executor.
The deployment enables DAG synchronization of a dedicated Github repository which includes our Airflow DAGs (see chapter 9). It also configures a persistent volume using Amazon EFS for Airflow logs and a Kubernetes Ingress resource to expose the Airflow web interface using an Application Load Balancer (ALB). Additionally, readiness and liveness probes are configured for the web server.

```javascript
#
# Helm Release Airflow
#
resource "kubernetes_secret" "airflow_db_credentials" {
  metadata {
    name      = local.k8s_airflow_db_secret_name
    namespace = helm_release.airflow.namespace
  }
  data = {
    "postgresql-password" = module.rds-airflow.rds_password
  }
}

resource "kubernetes_secret" "airflow_https_git_secret" {
  metadata {
    name      = local.git_airflow_repo_secret_name
    namespace = helm_release.airflow.namespace
  }
  data = {
    "username" = var.git_username
    "password" = var.git_token
  }
}

resource "kubernetes_secret" "airflow_organization_git_secret" {
  metadata {
    name      = local.git_organization_secret_name
    namespace = helm_release.airflow.namespace
  }
  data = {
    "GITHUB_CLIENT_ID"     = var.git_client_id
    "GITHUB_CLIENT_SECRET" = var.git_client_secret
  }
}

# RDS
resource "random_password" "rds_password" {
  length  = 16
  special = false
}

module "rds-airflow" {
  source                      = "../../infrastructure/rds"
  vpc_id                      = var.vpc_id
  private_subnets             = var.private_subnets
  private_subnets_cidr_blocks = var.private_subnets_cidr_blocks
  rds_port                    = var.rds_port
  rds_name                    = var.rds_name
  rds_password                = coalesce(var.rds_password, random_password.rds_password.result)
  rds_engine                  = var.rds_engine
  rds_engine_version          = var.rds_engine_version
  rds_instance_class          = var.rds_instance_class
  storage_type                = var.storage_type
  max_allocated_storage       = var.max_allocated_storage
}

# HELM
resource "helm_release" "airflow" {
  name             = var.name
  namespace        = var.namespace
  create_namespace = var.create_namespace

  repository = "https://airflow-helm.github.io/charts"
  chart      = var.helm_chart_name
  version    = var.helm_chart_version
  wait       = false # deactivate post install hooks otherwise will fail

  values = [yamlencode({
    airflow = {
      extraEnv = [
        {
          name = "GITHUB_CLIENT_ID"
          valueFrom = {
            secretKeyRef = {
              name = local.git_organization_secret_name
              key  = "GITHUB_CLIENT_ID"
            }
          }
        },
        {
          name = "GITHUB_CLIENT_SECRET"
          valueFrom = {
            secretKeyRef = {
              name = local.git_organization_secret_name
              key  = "GITHUB_CLIENT_SECRET"
            }
          }
        }
      ],
      config = {
        AIRFLOW__WEBSERVER__EXPOSE_CONFIG = true
        AIRFLOW__WEBSERVER__BASE_URL      = "http://${var.domain_name}/${var.domain_suffix}"

        AIRFLOW__CORE__LOAD_EXAMPLES = false
        # AIRFLOW__LOGGING__LOGGING_LEVEL          = "DEBUG"
        # AIRFLOW__LOGGING__REMOTE_LOGGING         = true
        # AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER = "s3://${module.s3-data-storage.s3_log_bucket_name}/airflow/logs"
        # AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID     = "aws_logs_storage_access"
        AIRFLOW__CORE__DEFAULT_TIMEZONE = "Europe/Amsterdam"
      },
      users = []
      image = {
        repository = "seblum/airflow"
        tag        = "2.6.3-python3.11-custom-light"
        pullPolicy = "IfNotPresent"
        pullSecret = ""
        uid        = 50000
        gid        = 0
      },
      executor           = "KubernetesExecutor"
      fernetKey          = var.fernet_key
      webserverSecretKey = "THIS IS UNSAFE!"
      connections = [
        {
          id          = "aws_logs_storage_access"
          type        = "aws"
          description = "AWS connection to store logs on S3"
          extra       = "{\"region_name\": \"${data.aws_region.current.name}\"}"
        }
      ],
      variables = [
        {
          key   = "MLFLOW_TRACKING_URI"
          value = "http://mlflow-service.mlflow.svc.cluster.local"
        },
        {
          key   = "s3_access_name"
          value = "${local.s3_data_bucket_secret_name}"
        }
      ]
    },
    serviceAccount = {
      create = true
      name   = "airflow-sa"
      annotations = {
        "eks.amazonaws.com/role-arn" = "${module.s3-remote-logging.s3_log_bucket_role_arn}"
      }
    },
    scheduler = {
      logCleanup = {
        enabled = false
      }
    },
    workers = {
      enabled = false
      logCleanup = {
        enables = true
      }
    },
    flower = {
      enabled = false
    },
    postgresql = {
      enabled = false
    },
    redis = {
      enabled = false
    },
    externalDatabase = {
      type              = "postgres"
      host              = module.rds-airflow.rds_host
      port              = var.rds_port
      database          = "airflow_db"
      user              = "airflow_admin"
      passwordSecret    = local.k8s_airflow_db_secret_name
      passwordSecretKey = "postgresql-password"
    },
    dags = {
      path = "/opt/airflow/dags"
      gitSync = {
        enabled               = true
        repo                  = var.git_repository_url
        branch                = var.git_branch
        revision              = "HEAD"
        repoSubPath           = "workflows"
        httpSecret            = local.git_airflow_repo_secret_name
        httpSecretUsernameKey = "username"
        httpSecretPasswordKey = "password"
        syncWait              = 60
        syncTimeout           = 120
      }
    },
    logs = {
      path = "/opt/airflow/logs"
      persistence = {
        enabled = true
        storageClass : "efs"
        size : "5Gi"
        accessMode : "ReadWriteMany"
      }
    },
    ingress = {
      enabled    = true
      apiVersion = "networking.k8s.io/v1"
      web = {
        annotations = {
          "external-dns.alpha.kubernetes.io/hostname"  = "${var.domain_name}"
          "alb.ingress.kubernetes.io/scheme"           = "internet-facing"
          "alb.ingress.kubernetes.io/target-type"      = "ip"
          "kubernetes.io/ingress.class"                = "alb"
          "alb.ingress.kubernetes.io/group.name"       = "mlplatform"
          "alb.ingress.kubernetes.io/healthcheck-path" = "/${var.domain_suffix}/health"
        }
        path = "/${var.domain_suffix}"
        host = "${var.domain_name}"
        precedingPaths = [{
          path        = "/${var.domain_suffix}*"
          serviceName = "airflow-web"
          servicePort = "web"
        }]
      }
    },
    web = {
      readinessProbe = {
        enabled             = true
        initialDelaySeconds = 45
      },
      livenessProbe = {
        enabled             = true
        initialDelaySeconds = 45
      },
      webserverConfig = {
        stringOverride = file("${path.module}/WebServerConfig.py")
      }
    },
  })]
}
```

In a final step of the Helm Chart, a custom `WebServerConfig.py` is specified which is set to integrate our Airflow deployment with a Github Authentication provider. The python script consists of two major parts: a custom AirflowSecurityManager class definition and the actual webserver_config configuration file for Apache Airflow's web server.

The custom `CustomSecurityManager` class extends the default AirflowSecurityManager to retrieves user information from the GitHub OAuth provider. The webserver_config configuration sets up the configurations for the web server component of Apache Airflow by indicating that OAuth will be used for user authentication. The `SECURITY_MANAGER_CLASS` is set to the previously defined `CustomSecurityManager` to customizes how user information is retrieved from the OAuth provider. Finally, the GitHub provider is configured with its required parameters like `client_id`, `client_secret`, and API endpoints.

```python
#######################################
# Custom AirflowSecurityManager
#######################################
from airflow.www.security import AirflowSecurityManager
import os


class CustomSecurityManager(AirflowSecurityManager):
    def get_oauth_user_info(self, provider, resp):
        if provider == "github":
            user_data = self.appbuilder.sm.oauth_remotes[provider].get("user").json()
            emails_data = (
                self.appbuilder.sm.oauth_remotes[provider].get("user/emails").json()
            )
            teams_data = (
                self.appbuilder.sm.oauth_remotes[provider].get("user/teams").json()
            )

            # unpack the user's name
            first_name = ""
            last_name = ""
            name = user_data.get("name", "").split(maxsplit=1)
            if len(name) == 1:
                first_name = name[0]
            elif len(name) == 2:
                first_name = name[0]
                last_name = name[1]

            # unpack the user's email
            email = ""
            for email_data in emails_data:
                if email_data["primary"]:
                    email = email_data["email"]
                    break

            # unpack the user's teams as role_keys
            # NOTE: each role key will be "my-github-org/my-team-name"
            role_keys = []
            for team_data in teams_data:
                team_org = team_data["organization"]["login"]
                team_slug = team_data["slug"]
                team_ref = team_org + "/" + team_slug
                role_keys.append(team_ref)

            return {
                "username": "github_" + user_data.get("login", ""),
                "first_name": first_name,
                "last_name": last_name,
                "email": email,
                "role_keys": role_keys,
            }
        else:
            return {}


#######################################
# Actual `webserver_config.py`
#######################################
from flask_appbuilder.security.manager import AUTH_OAUTH

# only needed for airflow 1.10
# from airflow import configuration as conf
# SQLALCHEMY_DATABASE_URI = conf.get("core", "SQL_ALCHEMY_CONN")

AUTH_TYPE = AUTH_OAUTH
SECURITY_MANAGER_CLASS = CustomSecurityManager

# registration configs
AUTH_USER_REGISTRATION = True  # allow users who are not already in the FAB DB
AUTH_USER_REGISTRATION_ROLE = (
    "Public"  # this role will be given in addition to any AUTH_ROLES_MAPPING
)

# the list of providers which the user can choose from
OAUTH_PROVIDERS = [
    {
        "name": "github",
        "icon": "fa-github",
        "token_key": "access_token",
        "remote_app": {
            "client_id": os.getenv("GITHUB_CLIENT_ID"),
            "client_secret": os.getenv("GITHUB_CLIENT_SECRET"),
            "api_base_url": "https://api.github.com",
            "client_kwargs": {"scope": "read:org read:user user:email"},
            "access_token_url": "https://github.com/login/oauth/access_token",
            "authorize_url": "https://github.com/login/oauth/authorize",
        },
    },
]

# a mapping from the values of `userinfo["role_keys"]` to a list of FAB roles
AUTH_ROLES_MAPPING = {
    "github-organization/airflow-users-team": ["User"],
    "github-organization/airflow-admin-team": ["Admin"],
}

# if we should replace ALL the user's roles each login, or only on registration
AUTH_ROLES_SYNC_AT_LOGIN = True

# force users to re-auth after 30min of inactivity (to keep roles in sync)
PERMANENT_SESSION_LIFETIME = 1800
```

<!---
### Mlflow

To enable model tracking, MLflow is deployed with specific requirements. These include a data store on AWS S3, a metadata store using PostgreSQL (RDS), and the MLflow server. The first two components are created using Terraform resources.

However, MLflow does not have native support for Kubernetes and does not offer an official Helm chart. Despite being an excellent tool, we need to set up a basic custom Helm chart to deploy the MLflow server in this case. We also use a custom container image for running MLflow. This process involves creating YAML configurations for deployment, service, and configmap, which will be executed on our Kubernetes cluster.

It's important to note that the deployment has an open endpoint, which means it lacks sufficient security measures.

```javascript

```
--->

### Jupyterhub

JupyterHub is utilized in the setup to provide an IDE (Integrated Development Environment). Belows Terraform code defines a `helm_release` that deploys JupyterHub on our EKS cluster. There are no other resources needed to run JupyterHub compared to the other components of our ML platform
The Helm configuration specifies various settings and customizations to include a JupyterHub instance with a single-user Jupyter notebook server. For example defining a post-start lifecycle hook to run a Git clone command inside the single-user notebook server container, or defining extra environment variable for the single-user server, namely `"MLFLOW_TRACKING_URI"` pointing to the previous specified MLflow service.

The Airflow configuration enables an Ingress resource to expose JupyterHub to the specified domain, and adds annotations to control routing and manage the AWS Application Load Balancer (ALB). It also includes settings for the JupyterHub proxy and enables a culling mechanism to automatically shut down idle user sessions.

Similar to the Airflow deployment, the JupyterHub instance is configured to use GitHub OAuthenticator for user authentication. The OAuthenticator is configured with the provided GitHub `client_id` and `client_secret`, and the `oauth_callback_url` to set a specific endpoint under the specified domain name.

```javascript
resource "helm_release" "jupyterhub" {
  name             = var.name
  namespace        = var.name
  create_namespace = var.create_namespace

  repository = "https://jupyterhub.github.io/helm-chart/"
  chart      = var.helm_chart_name
  version    = var.helm_chart_version

  values = [yamlencode({
    singleuser = {
      defaultUrl = "/lab"
      image = {
        name = "seblum/jupyterhub-server"
        tag  = "latest"
      },
      lifecycleHooks = {
        postStart = {
          exec = {
            command = ["git", "clone", "${var.git_repository_url}"]
          }
        }
      },
      extraEnv = {
        "MLFLOW_TRACKING_URI" = "http://mlflow-service.mlflow.svc.cluster.local"
      }
    },
    ingress = {
      enabled : true
      annotations = {
        "external-dns.alpha.kubernetes.io/hostname" = "${var.domain_name}"
        "alb.ingress.kubernetes.io/scheme"          = "internet-facing"
        "alb.ingress.kubernetes.io/target-type"     = "ip"
        "kubernetes.io/ingress.class"               = "alb"
        "alb.ingress.kubernetes.io/group.name"      = "mlplatform"
      }
      hosts = ["${var.domain_name}", "www.${var.domain_name}"]
    },
    proxy = {
      service = {
        type = "ClusterIP"
      }
      secretToken = var.proxy_secret_token
    }
    cull = {
      enabled = true
      users   = true
    }
    hub = {
      baseUrl = "/${var.domain_suffix}"
      config = {
        GitHubOAuthenticator = {
          client_id          = var.git_client_id
          client_secret      = var.git_client_secret
          oauth_callback_url = "http://${var.domain_name}/${var.domain_suffix}/hub/oauth_callback"
        }
        JupyterHub = {
          authenticator_class = "github"
        }
      }
    }
  })]
}
```

<!---

### Monitoring

Monitoring using Prometheus and Grafana is included in the setup. Although it is not directly part of the ML pipeline, it serves as a good example of how to monitor the cluster effectively. A basic monitoring setup is achieved using Prometheus and Grafana.

Prometheus is employed for [specific purpose], while Grafana is used for [specific purpose]. Both Prometheus and Grafana are deployed using a Helm chart. The connection between Grafana and Prometheus is established within the Grafana Helm chart.

#### Prometheus
#### Grafana
--->

