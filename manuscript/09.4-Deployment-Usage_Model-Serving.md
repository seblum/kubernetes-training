
## Model Serving & Inferencing

The process of serving and making inferences utilizes Docker containers and runs them within Kubernetes pods.

The concept involves running a Docker container that serves the pre-trained TensorFlow model using FastAPI. This containerized model is responsible for providing predictions and responses to incoming requests. Additionally, a Streamlit app is used to interact with the served model, enabling users to make inferences by sending input data to the model and receiving the corresponding predictions.


### Model Serving

The model serving application is built on FastAPI, which includes various endpoints catering to our use case. The primary endpoint, `predict`, allows for multiple predictions to be made, while additional maintenance endpoints such as `info` or `health` provide relevant information about the app itself.

To initiate the prediction process, the model is first retrieved from the MLflow registry. The specific model to be fetched and the location of the MLflow server are specified through environment variables. Once the model is loaded, it is used to generate predictions based on the provided input data. The API call returns the predictions in the form of a Python list.

```python

import io
import os
from io import BytesIO

import mlflow
import mlflow.keras
import numpy as np
import pandas as pd
from fastapi import FastAPI, File, HTTPException, UploadFile
from fastapi.encoders import jsonable_encoder
from fastapi.responses import JSONResponse
from PIL import Image
from tensorflow import keras

# Create FastAPI instance
app = FastAPI()

model_name = "Skin Cancer Detection"
version = "v1.0.0"


@app.get("/info")
async def model_info():
    """Return model information, version, how to call"""
    return {"name": model_name, "version": version}


@app.get("/health")
async def service_health():
    """Return service health"""
    return {"ok"}


@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    # Get environment variables
    MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI")
    MLFLOW_MODEL_NAME = os.getenv("MLFLOW_MODEL_NAME")
    MLFLOW_MODEL_VERSION = os.getenv("MLFLOW_MODEL_VERSION")

    # TODO: insert types
    def _read_imagefile(data) -> Image.Image:
        image = Image.open(BytesIO(data))
        return image

    def _preprocess_image(image) -> np.array:
        np_image = np.array(image, dtype="uint8")
        np_image = np_image / 255.0
        np_image = np_image.reshape(1, 224, 224, 3)
        return np_image

    if file.filename.endswith(".jpg"):
        print("[+] Read File")
        image = _read_imagefile(await file.read())

        print("[+] Initialize MLflow")
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

        print("[+] Load Model")
        model = mlflow.keras.load_model(f"models:/{MLFLOW_MODEL_NAME}/{MLFLOW_MODEL_VERSION}")

        print("[+] Preprocess Data")
        np_image = _preprocess_image(image)

        print("[+] Initiate Prediction")
        preds = model.predict(np_image)

        print("[+] Return Model Prediction")
        return {"prediction": preds.tolist()}
    else:
        # Raise a HTTP 400 Exception, indicating Bad Request
        raise HTTPException(status_code=400, detail="Invalid file format. Only JPG Files accepted.")

```

### Streamlit App

The Streamlit app offers a simple interface for performing inferences on the served model. The user interface enables users to upload a `jpg` image. Upon clicking the `predict` button, the image is sent to the model serving app, where a prediction is made. The prediction results are then returned as a JSON file, which can be downloaded upon request.

```python

import io
import json
import os

import pandas as pd
import requests
import streamlit as st
from PIL import Image

st.header("MLOps Engineering Project")
st.subheader("Skin Cancer Detection")

# FastAPI endpoint
FASTAPI_SERVING_IP = os.getenv("FASTAPI_SERVING_IP")
FASTAPI_SERVING_PORT = os.getenv("FASTAPI_SERVING_PORT")
FASTAPI_ENDPOINT = f"http://{FASTAPI_SERVING_IP}:{FASTAPI_SERVING_PORT}/predict"

# check for pngs?
test_image = st.file_uploader("", type=["jpg"], accept_multiple_files=False)

if test_image:
    image = Image.open(test_image)
    image_file = io.BytesIO(test_image.getvalue())
    files = {"file": image_file}

    col1, col2 = st.columns(2)

    with col1:
        st.image(test_image, caption="", use_column_width="always")

    with col2:
        if st.button("Start Prediction"):
            with st.spinner("Prediction in Progress. Please Wait..."):
                output = requests.post(FASTAPI_ENDPOINT, files=files, timeout=8000)
            st.success("Success! Click the Download button below to retrieve prediction results (JSON format)")
            st.json(output.json())
            st.download_button(
                label="Download",
                data=json.dumps(output.json()),  # Download as JSON file object
                file_name="cnn_skin_cancer_prediction_results.json",
            )

```

