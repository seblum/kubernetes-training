
## Model Serving & Inferencing

The process of serving and making inferences utilizes Docker containers and runs them within Kubernetes pods.

The concept involves running a Docker container that serves the pre-trained TensorFlow model using FastAPI. This containerized model is responsible for providing predictions and responses to incoming requests. Additionally, a Streamlit app is used to interact with the served model, enabling users to make inferences by sending input data to the model and receiving the corresponding predictions.


### Model Serving

The model serving application is built on FastAPI, which includes various endpoints catering to our use case. The primary endpoint, `predict`, allows for multiple predictions to be made, while additional maintenance endpoints such as `info` or `health` provide relevant information about the app itself.

To initiate the prediction process, the model is first retrieved from the MLflow registry. The specific model to be fetched and the location of the MLflow server are specified through environment variables. Once the model is loaded, it is used to generate predictions based on the provided input data. The API call returns the predictions in the form of a Python list.

```python

# Imports necessary packages
# ...

# Create FastAPI instance
app = FastAPI()

model_name = "Skin Cancer Detection"
version = "v1.0.0"

@app.get("/info")
async def model_info():
    """
    Endpoint to retrieve information about the model.

    Returns:
        - Dictionary containing the model name and version
    """
    return {"name": model_name, "version": version}

@app.get("/health")
async def service_health():
    """
    Endpoint to check the health status of the service.

    Returns:
        - Dictionary indicating the health status of the service
    """
    return {"ok"}


@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """
    Endpoint to make predictions on skin cancer images.

    Parameters:
        - file: Uploaded image file (JPG format)

    Returns:
        - Prediction results as a JSON object
    """
    # Get environment variables
    MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI")
    MLFLOW_MODEL_NAME = os.getenv("MLFLOW_MODEL_NAME")
    MLFLOW_MODEL_VERSION = os.getenv("MLFLOW_MODEL_VERSION")

    def _read_imagefile(data) -> Image.Image:
        """
        Read image file from bytes data.

        Parameters:
            - data: Bytes data of the image file

        Returns:
            - PIL Image object
        """
        image = Image.open(BytesIO(data))
        return image

    def _preprocess_image(image) -> np.array:
        """
        Preprocess the input image for model prediction.

        Parameters:
            - image: PIL Image object

        Returns:
            - Processed numpy array image
        """
        np_image = np.array(image, dtype="uint8")
        np_image = np_image / 255.0
        np_image = np_image.reshape(1, 224, 224, 3)
        return np_image

    if file.filename.endswith(".jpg"):
        print("[+] Read File")
        image = _read_imagefile(await file.read())

        print("[+] Initialize MLflow")
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

        print("[+] Load Model")
        model = mlflow.keras.load_model(f"models:/{MLFLOW_MODEL_NAME}/{MLFLOW_MODEL_VERSION}")

        print("[+] Preprocess Data")
        np_image = _preprocess_image(image)

        print("[+] Initiate Prediction")
        preds = model.predict(np_image)

        print("[+] Return Model Prediction")
        return {"prediction": preds.tolist()}
    else:
        # Raise a HTTP 400 Exception, indicating Bad Request
        raise HTTPException(status_code=400, detail="Invalid file format. Only JPG Files accepted.")

```

### Streamlit App

The Streamlit app offers a simple interface for performing inferences on the served model. The user interface enables users to upload a `jpg` image. Upon clicking the `predict` button, the image is sent to the model serving app, where a prediction is made. The prediction results are then returned as a JSON file, which can be downloaded upon request.

```python

# Imports necessary packages
# ...

st.header("MLOps Engineering Project")
st.subheader("Skin Cancer Detection")

# FastAPI endpoint
FASTAPI_SERVING_IP = os.getenv("FASTAPI_SERVING_IP")
FASTAPI_SERVING_PORT = os.getenv("FASTAPI_SERVING_PORT")
FASTAPI_ENDPOINT = f"http://{FASTAPI_SERVING_IP}:{FASTAPI_SERVING_PORT}/predict"

test_image = st.file_uploader("", type=["jpg"], accept_multiple_files=False)

if test_image:
    image = Image.open(test_image)
    image_file = io.BytesIO(test_image.getvalue())
    files = {"file": image_file}

    col1, col2 = st.columns(2)

    with col1:
        # Display the uploaded image in the first column
        st.image(test_image, caption="", use_column_width="always")

    with col2:
        if st.button("Start Prediction"):
            with st.spinner("Prediction in Progress. Please Wait..."):
                # Send a POST request to FastAPI for prediction
                output = requests.post(FASTAPI_ENDPOINT, files=files, timeout=8000)
            st.success("Success! Click the Download button below to retrieve prediction results (JSON format)")
            # Display the prediction results in JSON format
            st.json(output.json())
            # Add a download button to download the prediction results as a JSON file
            st.download_button(
                label="Download",
                data=json.dumps(output.json()),
                file_name="cnn_skin_cancer_prediction_results.json",
            )

```

