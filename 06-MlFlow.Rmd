<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
# MLflow

MLflow is an open source platform to manage the machine learning lifecycle end-to-end. This includes experimentation, reproducibility, deployment, and registration of a ML model. MLflow provides four primary components to manage the ML lifecycle. They can be either used on their own or they also to work together.

* **MLflow Tracking** is used to log and compare parameters, code versions, metrics, and artifacts of an ML code. Results can be stored to local files or to remote servers and allow to compare multiple runs. MLflow Tracking comes with an API and a web interface to easily observe the logged parameters and artifacts.
* **MLflow Models** enables to manage and deploy machine learning models from multiple libraries. It allows to package your own ML model for later use in downstream tasks, e.g. real-time serving through a REST API. The package format defines a convention that saves the model in different *“flavors”* that can be interpreted by different downstream tools.
<<<<<<< HEAD
* **MLflow Registry** provides a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations. It comes with an API and UI for easy use of such funtionalities and each of those aspects can be checked in MLflows' web interface.
* **MLflow Projects** packages data science code in a standard format for a reusable, and reproducible form to share your code with other data scientists or transfer it to production. Each project is basically a local directory or a Git repository, and uses a descriptor file to specify its dependencies and entrypoints. An existing MLflow Project can be run also either locally or from a Git repository.

MLflow is library-agnostic, which means one can use it with any ML library and  programming language. All functions are accessible through a [REST API](https://MLflow.org/docs/latest/rest-api.html#rest-api) and [CLI](https://MLflow.org/docs/latest/cli.html#cli), and project comes with a Python API, R API, and Java API already included. It requires minimal changes to integrate MLflow into an existing codebase and aims to make it reproducible and reusable as easy as possible so other Data Scientists can reuse the code. It is even possible to define your own [plugins](https://mlflow.org/docs/latest/plugins.html#mlflow-plugins "Permalink to this headline")

MLflow comes with a web interface to conveniently view and compare models and metrics.

![Web Interface of MLflow](images/06-MLflow/MLflow_web_interface-overview.png)


## Prerequisites

To go through this chapter it is necessary to have python and MLflow installed. One can install MLflow locally via `pip install MLflow`. The tutorial is based on MLflow v2.1.1. It is also recommended to have knowledge of VirtualEnv, Conda, or Docker when working with MLflow Projects.


## Functionality

In the following the four primary components of MLflow are shown in more detail and with exemplary code.


### MLflow Tracking

MLflow Tracking allows to log and compare parameters, code versions, metrics, and artifacts. This can be easily done by minimal changes to your code using the MLflow Tracking API. The following examples depict the basic concepts and show how to use it. To use MLflow within our code it needs to be imported first.
```python
import mlflow
```

#### MLflow experiment

MLflow experiments are a part of MLflow’s tracking component that allow to group runs together based on custom criteria. For example we might create a new experiment for each new model architecture we want to evaluate.

```python
experiment_name = "introduction-experiment"
mlflow.set_experiment(experiment_name)
```


#### MLflow run

An MLflow run is an execution environment for a piece of machine learning code. Whenever we want to track parameters or performance of a ML run or experiment we need to create a new MLflow. This can be done using `MLflow.start_run()`. Using `MLflow.end_run()` we can similarly end the run. It is a good practice to pass a run name to the MLflow run to identify it easily afterwards.

```python
run_name = "example-run"

mlflow.start_run()
run = mlflow.active_run()
print(f"Active run_id: {run.info.run_id}")
mlflow.end_run()
```

This will start and end a MLflow run. It is also possible to use the context manager as shown below, which allows for a smoother style.

```python
run_name = "context-manager-run"

with mlflow.start_run(run_name=run_name) as run:
    run_id = run.info.run_id
    print(f"Active run_id: {run_id}")
```

**Child runs**
It is possible to create child runs of the current run, based on the run ID. This can be used for example to gain a better overview of multiple run. Belows code depicts and example on how to create a child run.

```python
# Create child runs based on the run ID
with mlflow.start_run(run_id=run_id) as parent_run:
    print("parent run_id: {}".format(parent_run.info.run_id))
    with mlflow.start_run(nested=True, run_name="test_dataset_abc.csv") as child_run:
        mlflow.log_metric("acc", 0.91)
        print("child run_id : {}".format(child_run.info.run_id))

with mlflow.start_run(run_id=run_id) as parent_run:
    print("parent run_id: {}".format(parent_run.info.run_id))
    with mlflow.start_run(nested=True, run_name="test_dataset_xyz.csv") as child_run:
        mlflow.log_metric("acc", 0.90)
        print("child run_id : {}".format(child_run.info.run_id))
```

#### Logging metrics & parameters

The main reason we use MLflow Tracking is to log and store parameters and metrics during our MLflow run. *Parameters* represent the input parameters used for training, e.g. the initial learning rate. *Metrics:* are used to track the progress of the model training and are usually updated over the course of a run. MLflow allows to keep track of the model’s train and validation losses and visualize their development across the training run. Parameters and metrics can be easily logged by calling `MLflow.log_param`, or `MLflow.log_metric`. We can also specify a tag to identify our run using `MLflow.set_tag`. Belows example show how to use each method within a run.

```python
run_name = "tracking-example-run"
experiment_name = "tracking-experiment"
mlflow.set_experiment(experiment_name)

with mlflow.start_run(run_name=run_name) as run:

    # Parameters
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_params({"epochs": 0.05, "final_activation": "sigmoid"})

    # Tags
    mlflow.set_tag("env", "dev")
    mlflow.set_tags({"some_tag": False, "project": "xyz"})

    # Metrics
    mlflow.log_metric("loss", 0.001)
    mlflow.log_metrics({"acc": 0.92, "auc": 0.90})

    # It is possible to log a metrics series (for example a training history)
    for val_loss in [0.1, 0.01, 0.001, 0.00001]:
        mlflow.log_metric("val_loss", val_loss)

    for val_acc in [0.6, 0.6, 0.8, 0.9]:
        mlflow.log_metric("val_acc", val_acc)

    run_id = run.info.run_id
    experiment_id = run.info.experiment_id
    print(f"run_id: {run_id}")
    print(f"experiment_id: {experiment_id}")
```

It is also possible to add information after the experiment ran. We just need to specifiy our run ID from the previous run for our new run. We use the `mlflow.client.MLflowClient` in this example. The `mlflow.client` module provides a Python CRUD interface, which is a lower level API directly translating to the MLflow [REST API](https://mlflow.org/docs/latest/rest-api.html) calls. It can be used similarly to `mlflow`-module of the higher level API. We want to mention it here to give you a hint of its existence.

```python
from mlflow.tracking import MLflowClient

# add a note to the experiment
MLflowClient().set_experiment_tag(
    experiment_id, "MLflow.note.content", "my experiment note")
# add a note to the run
MLflowClient().set_tag(run_id, "MLflow.note.content", "my run note")

# Or we can even log further metrics by calling MLflow.start_run on a specific ID
with mlflow.start_run(run_id=run_id):
    run = mlflow.active_run()
    mlflow.log_metric("f1", 0.9)
    print(f"run_id: {run.info.run_id}")
```

#### Display & View metrics

How can we use the logged parameters and metrics afterwards? It is possible to give an overview of the currently stored runs using the MLflow API and printing the results. 

```python
current_experiment = dict(mlflow.get_experiment_by_name(experiment_name))
mlflow_run = mlflow.search_runs([current_experiment['experiment_id']])
print(f"MLflow_run: {mlflow_run}")
```

![MLflow Model Tracking CLI Run Overview](images/06-MLflow/MLflow_cli_interface-tracking.png)

Yet, viewing all the results in the web interface of MLflow gives a much better overview. By default, the tracking API writes the data to the local filesystem of the machine it’s running on under a `./mlruns` directory. We can acces the MLflow’s Tracking UI by running `MLflow ui` via our console, and view it in our browser under http://localhost:5000 (The port: 5000 is the MLflow default). The metrics dashboard of a run looks like to following

![MLflow Model Tracking Dashboard](images/06-MLflow/MLflow_web_interface-tracking.png)

It is also possible to configure MLflow to log to a remote tracking server, to manage results centrally or share them across a team. To get access to a remote tracking server it is needed to set a MLflow tracking URI. This can be done either by setting an environment variable `MLflow_TRACKING_URI` to the servers URI, or by adding it to the start of our code.

```python
import mlflow
mlflow.set_tracking_uri("http://YOUR-SERVER:YOUR-PORT")
mlflow.set_experiment("my-experiment")
```

#### Logging artifacts

*Artifacts* can represent any kind of file to save during training, e.g. plots and model weights. It is possible to log such files as well and place them within the same run (via its URI. So everything created within our ML run can be saves at one point. Those files can be either single local files or full directories. The following example creates a local files and logs it to our previous model run.

```python
import os

mlflow.set_tracking_uri("http://127.0.0.1:5000/")

# Create an example file output/test.txt
file_path = "outputs/test.txt"
if not os.path.exists("outputs"):
    os.makedirs("outputs")
with open(file_path, "w") as f:
    f.write("hello world!")

# Start the run based on the run ID and log the artifact
# we just created
with mlflow.start_run(run_id=run_id) as run:
    mlflow.log_artifact(
        local_path=file_path,
        # store the artifact directly in run's root
        artifact_path=None
    )
    mlflow.log_artifact(
        local_path=file_path,
        # store the artifact in a specific directory
        artifact_path="data/subfolder"
    )

    # get and print the URI where the artifacts have been logged to
    artifact_uri = mlflow.get_artifact_uri()
    print(f"run_id: {run.info.run_id}")
    print(f"Artifact uri: {artifact_uri}")
```

#### Autolog

Previously, we logged all the parameters, metrics, and files automatically. The *autolog*-feature of MLflow allows for automatic logging of log metrics, parameters, and models without the need for explicit log statements. We need to active this feature previous to the run by calling `MLflow.sklearn.autolog()`.

```python
import mlflow.sklearn
import numpy as np
from sklearn.ensemble import RandomForestRegressor

params = {"n_estimators": 4, "random_state": 42}

mlflow.sklearn.autolog()

run_name = 'autologging model example'
with mlflow.start_run(run_name=run_name) as run:
    rfr = RandomForestRegressor(
        **params).fit(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]), [1, 1, 1])
    print(f"run_id: {run.info.run_id}")

mlflow.sklearn.autolog(disable=True)
```

Even though this is a very convenient feature, it is a good practice to log metrics manually as this gives you more control over your ML run.


### MLflow Models

MLflow Models manages and deploys models from various different ML libraries such as scikit-learn, TensorFlow, PyTorch, Spark, or [many more](https://MLflow.org/docs/latest/models.html). It includes a generic `MLmodel` format that acts as a standard format to package ML models so they can be used in different projects and environments. The stored model can then be served as a python functions in an easy and convenient way that are run either locally or or Docker containers or commercial serving platforms. 

The format defines a convention that lets you save a model in so called “flavors”, for example `mlflow.sklearn` allows to load mlflow models back into scikit-learn.
Flavors are a key concept of MLflow since a stored model can be used by different deployment tools. This allows to load a TensorFlow model as a TensorFlow DAG for example, or to deploy a Python model to a Docker-based REST server, or to cloud platforms such as Azure ML and AWS SageMaker. The following example is based on the scikit-learn library.


```python
# Import the sklearn models from MLflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor

mlflow.set_tracking_uri("http://127.0.0.1:5000/")

run_name = "models-example-run"
params = {"n_estimators": 4, "random_state": 42}

# Start an MLflow run, train the RandomForestRegressor example model, and
# log its parameeters. In the end the model itself is logged and stored in MLflow
run_name = 'Model example'
with mlflow.start_run(run_name=run_name) as run:
    rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1])
    mlflow.log_params(params)
    mlflow.sklearn.log_model(rfr, artifact_path="sklearn-model")

model_uri = "runs:/{}/sklearn-model".format(run.info.run_id)
model_name = f"RandomForestRegressionModel"

print(f"model_uri: {model_uri}")
print(f"model_name: {model_name}")
```

Once a model is stored in the correct format it can be identified by its `model_uri`, and loaded and used for prediction.

```python
import mlflow.pyfunc

# Load the model and use it for predictions
model = mlflow.pyfunc.load_model(model_uri=model_uri)
data = [[0, 1, 0]]
model_pred = model.predict(data)
print(f"model_pred: {model_pred}")
```

![MLflow Models](images/06-MLflow/MLflow_web_interface-models.png)

### MLflow Model Registry

The MLflow Model Registry provides a central model store to manage the lifecycle of an ML Model. This allows to register MLflow models, such as the *RandomForestRegressor* from the previous section, to the Model Registry and include model versioning, stage transitions, and annotations. In fact, by running `MLflow.sklearn.log_model` we already did exactly that. Look at how easy the MLflow API is to use. Let's have a look at the code again.

```python
import mlflow.sklearn
import mlflow.pyfunc
from sklearn.ensemble import RandomForestRegressor

mlflow.set_tracking_uri("http://127.0.0.1:5000/")

run_name = "registry-example-run"
params = {"n_estimators": 4,
          "random_state": 42}

run_name = 'model registry example'
with mlflow.start_run(run_name=run_name) as run:
    rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1])
    mlflow.log_params(params)
    # Log and store the model and the MLflow Model Registry
    mlflow.sklearn.log_model(rfr, artifact_path="sklearn-model")

model_uri = f"runs:/{run.info.run_id}/sklearn-model"
model_name = f"RandomForestRegressionModel"

model = mlflow.pyfunc.load_model(model_uri=model_uri)
data = [[0, 1, 0]]
model_pred = model.predict(data)
print(f"model_pred: {model_pred}")
```

Yet, it is also possible to register the MLflow model in the model registry by calling `MLflow.register_model` such as show in belows example.

```python
# The previously stated Model URI and name are needed to register a MLflow Model
mv = mlflow.register_model(model_uri, model_name)
print("Name: {}".format(mv.name))
print("Version: {}".format(mv.version))
print("Stage: {}".format(mv.current_stage))
```

Once registered to the model registry the model is already versioned. This allows to load a model based on its specific version. A registered model can be also modified to transition to another version or stage. Both use cases are shown in the example below.
```python
import mlflow.pyfunc

# Load model for prediction. Keep note that we now specified the model version.
model = mlflow.pyfunc.load_model(
    model_uri=f"models:/{model_name}/{mv.version}"
)

# Predict based on the loaded model
data = [[0, 1, 0]]
model_pred = model.predict(data)
print(f"model_pred: {model_pred}")
```

Let's stage a model to `'Staging'` and have a look what models we have registered. 

```python
# Transition the model to another stage
from mlflow.client import MLflowClient 

client = MlflowClient()

stage = 'Staging'  # None, Production

client.transition_model_version_stage(
    name=model_name,
    version=mv.version,
    stage=stage
)

# print registered models
for rm in client.search_registered_models():
    pprint(dict(rm), indent=4)
```


### MLflow Projects

MLflow Projects allows to package code and its dependencies as a *project* that can be run reproducible on other data. Each project includes a *MLproject* file written in the *YAML* syntax that defines the projects dependencies and the commands and arguments it takes to run the project. It basically is a convention to organizes and describe your model code so other data scientists or automated tools can run it. MLflow currently supports the following environments to run your code: Virtualenv, conda, Docker Container, and system environment. A very basic `MLproject` file is shown below.

```yaml
name: mlprojects_tutorial

# Use Virtualenv: alternatively conda_env, docker_env.image
python_env: <MLFLOW_PROJECT_DIRECTORY>/python_env.yaml 

entry_points:
  main:
    parameters:
      alpha: {type: float, default: 0.5}
      l1_ratio: {type: float, default: 0.1}
    command: "python wine_model.py {alpha} {l1_ratio}"

```

A project can be run using the  `MLflow run` command in the command line, which runs a project from either a local directory or a GitHub URI. The `MLproject` file shows that two parameters can be passed to the command. This is optional in this case as they have default values.It is also possible to specify extra parameters such as the experiment name or to specify the tracking uri (check the !(official documentation)[https://mlflow.org/docs/latest/python_api/mlflow.projects.html]for more). By setting the MLFLOW\_TRACKING\_URI environment variable it is possible to specify an execution backend for the run.

```python
# Run the MLflow project from the current directory
# The parameters are optional in this case as the MLproject file has defaults
mlflow run . -P alpha=5.0

# It is also possible to specify an experiment name or to specify the 
# Tracking_URI, e.g.
MLFLOW_TRACKING_URI=http://localhost:<PORT> mlflow run . --experiment-name="models-experiment"

# Run the MLflow project from a Github URI and use the localhost as backend
MLFLOW_TRACKING_URI=http://localhost:<PORT> MLflow run https://github.com/seblum/mlops-practice#files/06-MLFlow/mlprojects --version=chapter/mlflow


```

The MLflow Projects API allows to chain projects together into workflows and also supports launching multiple runs in parallel. Combining this with for example the MLflow Tracking API enables an easy way of hyperparameter tuning to develop a model with a good fit.


## MLFflow Architecture

While MLflow can be run locally for your personal model implementation, it is usually deployed on a distributed architecture for large organizations or teams. The MLflow backend consists of three different main components, tracking server, backend store, and artifact store, all of which can reside on remote hosts.

The MLflow client can interface with a variety of backend and artifact storage configurations. The official [MLflow documentation](https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded) outlines several detailed configurations. The example below depicts the main interaction between the different architectural components of a remote MLflow Tracking Server, a Postgres database for backend entity storage, and an S3 bucket for artifact storage.

![MLflow Architecture Diagram](images/06-MLflow/MLflow_architecture_diagram.png)


### MLflow Tracking Server

The MLflow *Tracking Server* is the main component that handles the communication between the REST API to log parameters, metrics, experiments and metadata to a storage solution. Further, the Tracking UI allows to view runs on a web interface. Running the CLI command `mlflow ui` starts a web server on your local machine serving the MLFlow UI. Alternatively, the [MLflow tracking server](https://mlflow.org/docs/latest/tracking.html#tracking-server) serves the same UI which can be accessed using the URL `http://<TRACKING-SERVER-IP-ADDRESS>:5000` from any machine that cann connect to the tracking server.

Although it is possible to track parameters without running a server, it is recommended to create a MLflow tracking server to log your data to. Some of the functionality of the API is also available via the web interface, for example to create an experiment. The *Tracking Server* uses both, the backend store and the artifact store to store and read data from. 

### MLflow Backend Store

The MLflow *Backend Store* is where MLflow stores experiment and run metadata like parameters, metrics, and experiments of the runs. It is usually a relational database which means that all metadata will be stored but no large data files.

MLflow supports two types of backend stores: *file store* and *database-backed store*. By default, the backend store is set to the local file store backend at the `./mlruns` directory. A database-backed store must be configures using the `--backend-store-uri`. MLflow supports encoded Databases like *mysql*, *mssql*, *sqlite*, and *postgresql*. To be able to use the *MLflow Model Registry* the server must use a database-backed store.

It is possible to use a variety of externally hosted metadata stores to use as the *Backend Store*, for example MySQL, or AWS RDS


### MLflow Artifact Store

The MLflow *Artifact Store* is a location to store large data of an ML run that are not suitable for a relational database. This is where MLflow users log their artifact output, or data and image files to. In addition to the Backend store it is another storage place for the MLflow tracking server.  A user can access artifacts via HTTP requests to the MLflow Tracking Server. 

The location to the server’s artifact store defaults to local `./mlruns` directory. It is possible to specify another artifact store server using `--default-artifact-root`. The MLflow client caches the artifact location information on a per-run basis. It is therefore not recommended to alter a run’s artifact location before it has terminated.

The *Artifact Store* needs to be configured when running MLflow on a distributed system. In addition to local file paths, MLflow supports to configure the following cloud storage resources as an artifact stores: Amazon S3, Azure Blob Storage, Google Cloud Storage, SFTP server, and NFS.








# TODO

+ [x] set prerequisites
+ [x] insert images
+ [x] insert code
+ [x] run code
+ [x] write architecture
+ [ ] architectural diagram
+ [ ] proof read
+ [x] https://MLflow.org/docs/latest/concepts.html#the-machine-learning-workflow workflow into mlops intro
+ [x] resolve todos
+ [x] check order of MLflow components
+ [x] add MLflow projects example
+ [x] Check Tutorial
+ [x] check that intro is shorter than
+ [x] Test MLProjects yourself
+ [x] Build example project as code
=======
# Airflow
=======
# MLFlow
>>>>>>> 0db352b (initial notes)
=======
# MLflow
>>>>>>> cd2e73c (added final code)

MLflow is an open source platform to manage the machine learning lifecycle end-to-end. This includes experimentation, reproducibility, deployment, and registration of a ML model. MLflow provides four primary components to manage the ML lifecycle. They can be either used on their own or they also to work together.

* **MLflow Tracking** is used to log and compare parameters, code versions, metrics, and artifacts of an ML code. Results can be stored to local files or to remote servers and allow to compare multiple runs.
* **MLflow Projects** packages data science code in a standard format for a reusable, and reproducible form to share your code with other data scientists or transfer it to production. Each project is basically a directory with code or a Git repository, and uses a descriptor file to specify its dependencies, for example a `conda.yaml`. This allows to run existing MLflow Projects from a Git repository, such as GitHub. It is also possible to chain Projects into multi-step workflows.
* **MLflow Models** enables to manage and deploy machine learning models from multiple libraries. It allows to package your own ML model for later use in downstream tasks, e.g. real-time serving through a REST API. The package format defines a convention that saves the model in different *“flavors”* that can be interpreted by different downstream tools. This allows to load a TensorFlow model as a TensorFlow DAG for example, or to deploy a Python model to a Docker-based REST server, or to cloud platforms such as Azure ML and AWS SageMaker
=======
>>>>>>> 1c78f53 (draft for mlprojects)
* **MLflow Registry** provides a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations. It comes with an API and UI for easy use of such funtionalities and each of those aspects can be checked in MLflows' web interface.
* **MLflow Projects** packages data science code in a standard format for a reusable, and reproducible form to share your code with other data scientists or transfer it to production. Each project is basically a local directory or a Git repository, and uses a descriptor file to specify its dependencies and entrypoints. An existing MLflow Project can be run also either locally or from a Git repository.

MLflow is library-agnostic, which means one can use it with any ML library and  programming language. All functions are accessible through a [REST API](https://MLflow.org/docs/latest/rest-api.html#rest-api) and [CLI](https://MLflow.org/docs/latest/cli.html#cli), and project comes with a Python API, R API, and Java API already included. It requires minimal changes to integrate MLflow into an existing codebase and aims to make it reproducible and reusable as easy as possible so other Data Scientists can reuse the code. It is even possible to define your own plugins (TODO: True? example)

MLflow comes with a web interface to conveniently view and compare models and metrics.

![Web Interface of MLflow](images/06-MLflow/MLflow_web_interface-overview.png)


## Prerequisites

To go through this chapter it is necessary to have python and MLflow installed. One can install MLflow locally via `pip install MLflow`. The tutorial is based on MLflow version (TODO:Insert version). It is also recommended to have knowledge of VirtualEnv, Conda, or Docker when working with MLflow Projects.


## Functionality

In the following the four primary components of MLflow are shown in more detail and with exemplary code.


### MLflow Tracking

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
+ Create a docker image for the MLFlow tracking server.
+ Deploy Postgresql database on Kubernetes.
    + Helm to deploy PostgreSQL
+ Create YAML configurations for deployment, service and configmap to deploy the tracking server to Kubernetes.
    + The first thing we need to do is create the configmap and secrets for the tracking server.
    + 

<<<<<<< HEAD
<<<<<<< HEAD
A workflow describes here a set of steps to accomplish a given data engineering tasks, e.g. downloading files, copying data, filtering information, writing to a database, etc.
Workflows can be of varying levels of complexity and in general it is a term with various meaning depending on context.

Airflow can also be referred to as “Workflows as code”. It serves several purposes:
+ dynamic
+ extensible
+ flexible
+ 

Airflow can implement programs from any language, but workflows are written in Python.
They can be assessed via code, command-line, or via web interface.

TODO: Inser image of web-ui



runnings tasks/DAGs

Web interface to track everything

python language

## DAGs

Workflows are implemented as DAGs: Directed Acyclic Graphs.

* It is Directed, because there is an inherent flow representing dependencies between components.
* It is Acyclic, because it does not loop / cycle / repeat.
* Graph describes the actual set of components.

DAGs are also seen in Airflow, Apache Spark, Luigi

```python
from airflow.models import DAG
default_args = {
    'start_data':'2023-01-01'
}

example_dag = DAG(
    dag_id='etl_pipeline',
    default_args=default_args
)
```

a workflow can be run via the cli using 

```bash
airflow run <dag_id> <task_id> <start_date>
```


### Operators

Operators represent a single task in a workflow. They run independently (usually) and generally do not share any information. There are various operators to perform different tasks.

Gotchas: They are not guaranteed to run in the same location/environment. May require extensive use of Environment variables. Can be difficult to run tasks with elevated privileges.


BashOperator - expects a bash_command

PythonOperator - expects a python_callable

BranchPython - requires a python_callable and provide_context=True. The callable must accept `**kwargs`

EmailOperator

The EmailOperator does require the Airflow system to be configured with email server details.

```python
from airflow.operators.email_operator import EmailOperator

email_task = EmailOperator(
    task_id='email_sales_report',
    to='sales_manager@example.com',
    subject='automated Sales Report',
    html_content='Attached is the latest sales report',
    files='latest_sales.xlsx',
    dag=example_dag
)
```

###  Tasks

Tasks are Instances of operators. They are usually assigned to a variable on Python.
They are referred to by the task_id within the airflow tools.

```python
example_task = BashOperator(task_id='bash_example',
                           bash_command='echo "Example!"',
                           dag=example_dag)
```

It is possible to define a give order of task completion, meaning task dependencies. They are either referred to as upstream or downstream tasks.
In Airflow 1.8 and later, they are defined using the bitshift operators:
* >>, or the upstream operator (before)
* <<, or the downstream operator (after)

```python
# chained dependencies
task_1 >> task_2 >> task_3

# mixed dependencies
task_1 >> task_2 << task_3

task_1 >> task_2
task_3 >> task_2
```

*arguments*

supports arguments to tasks 
* Positional
* Keyword

Use the `op_kwargs` dictionary

```python
def sleep(length_of_time):
    time.sleep(lenght_of_time)
    
sleep_task = PythonOperator(
    task_id='sleep',
    python_callable=sleep,
    op_kwargs={'length_of_time':5},
    dag=example_dag
)

```
### Scheduling

DAG Runs
A DAG run is a specific instance of a workflow at a point in time. It can be run manually or via schedule_interval. Maintain state for each workflow and the tasks within:
* running
* failed
* success

When scheduling a DAG, there are severyl attributes to note:
* start_date - the date/time to initially schedule the DAG run
* end_date - optional attribute for when to stop running new DAG instances
* max_tries - optional attribute for how many attempts to make
* schedule_interval - how often to run


### Sensors

A Sensor is an operator that waits for a certain condition to be true, e.g.:
* creation of a file (existence of a file FileSensor)
* upload of a database record
* certain response from a web request

It can define how often to check for the condition to be true
Sensors are assigned to tasks.

Sensors have different arguments:
mode:  how to check for a condition
`mode='poke'` the default, run repeatedly
`mode='reschedule'` give up task slot and try again later
poke_interval: how often to wait between checks
timeout: how long to wait before failing task

```python
from airflow.sensors.base_sensor_operator
```

```python
from airflow.contrib.sensors.file_sensor import FileSensor

file_sensor_task = FileSensor(task='file_sense',
                             filepath='salesdata.csv',
                             poke_intervall=300,
                             dag=sales_report_dag
                             )
init_sales_cleanup >> file_sensor_task >> generate_report
```

Other sensors:
* ExternalTaskSensor - wait for a task in another DAG to complete
* HttpSensor - Request a web URL and check for content
* SqlSensor - Runs a SQL query to check for content
* Many other in `aorflow.sensors` and `airflow.contrib.sensors`
* 
### Executor

What is an executor? 
Executors run tasks. Different executors handle running the tasks differently, e.g.
+ SequentialExecutor
+ LocalExecutor
+ CeleryExecutor
+ 

## Templates

allow substituting DAG information
>>>>>>> 5b60218 (initial commit)
=======
## MLFlow Usage
>>>>>>> 0db352b (initial notes)
=======
=======
>>>>>>> b190388 (copy pasted from mlflow website)
## MLFlow Usage
=======
## Fundamental usage
>>>>>>> 79bcae0 (added subsections)
=======
MLFlow Tracking allows to save and compare experiment records and results. The following code snippet shows the basic concepts of it. 
>>>>>>> 2519db1 (added code and copied webpage)
=======
MLFlow Tracking allows to log and compare parameters, code versions, metrics, and artifacts. This can be easily done by minimal changes to your code using the MLFlow Tracking API. The following examples depict the basic concepts and show how to use it.
>>>>>>> 82d3105 (coherent text up until mlflow projects)
=======
MLflow Tracking allows to log and compare parameters, code versions, metrics, and artifacts. This can be easily done by minimal changes to your code using the MLflow Tracking API. The following examples depict the basic concepts and show how to use it. To use MLflow within our code it needs to be imported first.
```python
import   MLflow
```
>>>>>>> cd2e73c (added final code)

#### MLflow experiment

MLflow experiments are a part of MLflow’s tracking component that allow to group runs together based on custom criteria. For example we might create a new experiment for each new model architecture we want to evaluate.

```python
experiment_name = "introduction-experiment"
MLflow.set_experiment(experiment_name)
```

TODO set experiment, create experiment


#### MLflow run

An MLflow run is an execution environment for a piece of machine learning code. Whenever we want to track parameters or performance of a ML run or experiment we need to create a new MLflow. This can be done using `MLflow.start_run()`. Using `MLflow.end_run()` we can similarly end the run. It is a good practice to pass a run name to the MLflow run to identify it easily afterwards.

```python
run_name = "example-run"

MLflow.start_run()
run = MLflow.active_run()
print(f"Active run_id: {run.info.run_id}")
MLflow.end_run()
```

This will start and end a MLflow run. It is also possible to use the context manager as shown below, which allows for a smoother style.

```python
run_name = "context-manager-run"

with MLflow.start_run(run_name=run_name) as run:
    run_id = run.info.run_id
    print(f"Active run_id: {run_id}")
```

**Child runs**
It is possible to create child runs of the current run, based on the run ID. This can be used for example to gain a better overview of multiple run. Belows code depicts and example on how to create a child run.

```python
# Create child runs based on the run ID
with MLflow.start_run(run_id=run_id) as parent_run:
    print("parent run_id: {}".format(parent_run.info.run_id))
    with MLflow.start_run(nested=True, run_name="test_dataset_abc.csv") as child_run:
        MLflow.log_metric("acc", 0.91)
        print("child run_id : {}".format(child_run.info.run_id))

with MLflow.start_run(run_id=run_id) as parent_run:
    print("parent run_id: {}".format(parent_run.info.run_id))
    with MLflow.start_run(nested=True, run_name="test_dataset_xyz.csv") as child_run:
        MLflow.log_metric("acc", 0.90)
        print("child run_id : {}".format(child_run.info.run_id))
```

#### Logging metrics & parameters

The main reason we use MLflow Tracking is to log and store parameters and metrics during our MLflow run. *Parameters* represent the input parameters used for training, e.g. the initial learning rate. *Metrics:* are used to track the progress of the model training and are usually updated over the course of a run. MLflow allows to keep track of the model’s train and validation losses and visualize their development across the training run. Parameters and metrics can be easily logged by calling `MLflow.log_param`, or `MLflow.log_metric`. We can also specify a tag to identify our run using `MLflow.set_tag`. Belows example show how to use each method within a run.

```python
run_name = "tracking-example-run"
experiment_name = "tracking-experiment"
MLflow.set_experiment(experiment_name)

with MLflow.start_run(run_name=run_name) as run:

    # Parameters
    MLflow.log_param("learning_rate", 0.01)
    MLflow.log_params({"epochs": 0.05, "final_activation": "sigmoid"})

    # Tags
    MLflow.set_tag("env", "dev")
    MLflow.set_tags({"some_tag": False, "project": "xyz"})

    # Metrics
    MLflow.log_metric("loss", 0.001)
    MLflow.log_metrics({"acc": 0.92, "auc": 0.90})

    # It is possible to log a metrics series (for example a training history)
    for val_loss in [0.1, 0.01, 0.001, 0.00001]:
        MLflow.log_metric("val_loss", val_loss)

    for val_acc in [0.6, 0.6, 0.8, 0.9]:
        MLflow.log_metric("val_acc", val_acc)

    run_id = run.info.run_id
    experiment_id = run.info.experiment_id
    print(f"run_id: {run_id}")
    print(f"experiment_id: {experiment_id}")
```

It is also possible to add information after the experiment ran. We just need to specifiy our run ID from the previous run for our new run.

```python
from MLflow.tracking import MLflowClient

# add a note to the experiment
MLflowClient().set_experiment_tag(
    experiment_id, "MLflow.note.content", "my experiment note")
# add a note to the run
MLflowClient().set_tag(run_id, "MLflow.note.content", "my run note")

# Or we can even log further metrics by calling MLflow.start_run on a specific ID
with MLflow.start_run(run_id=run_id):
    run = MLflow.active_run()
    MLflow.log_metric("f1", 0.9)
    print(f"run_id: {run.info.run_id}")
```

#### Display & View metrics

How can we use the logged parameters and metrics afterwards? It is possible to give an overview of the currently stored runs using the MLflow API and printing the results. (TODO: set print?)

```python
current_experiment = dict(MLflow.get_experiment_by_name(experiment_name))
MLflow_run = MLflow.search_runs([current_experiment['experiment_id']])
print(f"MLflow_run: {MLflow_run}")
```

Yet, viewing all the results in the web interface of MLflow gives a much better overview. By default, the tracking API writes the data to the local filesystem of the machine it’s running on under a `./mlruns` directory. We can acces the MLflow’s Tracking UI by running `MLflow ui` via our console, and view it in our browser under http://localhost:5000 (The port: 5000 is the MLflow default). The metrics dashboard of a run looks like to following

![MLflow Model Tracking Dashboard](images/06-MLflow/MLflow_web_interface-tracking.png)

It is also possible to configure MLflow to log to a remote tracking server, to manage results centrally or share them across a team. To get access to a remote tracking server it is needed to set a MLflow tracking URI. This can be done either by setting an environment variable `MLflow_TRACKING_URI` to the servers URI, or by adding it to the start of our code.

```python
import MLflow
MLflow.set_tracking_uri("http://YOUR-SERVER:YOUR-PORT")
MLflow.set_experiment("my-experiment")
```

#### Logging artifacts

*Artifacts* can represent any kind of file to save during training, e.g. plots and model weights. It is possible to log such files as well and place them within the same run (via its URI. So everything created within our ML run can be saves at one point. Those files can be either single local files or full directories. The following example creates a local files and logs it to our previous model run.

```python
import os

MLflow.set_tracking_uri("http://127.0.0.1:5000/")

# Create an example file output/test.txt
file_path = "outputs/test.txt"
if not os.path.exists("outputs"):
    os.makedirs("outputs")
with open(file_path, "w") as f:
    f.write("hello world!")

# Start the run based on the run ID and log the artifact
# we just created
with MLflow.start_run(run_id=run_id) as run:
    MLflow.log_artifact(
        local_path=file_path,
        # store the artifact directly in run's root
        artifact_path=None
    )
    MLflow.log_artifact(
        local_path=file_path,
        # store the artifact in a specific directory
        artifact_path="data/subfolder"
    )

    # get and print the URI where the artifacts have been logged to
    artifact_uri = MLflow.get_artifact_uri()
    print(f"run_id: {run.info.run_id}")
    print(f"Artifact uri: {artifact_uri}")
```

#### Autolog

Previously, we logged all the parameters, metrics, and files automatically. The *autolog*-feature of MLflow allows for automatic logging of log metrics, parameters, and models without the need for explicit log statements. We need to active this feature previous to the run by calling `MLflow.sklearn.autolog()`.

```python
import MLflow.sklearn
import numpy as np
from sklearn.ensemble import RandomForestRegressor

params = {"n_estimators": 4, "random_state": 42}

MLflow.sklearn.autolog()

run_name = 'autologging model example'
with MLflow.start_run(run_name=run_name) as run:
    rfr = RandomForestRegressor(
        **params).fit(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]), [1, 1, 1])
    print(f"run_id: {run.info.run_id}")

MLflow.sklearn.autolog(disable=True)
```

Even though this is a very convenient feature, it is a good practice to log metrics manually as this gives you more control over your ML run.

TODO: Autologging stores already model

### MLflow Models

MLflow Models manages and deploys models from various different ML libraries such as scikit-learn, TensorFlow, PyTorch, Spark, or [many more](https://MLflow.org/docs/latest/models.html). It includes a generic `MLmodel` format that acts as a standard format to package ML models so they can be used in different projects and environments. The stored model can then be served as a python functions in an easy and convenient way that are run either locally or or Docker containers or commercial serving platforms. The following example is based on the scikit-learn library.

TODO: insert flavors
This allows to load a TensorFlow model as a TensorFlow DAG for example, or to deploy a Python model to a Docker-based REST server, or to cloud platforms such as Azure ML and AWS SageMaker


```python
# Import the sklearn models from MLflow
import MLflow.sklearn
from sklearn.ensemble import RandomForestRegressor

MLflow.set_tracking_uri("http://127.0.0.1:5000/")

run_name = "models-example-run"
params = {"n_estimators": 4, "random_state": 42}

# Start an MLflow run, train the RandomForestRegressor example model, and
# log its parameeters. In the end the model itself is logged and stored in MLflow
run_name = 'Model example'
with MLflow.start_run(run_name=run_name) as run:
    rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1])
    MLflow.log_params(params)
    MLflow.sklearn.log_model(rfr, artifact_path="sklearn-model")

model_uri = "runs:/{}/sklearn-model".format(run.info.run_id)
model_name = f"RandomForestRegressionModel"

print(f"model_uri: {model_uri}")
print(f"model_name: {model_name}")
```

Once a model is stored in the correct format it can be loaded and used for prediction. This example runs the stored model in a local setup (TODO: Check whether True).

```python
import MLflow.pyfunc

# Load the model and use it for predictions
model = MLflow.pyfunc.load_model(model_uri=model_uri)
data = [[0, 1, 0]]
model_pred = model.predict(data)
print(f"model_pred: {model_pred}")
```

![MLflow Models](images/06-MLflow/MLflow_web_interface-models.png)

### MLflow Model Registry

The MLflow Model Registry provides a central model store to manage the lifecycle of an ML Model. This allows to register MLflow models, such as the *RandomForestRegressor* from the previous section, to the Model Registry and include model versioning, stage transitions, and annotations. In fact, by running `MLflow.sklearn.log_model` we already did exactly that. Look at how easy the MLflow API is to use. Let's have a look at the code again.

```python
import MLflow.sklearn
import MLflow.pyfunc
from sklearn.ensemble import RandomForestRegressor

MLflow.set_tracking_uri("http://127.0.0.1:5000/")

run_name = "registry-example-run"
params = {"n_estimators": 4,
          "random_state": 42}

run_name = 'model registry example'
with MLflow.start_run(run_name=run_name) as run:
    rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1])
    MLflow.log_params(params)
    # Log and store the model and the MLflow Model Registry
    MLflow.sklearn.log_model(rfr, artifact_path="sklearn-model")

model_uri = f"runs:/{run.info.run_id}/sklearn-model"
model_name = f"RandomForestRegressionModel"

model = MLflow.pyfunc.load_model(model_uri=model_uri)
data = [[0, 1, 0]]
model_pred = model.predict(data)
print(f"model_pred: {model_pred}")
```

Yet, it is also possible to register the MLflow model in the model registry by calling `MLflow.register_model` such as show in belows example.

```python
# The previously stated Model URI and name are needed to register a MLflow Model
mv = MLflow.register_model(model_uri, model_name)
print("Name: {}".format(mv.name))
print("Version: {}".format(mv.version))
print("Stage: {}".format(mv.current_stage))
```

Once registered to the model registry the model is already versioned. This allows to load a model based on its specific version. A registered model can be also modified to transition to another version or stage. Both use cases are shown in the example below.
```python
import MLflow.pyfunc

# Load model for prediction. Keep note that we now specified the model version.
model = MLflow.pyfunc.load_model(
    model_uri=f"models:/{model_name}/{mv.version}"
)

# Predict based on the loaded model
data = [[0, 1, 0]]
model_pred = model.predict(data)
print(f"model_pred: {model_pred}")
```

Let's stage a model to `'Staging'` and have a look what models we have registered (TODO: Woher kommt client?)
```python
# Transition the model to another stage
client = MLflowClient()

stage = 'Staging'  # None, Production

client.transition_model_version_stage(
    name=model_name,
    version=mv.version,
    stage=stage
)

# print registered models
for rm in client.search_registered_models():
    pprint(dict(rm), indent=4)
```


### MLflow Projects

MLflow Projects allows to package code and its dependencies as a *project* that can be run reproducible on other data. Each project includes a *MLproject* file written in the *YAML* syntax that defines the projects dependencies and the commands and arguments it takes to run the project. It basically is a convention to organizes and describe your model code so other data scientists or automated tools can run it. MLflow currently supports the following environments to run your code: Virtualenv, conda, Docker Container, and system environment. A very basic `MLproject` file is shown below

```yaml
name: My_RandomForestRegressionModel

# Use Virtualenv: alternatively conda_env, docker_env.image
python_env: <MLFLOW_PROJECT_DIRECTORY>/python_env.yaml 

entry_points: 
   main: 
   parameters: 
   data_file: path 
   regularization: {type: float, default: 0.1} 
   command: "python train.py -r {regularization} {data_file}" 
   validate: 
   parameters: 
   data_file: path 
   command: "python validate.py {data_file}"
```

A project can be run using the  `MLflow run` command in the command line, which runs a project from either a local directory or a GitHub URI:

```python
# Run the MLflow project from a local directory
MLflow run My_RandomForestRegressionModel -P alpha=5.0

# Run the MLflow project from a Github URI
MLflow run https://github.com/MLflow/MLflow-example.git -P alpha=5.0
```

The MLflow Projects API allows to chain projects together into workflows and also supports launching multiple runs in parallel. Combining this with for example the MLflow Tracking API enables an easy way of hyperparameter tuning to develop a model with a good fit.


## MLFflow Architecture

While MLflow can be run locally for your personal model implementation, it is usually deployed on a bigger network for large organizations or teams. MLflow can be launched on a distributed infrastructure of your choice to scale its use The MLflow backend consists of three different main components, all of which can be deployed in a cloud infrastructure.

### MLflow Tracking Server

The MLflow *Tracking Server* exposes the API to log parameters, metrics, experiments and metadata. It also runs the MLflow Web Interface to visualize the results. The *Tracking Server* uses both, the backend store and the artifact store to store and read data from.

### MLflow Backend Store

<<<<<<< HEAD
<<<<<<< HEAD
### Artifact Store
<<<<<<< HEAD
>>>>>>> 230d939 (added initial structure)
=======
=======
The MLFlow *Backend Store* is where MLflow stores experiment and run metadata like parameters, metrics, and experiments of the runs. 
>>>>>>> 6af4b40 (added code files)
=======
The MLflow *Backend Store* is where MLflow stores experiment and run metadata like parameters, metrics, and experiments of the runs. 
>>>>>>> cd2e73c (added final code)

It is possible to use a variety of metadata stores to use as the *Backend Store*, for example MySQL, or AWS RDS

### MLflow Artifact Store

The MLflow *Artifact Store* is a location to store large data if an ML run. This is where MLflow users log their artifact output, e.g. models, or data files.

MLflow stores these data in the current (Todo: where does it store) when used locally. It is also possible to use cloud storage resources as an *Artifact Store*, such as AWS S3 Buckets.


<<<<<<< HEAD
+ Create a docker image for the MLFlow tracking server.
+ Deploy Postgresql database on Kubernetes.
    + Helm to deploy PostgreSQL
+ Create YAML configurations for deployment, service and configmap to deploy the tracking server to Kubernetes.
    + The first thing we need to do is create the configmap and secrets for the tracking server.
<<<<<<< HEAD
    + 
>>>>>>> b190388 (copy pasted from mlflow website)
=======
=======
>>>>>>> 6af4b40 (added code files)




# TODO
<<<<<<< HEAD
<<<<<<< HEAD
+ set prerequisites
+ insert images
+ insert code
+ run code
+ write architecture
+ https://mlflow.org/docs/latest/concepts.html#the-machine-learning-workflow workflow into mlops intro
<<<<<<< HEAD
>>>>>>> 2519db1 (added code and copied webpage)
=======
+ resolve todos
+ check order of mlflow components
+ add mlflow projects example
>>>>>>> 82d3105 (coherent text up until mlflow projects)
=======
+ [ ] set prerequisites
=======
+ [x] set prerequisites
>>>>>>> cd2e73c (added final code)
+ [x] insert images
+ [x] insert code
+ [x] run code
+ [ ] write architecture
+ [x] https://MLflow.org/docs/latest/concepts.html#the-machine-learning-workflow workflow into mlops intro
+ [ ] resolve todos
+ [x] check order of MLflow components
+ [ ] add MLflow projects example
+ [x] Check Tutorial
<<<<<<< HEAD
>>>>>>> 3670df9 (renamed images and put into code)
=======
+ [ ] check that intro is shorter than
<<<<<<< HEAD
>>>>>>> cd2e73c (added final code)
=======
+ [ ] Test MLProjects yourself
+ [ ] Build example project as code
>>>>>>> 1c78f53 (draft for mlprojects)
