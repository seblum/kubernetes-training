# MLFlow

MLflow is an open source platform to manage the machine learning lifecycle end-to-end. 
MLflow provides four primary components to manage the ML lifecycle. They can be either used on their own or they also to work together.

* **MLflow Tracking** is used to log and compare parameters, code versions, metrics, and artifacts of an ML code. Results can be stored to local files or to remote servers and allow to compare multiple runs.
* **MLflow Projects** packages data science code in a standard format for a reusable, and reproducible form to share your code with other data scientists or transfer it to production. Each project is basically a directory with code or a Git repository, and uses a descriptor file to specify its dependencies, for example a `conda.yaml`. This allows to run existing MLflow Projects from a Git repository, such as GitHub. It is also possible to chain Projects into multi-step workflows.
* **MLflow Models** enables to manage and deploy machine learning models from multiple libraries. Each Model is saved as a directory containing also a descriptor file. This allows to load a TensorFlow model as a TensorFlow DAG for example, or to deploy a Python model to a Docker-based REST server, or to cloud platforms such as Azure ML and AWS SageMaker
* **MLflow Registry** provides a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations. It comes with an API for easy use of such funtionalities and each of those aspects can be checked in MLFlows' web interface.

MLflow is library-agnostic, which means one can use it with any ML library and  programming language. All functions are accessible through a [REST API](https://mlflow.org/docs/latest/rest-api.html#rest-api) and [CLI](https://mlflow.org/docs/latest/cli.html#cli), and project comes with a Python API, R API, and Java API already included. It requires minimal changes to integrate MLFlow into an existing codebase and aims to make it reproducible and reusable as easy as possible so other Data Scientists can reuse the code. It is even possible to define your own plugins (TODO: True? example)

MLFlow comes with a web interface to conveniently view and compare models and metrics.

![Web Interface of MLFlow](images/mlflow_experiments-overview.png)


## Prerequisites

To go through this chapter it is necessary to have python and mlflow installed. One can install MLFlow locally via `pip install mlflow`. 


## Functionality

In the following the four primary components of MLFlow are shown in more detail and with exemplary code.


### MLFlow Tracking

MLFlow Tracking allows to log and compare parameters, code versions, metrics, and artifacts. This can be easily done by minimal changes to your code using the MLFlow Tracking API. The following examples depict the basic concepts and show how to use it.

#### MLFlow run

Whenever we want to track parameters or performance of a ML run we need to create a new MLflow. This can be done using `mlflow.start_run()`. Using `mlflow.end_run()` we can similarly end the run. It is a good practice to pass a run name to the MLFlow run to identify it easily afterwards.

```python
import mlflow

run_name="example-run"

mlflow.start_run()
run = mlflow.active_run()
print(f"Active run_id: {run.info.run_id}")
mlflow.end_run()

```

This will start and end a MLFlow run. It is also possible to use the context manager as shown below, which allows for a smoother style.

```python
import mlflow

with mlflow.start_run(run_name=run_name) as run:
    print(f"Active run_id: {run.info.run_id}")
```

**Child runs**
It is possible to create child runs of the current run, based on the run ID. This can be used for example to gain a better overview of multiple run. Belows code depicts and example on how to create a child run.

```python
# Create child runs based on the run ID
with mlflow.start_run(run_id=run_id) as parent_run:
    print("parent run_id: {}".format(parent_run.info.run_id))
    with mlflow.start_run(nested=True, run_name="test_dataset_abc.csv") as child_run:
        mlflow.log_metric("acc", 0.91)
        print("child run_id : {}".format(child_run.info.run_id))

with mlflow.start_run(run_id=run_id) as parent_run:
    print("parent run_id: {}".format(parent_run.info.run_id))
    with mlflow.start_run(nested=True, run_name="test_dataset_xyz.csv") as child_run:
        mlflow.log_metric("acc", 0.90)
        print("child run_id : {}".format(child_run.info.run_id))
```

#### Logging metrics & parameters

The main reason we use MLFlow Tracking is to log and store metrics during our MLFlow run. Parameters and metrics can be easily logged by calling `mlflow.log_param`, or `mlflow.log_metric`. We can also specify a tag to identify our run using `mlflow.set_tag`. Belows example show how to use each method within a run.

```python
run_name="tracking-example-run"

with mlflow.start_run(run_name=run_name) as run:
    
    # Parameters
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_params({"epochs": 0.05, "final_activation": "sigmoid"})

    # Tags
    mlflow.set_tag("env", "dev")
    mlflow.set_tags({"some_tag": False, "project": "xyz"})

    # Metrics
    mlflow.log_metric("loss", 0.001)
    mlflow.log_metrics({"acc": 0.92, "auc": 0.90})

    # It is possible to log a metrics series (for example a training history)
    for val_loss in [0.1,0.01,0.001,0.00001]:
        mlflow.log_metric("val_loss", val_loss)
        
    for val_acc in [0.6,0.6,0.8,0.9]:
        mlflow.log_metric("val_acc", val_acc)
    
    run_id=run.info.run_id
    experiment_id=run.info.experiment_id
    print(f"run id: {run_id}")
    print(f"experiment id: {experiment_id}")
```

It is also possible to add information after the experiment ran. We just need to specifiy our run ID from the previous run for our new run.

```python
from mlflow.tracking import MlflowClient

# add a note to the experiment
MlflowClient().set_experiment_tag(experiment_id, "mlflow.note.content","my experiment note")
# add a note to the run
MlflowClient().set_tag(run_id, "mlflow.note.content","my run note")

# Or we can even log further metrics by calling mlflow.start_run on a specific ID
with mlflow.start_run(run_id=run_id):
    run = mlflow.active_run()
    mlflow.log_metric("f1", 0.9)
    print(run.info.run_id)
```

#### Display & View metrics

How can we use the logged parameters and metrics afterwards? It is possible to give an overview of the currently stored runs using the MLFlow API and printing the results. (TODO: set print?)

```python
current_experiment=dict(mlflow.get_experiment_by_name(experiment_name))
mlflow.search_runs([current_experiment['experiment_id']])
```

Yet, viewing all the results in the web interface of MLFlow gives a much better overview. By default, the tracking API writes the data to the local filesystem of the machine it’s running on under a `./mlruns` directory. We can acces the MLflow’s Tracking UI by running `mlflow ui` via our console, and view it in our browser under http://localhost:5000. The metrics dashboard of a run looks like to following

![MLFlow experiments dashboard](images/mlflow_experiments-overview.png)

It is also possible to configure MLflow to log to a remote tracking server, to manage results centrally or share them across a team. To get access to a remote tracking server it is needed to set a MLFlow tracking URI. This can be done either by setting an environment variable `MLFLOW_TRACKING_URI` to the servers URI, or by adding it to the start of our code.

```python
import mlflow
mlflow.set_tracking_uri("http://YOUR-SERVER:4040")
mlflow.set_experiment("my-experiment")
```

#### Logging artifacts

It is possible to log files as artifacts as well and place it within the runs URI. So everything created within our ML run can be saves at one point. Those files can be either single local files or full directories. The following example creates a local files and logs it to our previous model run.

```python
import os

# Create an example file output/test.txt
file_path="outputs/test.txt"
if not os.path.exists("outputs"):
    os.makedirs("outputs")
with open(file_path, "w") as f:
    f.write("hello world!")

# Start the run based on the run ID and log the artifact 
# we just created    
with mlflow.start_run(run_id=run_id):
    mlflow.log_artifact(
        local_path=file_path,
        # store the artifact directly in run's root 
        artifact_path=None   
    ) 
    mlflow.log_artifact(
        local_path=file_path,
        # store the artifact in a specific directory
        artifact_path="data/subfolder"
    )

    # get and print the URI where the artifacts have been logged to
    artifact_uri = mlflow.get_artifact_uri()
    print("Artifact uri: {}".format(artifact_uri))
```

#### Autolog

Previously, we logged all the parameters, metrics, and files automatically. The *autolog*-feature of MLFLow allows for automatic logging of log metrics, parameters, and models without the need for explicit log statements. We need to active this feature previous to the run by calling `mlflow.sklearn.autolog()`.

```python
import mlflow.sklearn
import numpy as np
from sklearn.ensemble import RandomForestRegressor

params = {"n_estimators": 4, "random_state": 42}

mlflow.sklearn.autolog()

run_name = 'autologging model example'
with mlflow.start_run(run_name=run_name) as run:
   rfr = RandomForestRegressor(**params).fit(np.array([[0, 1, 0],[0, 1, 0],[0, 1, 0]]), [1,1,1])

mlflow.sklearn.autolog(disable=True)
```

Even though this is a very convenient feature, it is a good practice to log metrics manually as this gives you more control over your ML run.


### MLFlow Models

MLFlow Models manages and deploys models from various different ML libraries such as scikit-learn, TensorFlow, PyTorch, Spark, or [many more](https://mlflow.org/docs/latest/models.html). It includes a generic `MLmodel` format that acts as a standard format to package ML models so they can be used in different projects and environments. The stored model can then be served as a python functions in an easy and convenient way that are run either locally or or Docker containers or commercial serving platforms. The following example is based on the scikit-learn library.

```python
# Import the sklearn models from mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor

run_name="models-example-run"
params = {"n_estimators": 4, "random_state": 42}

# Start an MLFlow run, train the RandomForestRegressor example model, and 
# log its parameeters. In the end the model itself is logged and stored in MLFlow
run_name = 'logging model example'
with mlflow.start_run(run_name=run_name) as run:
   rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1])
   mlflow.log_params(params)
   mlflow.sklearn.log_model(rfr, artifact_path="sklearn-model")

model_uri = "runs:/{}/sklearn-model".format(run.info.run_id)
model_name = f"{namespace}-RandomForestRegressionModel"
print(model_uri)
print(model_name)
```

Once a model is stored in the correct format it can be loaded and used for prediction. This example runs the stored model in a local setup (TODO: Check whether True).

```python
import mlflow.pyfunc

# Load the model and use it for predictions
model = mlflow.pyfunc.load_model(model_uri=model_uri)
data = [[0, 1, 0]]
model.predict(data)
```


### MLFlow Model Registry

The MLFlow Model Registry provides a central model store to manage the lifecycle of an ML Model. This allows to register MLFlow models, such as the *RandomForestRegressor* from the previous section, to the Model Registry and include model versioning, stage transitions, and annotations. In fact, by running `mlflow.sklearn.log_model` we already did exactly that. Look at how easy the MLFlow API is to use. Let's have a look at the code again.

```python
import mlflow.sklearn
import mlflow.pyfunc
from sklearn.ensemble import RandomForestRegressor

run_name="registry-example-run"
params = {"n_estimators": 4, "random_state": 42}

run_name = 'logging model example'
with mlflow.start_run(run_name=run_name) as run:
   rfr = RandomForestRegressor(**params).fit([[0, 1, 0]], [1])
   mlflow.log_params(params)
   # Log and store the model and the MLFlow Model Registry
   mlflow.sklearn.log_model(rfr, artifact_path="sklearn-model")

model_uri = "runs:/{}/sklearn-model".format(run.info.run_id)
model_name = f"{namespace}-RandomForestRegressionModel"

model = mlflow.pyfunc.load_model(model_uri=model_uri)
data = [[0, 1, 0]]
model.predict(data)
```

Yet, it is also possible to register the MLFlow model in the model registry by calling `mlflow.register_model` such as show in belows example.

```python
# The previously stated Model URI and name are needed to register a MLFlow Model
mv = mlflow.register_model(model_uri, model_name)
print("Name: {}".format(mv.name))
print("Version: {}".format(mv.version))
print("Stage: {}".format(mv.current_stage))
```

Once registered to the model registry the model is already versioned. This allows to load a model based on its specific version. A registered model can be also modified to transition to another version or stage. Both use cases are shown in the example below.
```python
import mlflow.pyfunc

# Load model for prediction. Keep note that we now specified the model version.
model = mlflow.pyfunc.load_model(
   model_uri=f"models:/{model_name}/{mv.version}"
)

# Predict based on the loaded model
data = [[0, 1, 0]]
model.predict(data)
```

Let's stage a model to `'Staging'`. (TODO: Woher kommt client?)
```python
# Transition the model to another stage
client = MlflowClient()

stage = 'Staging' # None, Production

client.transition_model_version_stage(
    name=model_name,
    version=mv.version,
    stage=stage
)
```


### MLFlow Projects

MLflow allows you to package code and its dependencies as a _project_ that can be run in a reproducible fashion on other data. Each project includes its code and a `MLproject` file that defines its dependencies (for example, Python environment) as well as what commands can be run into the project and what arguments they take.

You can easily run existing projects with the `mlflow run` command, which runs a project from either a local directory or a GitHub URI:

```python
mlflow run sklearn_elasticnet_wine -P alpha=5.0

mlflow run https://github.com/mlflow/mlflow-example.git -P alpha=5.0
```

There’s a sample project in `tutorial`, including a `MLproject` file that specifies its dependencies. if you haven’t configured a [tracking server](https://mlflow.org/docs/latest/tracking.html#tracking-server), projects log their Tracking API data in the local `mlruns` directory so you can see these runs using `mlflow ui`.


## MLFflow Architecture

 Scalability and Big Data

Data is the key to obtaining good results in machine learning, so MLflow is designed to scale to large data sets, large output files (for example, models), and large numbers of experiments. Specifically, MLflow supports scaling in four dimensions:

* An individual MLflow run can execute on a distributed cluster, for example, using [Apache Spark](https://spark.apache.org/). You can launch runs on the distributed infrastructure of your choice and report results to a Tracking Server to compare them. MLflow includes a built-in API to launch runs on [Databricks](https://databricks.com/).
    
* MLflow supports launching multiple runs in parallel with different parameters, for example, for hyperparameter tuning. You can simply use the [Projects API](https://mlflow.org/docs/latest/projects.html#projects) to start multiple runs and the [Tracking API](https://mlflow.org/docs/latest/tracking.html#tracking) to track them.
    
* MLflow Projects can take input from, and write output to, distributed storage systems such as AWS S3 and [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html). MLflow can automatically download such files locally for projects that can only run on local files, or give the project a distributed storage URI if it supports that. This means that you can write projects that build large datasets, such as featurizing a 100 TB file.
    
* MLflow Model Registry offers large organizations a central hub to collaboratively manage a complete model lifecycle. Many data science teams within an organization develop hundreds of models, each model with its experiments, runs, versions, artifacts, and stage transitions. A central registry facilitates model discovery and model’s purpose across multiple teams in a large organization.


### MLFLow Server

runs web UI

### MLFlow Database

MySQL, RDS, Whatever?

### Artifact Store

storage system like S3?

### MLFlow Infrastructure


+ Create a docker image for the MLFlow tracking server.
+ Deploy Postgresql database on Kubernetes.
    + Helm to deploy PostgreSQL
+ Create YAML configurations for deployment, service and configmap to deploy the tracking server to Kubernetes.
    + The first thing we need to do is create the configmap and secrets for the tracking server.




# TODO
+ set prerequisites
+ insert images
+ insert code
+ run code
+ write architecture
+ https://mlflow.org/docs/latest/concepts.html#the-machine-learning-workflow workflow into mlops intro
+ resolve todos
+ check order of mlflow components
+ add mlflow projects example
