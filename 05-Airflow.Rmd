# Airflow

[Apache Airflow](https://github.com/apache/airflow) is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows. Airflow comes with a web interface to help you manage the state of your workflows. 

Airflow can be deployed deployable in many ways, starting from a single process on your own laptop to a distributed setup with multiple compute resources for the biggest workflows. The aim of this tutorial is to show you how to use Airflow from a developer / user perspective and is based on the local installation. Please refer to the prerequisits on what is needed to follow through.
A detailed description of what an Airflow deployment involves is shown in the chapter [Airflow Infrastructure](https://github.com/apache/airflow).

## Prerequisites

installation with pip?

## Introduction

**What is Airflow?**

Airflow is a platform to program workflows, including their creation, schedulung, and monitoring.

A workflow describes here a set of steps to accomplish a given data engineering tasks, e.g. downloading files, copying data, filtering information, writing to a database, etc.
Workflows can be of varying levels of complexity and in general it is a term with various meaning depending on context.

Airflow can also be referred to as “Workflows as code”. It serves several purposes:
+ dynamic
+ extensible
+ flexible
+ 

Airflow can implement programs from any language, but workflows are written in Python.
They can be assessed via code, command-line, or via web interface.

TODO: Inser image of web-ui



runnings tasks/DAGs

Web interface to track everything

python language

## DAGs

Workflows are implemented as DAGs: Directed Acyclic Graphs.

* It is Directed, because there is an inherent flow representing dependencies between components.
* It is Acyclic, because it does not loop / cycle / repeat.
* Graph describes the actual set of components.

DAGs are also seen in Airflow, Apache Spark, Luigi

```python
from airflow.models import DAG
default_args = {
    'start_data':'2023-01-01'
}

example_dag = DAG(
    dag_id='etl_pipeline',
    default_args=default_args
)
```

a workflow can be run via the cli using 

```bash
airflow run <dag_id> <task_id> <start_date>
```


### Operators

Operators represent a single task in a workflow. They run independently (usually) and generally do not share any information. There are various operators to perform different tasks.

Gotchas: They are not guaranteed to run in the same location/environment. May require extensive use of Environment variables. Can be difficult to run tasks with elevated privileges.


BashOperator - expects a bash_command

PythonOperator - expects a python_callable

BranchPython - requires a python_callable and provide_context=True. The callable must accept `**kwargs`

EmailOperator

The EmailOperator does require the Airflow system to be configured with email server details.

```python
from airflow.operators.email_operator import EmailOperator

email_task = EmailOperator(
    task_id='email_sales_report',
    to='sales_manager@example.com',
    subject='automated Sales Report',
    html_content='Attached is the latest sales report',
    files='latest_sales.xlsx',
    dag=example_dag
)
```

###  Tasks

Tasks are Instances of operators. They are usually assigned to a variable on Python.
They are referred to by the task_id within the airflow tools.

```python
example_task = BashOperator(task_id='bash_example',
                           bash_command='echo "Example!"',
                           dag=example_dag)
```

It is possible to define a give order of task completion, meaning task dependencies. They are either referred to as upstream or downstream tasks.
In Airflow 1.8 and later, they are defined using the bitshift operators:
* >>, or the upstream operator (before)
* <<, or the downstream operator (after)

```python
# chained dependencies
task_1 >> task_2 >> task_3

# mixed dependencies
task_1 >> task_2 << task_3

task_1 >> task_2
task_3 >> task_2
```

*arguments*

supports arguments to tasks 
* Positional
* Keyword

Use the `op_kwargs` dictionary

```python
def sleep(length_of_time):
    time.sleep(lenght_of_time)
    
sleep_task = PythonOperator(
    task_id='sleep',
    python_callable=sleep,
    op_kwargs={'length_of_time':5},
    dag=example_dag
)

```
### Scheduling

DAG Runs
A DAG run is a specific instance of a workflow at a point in time. It can be run manually or via schedule_interval. Maintain state for each workflow and the tasks within:
* running
* failed
* success

When scheduling a DAG, there are severyl attributes to note:
* start_date - the date/time to initially schedule the DAG run
* end_date - optional attribute for when to stop running new DAG instances
* max_tries - optional attribute for how many attempts to make
* schedule_interval - how often to run


### Sensors

A Sensor is an operator that waits for a certain condition to be true, e.g.:
* creation of a file (existence of a file FileSensor)
* upload of a database record
* certain response from a web request

It can define how often to check for the condition to be true
Sensors are assigned to tasks.

Sensors have different arguments:
mode:  how to check for a condition
`mode='poke'` the default, run repeatedly
`mode='reschedule'` give up task slot and try again later
poke_interval: how often to wait between checks
timeout: how long to wait before failing task

```python
from airflow.sensors.base_sensor_operator
```

```python
from airflow.contrib.sensors.file_sensor import FileSensor

file_sensor_task = FileSensor(task='file_sense',
                             filepath='salesdata.csv',
                             poke_intervall=300,
                             dag=sales_report_dag
                             )
init_sales_cleanup >> file_sensor_task >> generate_report
```

Other sensors:
* ExternalTaskSensor - wait for a task in another DAG to complete
* HttpSensor - Request a web URL and check for content
* SqlSensor - Runs a SQL query to check for content
* Many other in `aorflow.sensors` and `airflow.contrib.sensors`
* 
### Executor

What is an executor? 
Executors run tasks. Different executors handle running the tasks differently, e.g.
+ SequentialExecutor
+ LocalExecutor
+ CeleryExecutor
+ 

## Templates

allow substituting DAG information