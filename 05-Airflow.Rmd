# Airflow

[Apache Airflow](https://github.com/apache/airflow) is an open-source platform to develop, schedule and monitor workflows. Airflow comes with a web interface to interact with and manage the state of workflows. The web user interface aims to make managing workflows as easy as possible and provides a good overview of each workflow over time and the ability to inspect logs and manage tasks, for example retrying a task in case of failure.

![Airflow Web Interface](images/05-Airflow/web-interface-overview.png) 

However, the philosophy of Airflow is to define workflows as code so coding will always be required. Thus, Airflow can also be referred to as a “Workflows as code”-tool that allows for a dynamic, extensible, and flexible management of its workflows.

The Airflow platform contains different operators to connect with many other technologies to easily extend and connect with new technologies. Being able to manage a workflow for all stages of the training of ML models, and the possibility to combine Airflow with other tools e.g. for the tracking of ML models (MLflow), make Apache Airflow a create tool to incorporate in a MLOps architecture.

The aim of this chapter is to give a tutorial on how to use Airflow from a user perspective, as well as give a short overview on its deployment. Airflow can be deployed in multiple ways, starting from a single process on a local machine to a distributed setup with multiple compute resources for large workflows in a production setting. A detailed description of what an Airflow deployment involves is shown in the section Airflow Infrastructure (TODO: section link). This tutorial is based based on the local installation of Airflow. Please refer to the prerequisits on what is needed to follow through.

## Prerequisites

TODO: local installation 

## Basic usage concepts

Airflow serves as a batch-oriented workflow orchestration plattform. Workflows vary in their levels of complexity and in general it is a term with various meaning depending on context. When working with Airflow, a workflow usually describes a set of steps to accomplish a given data engineering tasks, e.g. downloading files, copying data, filtering information, writing to a database, etc.

Exemplary workflows or use cases might be for example to set up a ETL pipeline that extracts data from multiple sources, the transformation of the data, as well as loading them into a machine learning model. Even the training itself of a ML model can triggered via Airflow. Another workflow step might involve the generation of a report or backups. 

Even though Airflow can implement programs from any language, the workflows are written and defined as Python code. Airflow allows to access them via code, command-line, or via web interface. Writing workflows in code allows to store the in version control to ensure roll backs to previous versions as well as to develop a workflow with a team of developers simultaneously. It also allows to include further DevOps principles such as testing and validating the codes functionality.

### DAGs

A workflows in Airflow is implemented as a DAG, a *Directed Acyclic Graph*. A *graph* describes the actual set of components of a workflow. It is *directed* because it has an inherent flow representing dependencies between its components. It is *acyclic* as it does not loop or repeat.

An Airflow workflow is just a Python script that defines an Airflow DAG object as code. The DAG object is needed to nest the separate tasks of a workflow into. A workflow specified in code, e.g. python, is often also referred to as a *pipeline*. This terminology can be used synonymosly when working with Airflow. The following code-snipped depicts how to define a DAG object in code. The `dag_id` string is a unique identifier to a DAG. The `default_args` dictionary consists of additional parameters to be specified. The snippet below only shows two of them, there are a lot more though, which can also be seen here (TODO insert link)

```python
from airflow.models import DAG

# Using extra arguments allows a lot of customization, e.g.
# it makes the creation of a time zone aware DAG quite easy.
default_args = {
    'start_data':'2023-01-01',
    'schedule_interval':'None'
}

example_dag = DAG(
    dag_id='etl_pipeline',
    default_args=default_args
)
```

A workflow can be run either via the web interface or via the command line interface. The following command shows who to list the active DAGs from the directory.

```bash
# initialize the database tables
airflow db init

# print the list of active DAGs
airflow dags list
```

People sometimes think of the DAG definition file as a place where they can do some actual data processing - that is not the case at all! The script’s purpose is to define a DAG object. It needs to evaluate quickly (seconds, not minutes) since the scheduler will execute it periodically to reflect the changes if any. (rephrase - cause its copied)

### Operators

Operators represent a single task in a workflow and are basically a unit of work for Airflow to complete. Operators usually run independently and generally do not share any information. There are different categories of operators to perform different tasks, for example *Action operators*, *Transfer operators*, or *Sensors*. 
Action operators executes a basic task based on the operators specifications, for example the `BashOperator`, the `PythonOperator`, or `KubernetesPodOperator`. The operator names already suggest what kind of executions they provide. Transfer operators are designed to transfer data from one place to another, for example to copy data from one cloud bucket to another. Those operators are often stateful, which means the downloaded data is first stored locally and the uploaded to the destination storage. This different principle of execution defines them as an own category of operator. Finally, Sensors are a special subclass of operators that are triggered when an external event is happening. They are further outlined in the subsection (TODO reference to own section).

#### BashOperator 
expects a bash_command
```python
example_task = BashOperator(task_id='bash_example',
                           bash_command='echo "Example!"',
                           dag=example_dag)
```

#### PythonOperator
expects a python_callable
```python
from airflow.operators.python_operator import PythonOperator

def sleep(length_of_time):
    time.sleep(lenght_of_time)
    
sleep_task = PythonOperator(
    task_id='sleep',
    python_callable=sleep,
    op_kwargs={'length_of_time':5},
    dag=example_dag
)
```

#### BranchPython
requires a python_callable and provide_context=True. The callable must accept `**kwargs`
```python

```

#### EmailOperator

The `EmailOperator` allow to send predefined emails from an Airflow DAG run. This could be use for example to notify if a workflow was successfull or not. The `EmailOperator` does require the Airflow system to be configured with email server details.
```python
from airflow.operators.email_operator import EmailOperator

email_task = EmailOperator(
    task_id='email_sales_report',
    to='sales_manager@example.com',
    subject='automated Sales Report',
    html_content='Attached is the latest sales report',
    files='latest_sales.xlsx',
    dag=example_dag
)
```

###  Tasks

To use an operator in a DAG it needs to be instantiated as a task. Tasks determine how to execute an operator’s work within the context of a DAG. The concepts of a *Task* and *Operator* are actually somewhat interchangeable as each task is actually a subclass of Airflow’s `BaseOperator`. However, it is useful to think of them as separate concepts. Tasks are Instances of operators and are usually assigned to a variable on Python. The following code instantiates the `BashOperator` to two different variables `t1` and `t2`.

```python
t1 = BashOperator(
    task_id="print_date",
    bash_command="date",
)

t2 = BashOperator(
    task_id="sleep",
    depends_on_past=False,
    bash_command="sleep 5",
    retries=3,
)
```

Tasks can be referred to by their task_id either using the web interface or using the CLI within the airflow tools.

```bash
airflow run <dag_id> <task_id> <start_date>
```


#### Setting up Dependencies

It is possible to define a given order of task completion, meaning task dependencies. They are either referred to as upstream or downstream tasks.
In Airflow 1.8 and later, they are defined using the bitshift operators:
* >>, or the upstream operator (before)
* <<, or the downstream operator (after)

```python
t1.set_downstream(t2)

# chained dependencies
task_1 >> task_2 >> task_3

# mixed dependencies
task_1 >> task_2 << task_3

task_1 >> task_2
task_3 >> task_2
```
the actual tasks defined in a DAG run on different workers, which means that in the below script the cannot be used to cross communicate between tasks

*arguments*

supports arguments to tasks 
* Positional
* Keyword

Use the `op_kwargs` dictionary

```python
def sleep(length_of_time):
    time.sleep(lenght_of_time)
    
sleep_task = PythonOperator(
    task_id='sleep',
    python_callable=sleep,
    op_kwargs={'length_of_time':5},
    dag=example_dag
)

```

Try to understand Operators and Sensors as templates that make up a task when they are called in a DAG file.


### CLI commands

```bash
airflow run <dag_id> <task_id> <start_date>
```


```bash
# prints the list of tasks in the "tutorial" DAG
airflow tasks list tutorial

# prints the hierarchy of tasks in the "tutorial" DAG
airflow tasks list tutorial --tree
```

`airflow tasks test` runs task instances locally, outputs their log to stdout (on screen), does not bother with dependencies, and does not communicate state (running, success, failed, …) to the database. It simply allows testing a single task instance. Same goes with airflow dags test


```bash
# testing
# testing print_date
airflow tasks test tutorial print_date 2015-06-01

# testing sleep
airflow tasks test tutorial sleep 2015-06-01

airflow dags test
```



### Scheduling

DAG Runs
A DAG run is a specific instance of a workflow at a point in time. It can be run manually or via schedule_interval. Maintain state for each workflow and the tasks within:
* running
* failed
* success

When scheduling a DAG, there are severyl attributes to note:
* start_date - the date/time to initially schedule the DAG run
* end_date - optional attribute for when to stop running new DAG instances
* max_tries - optional attribute for how many attempts to make
* schedule_interval - how often to run


### Sensors

A Sensor is an operator that waits for a certain condition to be true, e.g.:
* creation of a file (existence of a file FileSensor)
* upload of a database record
* certain response from a web request

It can define how often to check for the condition to be true
Sensors are assigned to tasks.

Sensors have different arguments:
mode:  how to check for a condition
`mode='poke'` the default, run repeatedly
`mode='reschedule'` give up task slot and try again later
poke_interval: how often to wait between checks
timeout: how long to wait before failing task

```python
from airflow.sensors.base_sensor_operator
```

```python
from airflow.contrib.sensors.file_sensor import FileSensor

file_sensor_task = FileSensor(task='file_sense',
                             filepath='salesdata.csv',
                             poke_intervall=300,
                             dag=sales_report_dag
                             )
init_sales_cleanup >> file_sensor_task >> generate_report
```

Other sensors:
* ExternalTaskSensor - wait for a task in another DAG to complete
* HttpSensor - Request a web URL and check for content
* SqlSensor - Runs a SQL query to check for content
* Many other in `aorflow.sensors` and `airflow.contrib.sensors`
* 
### Executor

What is an executor? 
Executors run tasks. Different executors handle running the tasks differently, e.g.
+ SequentialExecutor
+ LocalExecutor
+ CeleryExecutor
+ * `LocalExecutor`—executes the tasks in separate processes on a single machine. It’s the only non-distributed executor which is production ready. It works well in relatively small deployments.
* `CeleryExecutor`—the most popular production executor, which uses under the hood the Celery queue system. When using this executor users can deploy multiple workers that read tasks from the broker queue (Redis or RabbitMQ) where tasks are sent by scheduler. This executor can be distributed between many machines and users can take advantage of queues that allow them to specify what task should be executed where. This is for example useful for routing compute-heavy tasks to more resourceful workers.
* `KubernetesExecutor`— is another widely used production-ready executor. As the name suggests it requires a Kubernetes cluster. When using this executor Airflow will spawn a new pod with an Airflow instance to run each task. This creates an additional overhead which can be problematic for short running tasks.
* `CeleryKubernetsExecutor`— the name says it all, this executor uses both CeleryExecutor and KubernetesExecutor. When users select this executor they can use a special `kubernetes` queue to specify that particular tasks have to be executed using KubernetesExecutor. Otherwise tasks are routed to celery workers. In this way users can take full advantage of horizontal auto scaling of worker pods and possibility of delegating longrunning / compute heavy tasks to `KubernetesExecutor`.
* `DebugExecutor`—this is a debug executor. Its main purpose is to debug DAG locally. It’s the only executor that uses a single process to execute all tasks. By doing so it’s simple to use it from IDE level as described in [docs](https://airflow.apache.org/docs/stable/executor/debug.html).
Executor is one of the crucial components of Airflow and it can be configured by the users. It defines where and how the Airflow tasks should be executed. The executor should be chosen to fit your needs and as it defines many aspects of how Airflow should be deployed.

### XCom

While hooks’ purpose is to implement communication layer with external services, the XCom purpose is to implement communication mechanism that allows information passing between tasks in DAG.

The fundamental part of XCom is the underlying metadatabase table (with same name) which works as a key-value storage. The key consists is a tuple (`dag_id`, `task_id`, `execution_date`, `key`) where the `key` attribute is a configurable string (by default it’s `return_value`). The stored value has to be json serializable and relatively small (max 48KB is allowed).

This means that the XCom purpose is to store metadata not the data. For example, if we have a dag with `task_a >> task_b` and a big data frame has to be passed from `task_a` to `task_b` then we have to store it somewhere in a persistent place (storage bucket, database etc) between those tasks. Then `task_a` should upload the data to storage and write to the XCom table an information where this data can be found, for example a uri to storage bucket or name of a temporary table. Once this information is in the XCom table, the `task_b` can access this value and retrieve the data from external storage.

In many cases this may sound like a lot of additional logic of uploading and downloading the data in operators. That’s true. But first of all, that’s where hooks came to the rescue — you can reuse logic for storing data in many different places. Second, there’s a possibility to specify a custom XCom backend. In this way users can simply write a class that will define how to serialize data before it’s stored in the XCom table and how to deserialize it when it’s retrieved from metadatabase. This, for example, allows users to automate the logic of persisting data frames as we described in this [article](https://turbaszek.medium.com/airflow-2-0-dag-authoring-redesigned-651edc397178).

### taskflow
writing data pipelines using the TaskFlow API paradigm
airflow 2.0


In this data pipeline, tasks are created based on Python functions using the `@task` decorator as shown below. The function name acts as a unique identifier for the task.
Main flow of the DAG
Now that we have the Extract, Transform, and Load tasks defined based on the Python functions, we can move to the main part of the DAG.

## Templates

allow substituting DAG information


## Deploying Airflow

Before Data Scientists and Machine Learning Engineers can utilize the power of Airflow Workflows, Airflow obviously needs to be set up and deployed. There are multiple ways an Airflow deployment can take place. It can be run either on a single machine or in a distributed setup on a cluster of machines. As stated in the prerequisites for this tutorial we set up Airflow locally on a single machine to introduce you on how to work with airflow. Although Airflow can be run on a single machine, it is beneficial to deploy it as a distributed system to utilize its full power.

### Airflow as a distributed system

Airflow consists of several separate parts. While this separation is somewhat simulated on a local deployment, each several part of Airflow can be deployed separately when deploying Airflow in a distributed manner. This comes with benefits of safety, security, and reliability (TODO check terms).

An Airflow deployment generally consists of five different compontens:

* **Scheduler:** The schedules handles triggering scheduled workflows and submitting tasks to the executor to run.
* **Executor:** The executor handles running the tasks themselves. In a local installation of Airflow, the tasks are run by the executor itself, whereas in a production ready deployment if Airflow the executor pushes the task execution to separate worker instances.
* **Webserver:** The webserver provides the user interface of Airflow we have seen before that allows to inspect, trigger, and debug DAGs and tasks.
* **DAG Directory:** The DAG directory is a directory that contains the DAG files which are read by the scheduler and executor.
* **Metadata Database:** The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, and user settings.

The following graphs shows how the components build up the Airflow architecture.

![airflow_architecture.png](./files/05-Airflow/airflow_architecture.png)

### Scheduler 

The scheduler is basically the brain and heart of Airflow. It handles triggering and scheduling of workflows as well as submitting tasks to the executor to run. To be able to do this, the scheduler is responsible to parse the DAG files from the *DAG directory*, manage the database states in the *metadata database*, and to communicate with the executor to schedule tasks. Since the release of Airflow 2.0 it is possible to run multiple schedulers at a time to ensure a high availability and reliability (TODO check term) of this centerpiece of a distributed Airflow.

### Webserver

The webserver runs the web interface of Airflow and thus the user interface every Airflow user sees. This allows to inspect, trigger, and debug DAGs and tasks in Airflow (and much more!), such as seen in the previous chapters. Each user interaction and change is written to the *DAG directory* or the *metadata database*, from where the *scheduler* will read and act upon.

### Executor 

The executor handles running the tasks themselves. In a local installation of Airflow, the tasks are run by the executor itself, whereas in a production ready deployment if Airflow the executor pushes the task execution to separate worker instances. The benefit of a distributed deployment is either reliability, but also the possibility to run tasks on different instances based on their needs, for example to run the training step of a machine learning model on a GPU node.

TODO
a [Celery](https://docs.celeryproject.org/en/stable/getting-started/introduction.html) worker application which consumes and executes tasks scheduled by scheduler when using a Celery-like executor (more details in next section). It is possible to have many workers in different places (for example using separate VMs or multiple kubernetes pods).

### DAG Directory

The DAG directory contains the DAG files written in python. Each file is read by the the other Airflow components for a different purpose. The *web interface* lists all written DAGs from the directory as well as their content. The scheduler and executor run a DAG or a task based on the input read from the DAG directory.
The DAG directory can be of different nature either a local folder in case of a local installation, or a separate (git) repository where the DAG files are stored. (TODO how does it handle subdirectories?)

### Metadata Database

The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, and user settings. It is beneficial to run the metadata database as a separate component to keep all data safe and secure in case there are bugs in other parts of the infrastructure. Such thoughts also account for the deployment of the metadata database itself. It is possible to run Airflow on an instance within a Kubernetes cluster along all other components of a distributed Airflow installation. However, this is not necessarily recommended and it is also possible to use a clouds' distinguished database resources to store the metadata, for example the Relational Database Service (RDS) on the amazon cloud.



* [ ] Bilder einfügen
* [ ] Code einfügen
* [ ] Todos bearbeiten
* [ ] prerequisites installation
* [ ] https://towardsdatascience.com/a-complete-introduction-to-apache-airflow-b7e238a33df
* [ ] Text einheitlich schreiben
* [ ] Korrekturlesen
* [ ] Taskflow?



## SAVE

**DAG workload**

A DAG usually consists of multiple steps it runs through, also names as tasks. There are different types of tasks in Airflow,
for example *Operators*, *Sensors*, *TaskFlow-decorated tasks*. All of them will be outlined in the following subsections.

    
    <> * *Operators*: are predefined tasks that can be stringed together to quickly build most parts of the DAG and basically the main pipeline steps.
    <> * *Sensors:* are a special subclass of operators that are triggered when an external event is happening.
    <> * *TaskFlow-decorated tasks:* are part of a custom python function where a task cann be packaged via the decorator `@task`.
    
    
Using operators is the classic approach to defining work in Airflow. For some use cases, it’s better to use the TaskFlow API to define work in a Pythonic context as described in [Working with TaskFlow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html).


Gotchas: They are not guaranteed to run in the same location/environment. May require extensive use of Environment variables. Can be difficult to run tasks with elevated privileges.
