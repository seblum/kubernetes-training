# Introduction

*"MLOps Engineering with Airflow and MLflow on Kubernetes: Building, Deploying, and Managing Machine Learning Workflows"* is a comprehensive guide to building and deploying machine learning models using Airflow and MLflow on an Kubernetes cluster deployed using Terraform. The book is designed for data scientists, engineers and DevOps practitioners who are looking to dive into the realm of machine learning operations and automate the deployment of their models in a production environment.

The book begins by introducing the concepts of MLOps and the challenges associated with building, deploying, and managing machine learning models in a production environment. It then covers the basics of *Airflow*, a powerful tool for building and managing machine learning workflows, and *MLflow*, a platform for managing the machine learning lifecycle.

Next, the book covers the basics of *Terraform*, a tool for provisioning and managing infrastructure as code, and *Kubernetes*, an open-source system for automating the deployment, scaling, and management of containerized applications. 

Finally, the book covers the process of deploying Airflow and MLflow on an Kubernetes cluster using Terraform and hencefore the implementation of the previously introduced tools. A machine learning workflow using Airflow is set up on the deployed infrastructure, including data preprocessing, model training, and model deployment, as well as tracking the experiment and deploying the model into production using MLFlow. 

<!---
Finally, the book covers advanced topics such as monitoring and troubleshooting machine learning workflows, and automating the management of machine learning models using Airflow and MLflow.

Finally, the book covers advanced topics such as implementing ML workflows in a CI/CD pipeline, and automating the management of machine learning models using Airflow and MLflow.
-->

## Machine Learning Workflow

A machine learning workflow typically involves several stages. These stages are closely related are sometimes overlap as some stages may involve multiple iterations. The stages are often handled by the same tool or platform a a clear differentiation across stages and tools is sometimes fairly difficult. Further, some machine learning workflows will not have all the steps or they might have some variations. In the following, the machine learning workflow is broken down to five different stages to make things easier and give an overview.

**1. Data Preparation**
In the first stage, the data used to train a machine learning model is collected, cleaned, and preprocessed . This includes tasks to remove missing or duplicate data, normalize data, and split the data into training and testing sets.
    
**2. Model Building** 
In the second stage, a machine learning model is selected and trained using the prepared data. This includes tasks such as selecting an appropriate algorithm, training the model, and tuning the model's parameters to improve its performance.
    
**3. Model Evaluation**
Afterward, the performance of the trained model is evaluated using the test data set. This includes tasks such as measuring the accuracy and other performance metrics, comparing the performance of different models, and identifying potential issues with the model.
    
**4. Model Deployment** 
Finally, the selected and optimized model is deployed to a production environment where it can be used to make predictions on new data. This stage includes tasks such as scaling the model to handle large amounts of data, and deploying the model to different environments to be used in different contexts
    
**5. Model Monitoring and Maintenance**
Once the model is deployed, it is important to monitor the model performance and update it as needed. This includes tasks such as collecting feedback, monitoring the model's performance metrics, and updating the model as necessary.

A machine learning workflow is thereby not a walk in the park. Being productive with machine learning, monitoring its performance and continuously retraining it on new data with possible alternative models can be challenging and involves the right tools.


### ML Worflow Tools

There are several machine learning workflow tools that can help streamline the process of building and deploying machine learning models. Integrating them into the machine learning workflow their functions can be distinguished into three processes. (1) *Workflow Management*, (2) *Model Tracking*, and (3) *Model Serving*. All three of these processes are closely related and are often handled by the same tool or platform.

#### Workflow Management

Workflow Management is the process of automating and streamlining the steps involved in building, training, and deploying machine learning models. This includes tasks such as data preprocessing, model training, and model deployment. Workflow management allows for the coordination of all the necessary steps in the ML pipeline, and the tracking of the pipelineâ€™s progress. 

Workflow management tools are used throughout the entire machine learning workflow. *Apache Airflow* is an open-source platform for workflow management and is widely used to automate the training and deployment of machine learning models.

#### Model Tracking

Model Tracking is the process of keeping track of the different versions of a machine learning model including tracking the performance of each version, the parameters used, and the data used to train it
Model Tracking tools are often used at the development and testing stages of the machine learning workflow. During development, tracking allows to keep track of the different versions of a model, compare their performance and learning parameters, and finally helps to select the best model version to deploy. Model tracking also allows to check the performance of a machine learning model during testing and assures it meets industry requirements.

*MLflow* is an open-source platform to manage the machine learning lifecycle, including experiment tracking, reproducibility, and deployment. Similarly, *DVC* (Data Version Control) is a tool that allows to version control, manage, and track not only models but also data.

#### Model Serving

Model Serving refers to the process of deploying a machine learning model in a production environment, so it can be used to make predictions on new data. This includes tasks such as scaling the model to handle large amounts of data, deploying the model to different environments, and monitoring the performance of the deployed model.

Model serving tools are specifically used at the deployment stage of the machine learning workflow. Model serving tools can handle the previous tasks such as deployment, scaling, and monitoring of a machine learning model.
Multiple tools integrate the funtionality of serving models, each different in its specific use cases, for example *TensorFlow*, *Kubernetes*, *DataRobot*, or also the already mentioned tools *MLflow* and *Airflow*


## Machine Learning Operations (MLOps)

Machine Learning Operations (MLOps) combines principles from machine learning development and operations to enable the efficient deployment and management of machine learning models in a production environment. The goal of MLOps is to automate and streamline the machine learning workflow as much as possible, while also ensuring that each machine learning model is performing as expected in production. 
Using MLOps principles allows organizations to move quickly and efficiently from prototyping to production.

The infrastructure involved in MLOps includes both hardware and software components. Hardware components include servers and storage devices for data storage and model deployment, as well as specialized hardware such as GPUs for training and inferencing. Software components include version control systems, continuous integration and continuous deployment (CI/CD) tools, containerization and orchestration tools, and monitoring and logging tools. 
There are several cloud-based platforms, such as AWS SageMaker, Azure Machine Learning, and Google Cloud AI Platform, that provide pre-built infrastructure to manage and deploy models, automate machine learning workflows, and monitor and optimize the performance of models in production.

The key concepts and best practices of MLOps, and the use of tools and technologies to support the MLOps process are closely related to *regular* DevOps principles which are a standard for the operation of software in the industry.


### ML + (Dev)-Ops

Machine Learning Operations (MLOps) and Development Operations (DevOps) both aim to improve the efficiency and effectiveness of software development and deployment. Both practices share major similarities, but there are some key differences that set them apart.

The goal of DevOps is to automate and streamline the process of building, testing, and deploying software to make the management of the software lifecycle as quick and efficient as possible. This can be achieved by using tools, such as:

1.  *Containerization*: Tools such as Docker and Kubernetes are used to package and deploy applications and services in a consistent and reproducible manner.
2.  *Version Control*: Tools such as Git, SVN, and Mercurial are used to track and manage changes to code and configuration files, allowing teams to collaborate and roll back to previous versions if necessary.
3.  *Continuous Integration and Continuous Deployment (CI/CD)*: Tools such as Jenkins, Travis CI, and Github Actions are used to automate the building, testing, and deployment of code changes.
4.  *Monitoring and Logging*: Tools such as Prometheus, Grafana, and ELK are used to monitor the health and performance of applications and services, as well as to collect and analyze log data.
5.  *Cloud Management Platforms*: AWS, Azure, and GCP are used to provision, manage, and scale infrastructure and services in the cloud. Scaling and provisioning of infrastructure can be automated by using Infrastructure as Code (IaC) Tools like Terraform.

DevOps focuses on the deployment and management of software in general, while MLOps focuses specifically on the deployment and management of machine learning models in a production environment. The goal is basically the same as in DevOps, yet deploying machine learning model. While this is achieved by the same tools and best practices used in DevOps, deploying machine learning models (compared to software) adds a lot of complexity to the process.

Machine learning models are not just lines of code, but also require large amounts of data, and specialized hardware, to function properly. Further, their complex algorithms might need to change when there is a shift in new data. This process of ensuring that machine learning models are accurate and reliable lead to further challenges. Another key difference is that MLOps places a greater emphasis on model governance, which needs to ensure that machine learning models are compliant with relevant regulations and standards. The above list of tools within DevOps can be extended to the following for MLOps.

1.  *Machine Learning Platforms*: These platforms such as TensorFlow, PyTorch, and scikit-learn are used to develop and train machine learning models.
2.  *Experiment Management Tools*: Tools such as MLflow, Weights & Biases, and Airflow are used to track, version, and reproduce experiments, as well as to manage the model lifecycle.
3.  *Model Deployment and Management Tools*: Tools such as TensorFlow Serving, Clipper, and Seldon Core are used to deploy and manage machine learning models in production.
4.  *Data Versioning and Management Tools*: Tools such as DVC, Data Version Control, and Pachyderm are used to version and manage the data used for training and evaluating models.    
5.  *Automated Model Evaluation and Deployment Tools*: Tools such as Kubeflow, AlgoTrader, and AlgoHub are used to automate the evaluation of machine learning models and deploy the best-performing models to production.

It's important to note that the specific tools used in MLOps and DevOps may vary depending on the organization's needs and some of the tools are used both but applied differently in each, e.g. container orchestration tools like Kubernetes. The above lists do also not claim to be complete.


### MLOps Engineering

MLOps Engineering is the discipline that applies the previously mentioned software engineering principles to create the needed development and production environment.
It usually comines the skills and expertise of data scientists, machine learning engineers, and DevOps engineers to ensure that machine learning models are deployed and managed efficiently and effectively.

One of the key aspects of MLOps Engineering is infrastructure management. The infrastructure refers to the hardware, software and networking resources that are needed to support the machine learning models in production. The infrastructure is crucial for the success of MLOps as it ensures that the models are running smoothly, are highly available and are able to handle the load of the incoming requests. It also helps to prevent downtime, ensure data security and guarantee the scalability of the models.

MLOps Engineering is responsible for designing, building, maintaining and troubleshooting the infrastructure that supports machine learning models in production. This includes provisioning, configuring, and scaling the necessary resources like cloud infrastructure, Kubernetes clusters, machine learning platforms and model serving infrastructure. It is useful to use configuration management and automation tools like Terraform to manage the infrastructure in a consistent, repeatable and version controlled way. This allows to easily manage and scale the infrastructure as needed, and roll back to previous versions if necessary.

It is important to note that one person can not exceed on all of the above mentioned tasks of designing, building, maintaining and troubleshooting. Developing the infrastructure for machine learning models in production usually requires multiple people working with different and specialized skillsets and roles.


## Roles and Tasks in MLOps

The MLOps lifecycle typically involves several key roles and responsibilities, each with their own specific tasks and objectives.

#### Data Engineer

A data engineer designs, builds, maintains, and troubleshoots the infrastructure and systems that support the collection, storage, processing, and analysis of large and complex data sets. Task include designing and building data pipelines, managing data storage, optimizing data processing, ensuring data quality, and monitoring the necessary data pipelines and systems. They usually work closely with data scientists to understand their data needs and ensure the infrastructure supports their requirements.

#### Data Scientist
Data Scientists are usually responsible for developing and testing machine learning models within the development stage. Their work typically involves investigating large amounts of data, the necessary preprocessing steps, and choosing a suitable machine learning model that solves the business need. They further develop a functioning ML model respective to the data 
    
#### ML Engineer
ML Engineers work closely with Data Scientists to develop and deploy machine learning models in a production environment. They are responsible for creating and maintaining the infrastructure and tools needed to run machine learning models at scale and in production. They are also responsible for the day-to-day management and monitoring of machine learning models in a production environment. They use tools and technologies to track model performance and make sure that models are running smoothly and produce accurate results. This also includes taking measures in case of data or model shifts [^1].

[^1]: Model shift refers to the change of a deployed model compared to one used during development and testing, while data shift refers to a change in the distribution of input data compared to the data used during development and testing.
    
#### MLOps Engineer
The MLOps engineer is responsible to create and maintain the infrastructure and tooling needed to train and deploy models, monitoring the performance of deployed models, and implementing processes for collaboration and version control. This includes tasks such as setting up and configuring the necessary hardware and software environments, creating and maintaining documentation, and implementing automated testing and deployment processes. The MLOps engineer must be able to navigate this complexity and ensure that the models are deployed and managed in a way that is both efficient and effective.
  
#### DevOps Engineer
DevOps Engineers are responsible for automating and streamlining the process of building, testing, and deploying software. They use best practices from software development and operations, such as version control, continuous integration and delivery, and monitoring and observability, to ensure that models are performing as expected in production. 
    
#### Additional roles & function
The previous roles only show a small portion of all contributors within data projects. Additional roles within an industry context might also include *Model Governance*, which is responsible to ensure that models are accurate, reliable, and compliant with relevant regulations and standards, or *Business stakeholders*, that provide feedback and requirements for the models in a business context.

It's also worth noting that some of these roles may overlap and different organizations may have different ways of structuring their teams and responsibilities. The most important thing is to have clear communication and collaboration between all the different teams, roles and stakeholders involved in the MLOps lifecycle.


## MLOps Engineering with Airflow and MLflow on Kubernetes

This work shows an example of a MLOps plattform in form of a deployment of Airflow and MLflow on a Kubernetes cluster using Terraform. It wil focus in the part of MLOps engineering as the whole infrastructure is set up from scratch. It will also introduce the work done by machine learning scientist and data scientists to implement basic machine learning models on the MLOps plattform.

Hencefore it gives an introductory tutorial each on the implementation of the previously introduced tools. A machine learning workflow using Airflow is set up on the deployed infrastructure, including data preprocessing, model training, and model deployment, as well as tracking the experiment and deploying the model into production using MLFlow. 

The structure on Git can be distinguished on two different repositories, one for infrastructure deployment, and the other for the used machine learning models. The project has the following architecture. 

![Architecture Overview](images/01-Introduction/architecture-overview.png)

The necessary AWS infrastructure is set up using Terraform. This includes creating an AWS EKS cluster and the associated ressources like a virtual private cloud (VPC), subnets, security groups, IAM roles, as well as further AWS ressources needed to deploy Airflow and MLflow.
Once the EKS cluster is set up, Kubernetes can be used to deploy and manage applications on the cluster. Helm, a package manager for Kubernetes, is used to manage the deployment of Airflow and MLflow. The EKS cluster allows for easy scalability and management of these platforms. Github Actions is used for automating the deployment of the infrastructure using CI/CD principles. 

Once the infrastructure is set up, machine learning models can be deployed to the EKS cluster as Kubernetes pods, using Helm charts as well. This is can be done in an automated fashion using Airflow's ability to scan the second repository responsible for the machine learning models. Similarly, to building Airflow workflows, those algorithms would also include the use of the MLFlow API to enable model tracking and serving. Github Actions is used as a CI/CD pipeline to automatically build, test, and deploy machine learning models to this repository similarly as it is used in the repository for the infrastructure. 

<!---
Monitoring and logging would be achieved using CloudWatch to monitor the health and performance of the EKS cluster and its components, such as worker nodes, Kubernetes pods, etc and ELK stack or similar for logging of the system and applications. Networking would be handled by AWS Elastic Load Balancing service or Ingress controller to route traffic to the correct service/pod in the cluster.
-->

Whereas the deployment of the infrastructure would be taken care of by MLOps-, DevOps-, and Data Engineers, the development of the Airflow workflows including MLFlow would be taken care of by Data Scientist and ML Engineers.


# TODO:

- [x] resolve todos
- [x]  mention terraform, who uses it? when used?
- [x]  mention data engineer
- [ ]  proof read
- [ ]  insert images
