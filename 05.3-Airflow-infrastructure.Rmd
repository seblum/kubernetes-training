
## Airflow infrastructure

Before Data Scientists and Machine Learning Engineers can utilize the power of Airflow Workflows, Airflow obviously needs to be set up and deployed. There are multiple ways an Airflow deployment can take place. It can be run either on a single machine or in a distributed setup on a cluster of machines. As stated in the prerequisites for this tutorial we set up Airflow locally on a single machine to introduce you on how to work with airflow. Although Airflow can be run on a single machine, it is beneficial to deploy it as a distributed system to utilize its full power.

### Airflow as a distributed system

Airflow consists of several separate parts. While this separation is somewhat simulated on a local deployment, each several part of Airflow can be deployed separately when deploying Airflow in a distributed manner. This comes with benefits of availability, security, reliability, and scalability.

An Airflow deployment generally consists of five different compontens:

* **Scheduler:** The schedules handles triggering scheduled workflows and submitting tasks to the executor to run.
* **Executor:** The executor handles running the tasks themselves. In a local installation of Airflow, the tasks are run by the executor itself, whereas in a production ready deployment if Airflow the executor pushes the task execution to separate worker instances.
* **Webserver:** The webserver provides the user interface of Airflow we have seen before that allows to inspect, trigger, and debug DAGs and tasks.
* **DAG Directory:** The DAG directory is a directory that contains the DAG files which are read by the scheduler and executor.
* **Metadata Database:** The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, and user settings.

The following graphs shows how the components build up the Airflow architecture.

![airflow_architecture](./images/05-Airflow/airflow_architecture.png)

### Scheduler 

The scheduler is basically the brain and heart of Airflow. It handles triggering and scheduling of workflows as well as submitting tasks to the executor to run. To be able to do this, the scheduler is responsible to parse the DAG files from the *DAG directory*, manage the database states in the *metadata database*, and to communicate with the executor to schedule tasks. Since the release of Airflow 2.0 it is possible to run multiple schedulers at a time to ensure a high availability and reliability of this centerpiece of a distributed Airflow.

### Webserver

The webserver runs the web interface of Airflow and thus the user interface every Airflow user sees. This allows to inspect, trigger, and debug DAGs and tasks in Airflow (and much more!), such as seen in the previous chapters. Each user interaction and change is written to the *DAG directory* or the *metadata database*, from where the *scheduler* will read and act upon.

### Executor 

How and where are the DAG workflows actually run? This is where executors come into play - they run tasks. An executor defines where and how the Airflow tasks should be executed. This crucial component of Airflow can be configured by the user and should be chosen to fit the users specific needs. There are several different executors, each handleing the run of a task a bit differently. This also relies on the underlying infrastructure Airflow is build on.  

In a local installation of Airflow, the tasks are run by the executor itself, whereas in a production ready deployment if Airflow the executor pushes the task execution to separate worker instances. This allows for different setups such as a Celery-like executor or an executor based on Kubernetes. The benefit of a distributed deployment is reliability and availability, as it is possible to have many workers in different places (for example using separate VMs or multiple kubernetes pods). It is further possibility to run tasks on different instances based on their needs, for example to run the training step of a machine learning model on a GPU node.

+ The `SequentialExecutor` is the default executor in Airflow. This executor only runs one task instance at a time and should therefore not used in a production use case.
+ The `LocalExecutor`executes each task in a separate processe on a single machine. It’s the only non-distributed executor which is production ready and works well in relatively small deployments. If you installed Airflow as show above in the prerequisites, you will use this executor. 
+ In contrast, the `CeleryExecutor` uses under the hood the Celery queue system that allows users to deploy multiple workers that read tasks from the broker queue (Redis or RabbitMQ) where tasks are sent by scheduler. This enables Airflow to distribute tasks between many machines and allows users to specify what task should be executed where. This is for example useful for routing compute-heavy tasks to more resourceful workers and is the most popular production executor.
+ The `KubernetesExecutor` is another widely used production-ready executor and works similarly to the `CeleryExecutor`. As the name allready suggests it requires an underlying Kubernetes cluster that enables Airflow to spawn a new pod to run each task. Even though this is a robust method to account for machine or pod failure, the additional overhead in creation can be problematic for short running tasks.
+ The `CeleryKubernetsExecutor` uses both, the `CeleryExecutor` and `KubernetesExecutor` (as the name already says). It allows to distinguish whether a particular task should be executed on kubernetes or routed to the celery workers. This way users can take full advantage of horizontal auto scaling of worker pods and to delegate computational heavy tasks to kubernetes.
+ An additional `DebugExecutor`works as a is a debug executor. Its main purpose is to debug DAGs locally. It’s the only executor that uses a single process to execute all tasks.

Checking which executor is currently set can be done by running the following command.

```bash
airflow config get-value core executor
```


### DAG Directory

The DAG directory contains the DAG files written in python. Each file is read by the the other Airflow components for a different purpose. The *web interface* lists all written DAGs from the directory as well as their content. The scheduler and executor run a DAG or a task based on the input read from the DAG directory.
The DAG directory can be of different nature either a local folder in case of a local installation, or a separate (git) repository where the DAG files are stored. The scheduler will recurse through the DAG Directory so it is also possible to create subfolders for example based on different projects.

### Metadata Database

The metadata database is used to store data of the scheduler, executor, and the webserver, such as scheduling- or runtime, and user settings. It is beneficial to run the metadata database as a separate component to keep all data safe and secure in case there are bugs in other parts of the infrastructure. Such thoughts also account for the deployment of the metadata database itself. It is possible to run Airflow on an instance within a Kubernetes cluster along all other components of a distributed Airflow installation. However, this is not necessarily recommended and it is also possible to use a clouds' distinguished database resources to store the metadata, for example the Relational Database Service (RDS) on the amazon cloud.


